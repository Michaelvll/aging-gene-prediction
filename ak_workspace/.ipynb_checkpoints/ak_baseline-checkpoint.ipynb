{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd862f24-3f46-4870-86c3-87ba919d36fd",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6af8be12-a202-4815-b6d9-cc4aa6b9fc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install kornia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ace26c5-5e11-4587-a8e3-70d3a148f53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gale/netapp/home2/aklein/miniconda3/envs/agp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from data_loader import load_data, get_balanced_data, normalize_features\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# import wandb\n",
    "# wandb.init(project='gene')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "847e86cb-b7f2-4b53-a168-9da564e13d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kornia\n",
    "# from kornia.losses import FocalLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d399ad00-91ff-46bd-89f9-3d1401c6251a",
   "metadata": {},
   "source": [
    "## Session parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1491f013-244a-49fe-98db-5d861b55393c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.15\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01f8441b-76ca-4d9e-b94a-f1418e89bfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n",
      "1.26.4\n",
      "True\n",
      "11.3\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__), print(np.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce3831e9-3361-464a-8f59-52bd31a5c4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 NVIDIA A40\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.current_device(), torch.cuda.device_count(), torch.cuda.get_device_name(0))\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9f9cc7-991a-4b67-8845-da7d5024a48b",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "065f008e-167e-4915-8f3c-6e4c511dbcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ct = \"Oligo_NN\"\n",
    "_testing_type = \"imbalanced\"\n",
    "# FEATURE_TYPES = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccd08a71-6700-47c4-9c16-c04b86a352ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed mcg data\n",
      "Processed genebody data\n",
      "Processed atac data\n",
      "Processed hic data\n",
      "mcg 5546\n",
      "genebody 5546\n",
      "atac 5546\n",
      "hic 5546\n"
     ]
    }
   ],
   "source": [
    "data = load_data(y_val = \"DEG\", ct=_ct)\n",
    "\n",
    "# For balanced testing (Regular Cross Entropy Loss)\n",
    "if _testing_type == \"balanced\":\n",
    "    X_balanced, y_balanced = get_balanced_data(data, method='balanced', y_val=\"DEG\")\n",
    "    \n",
    "# For imbalanced training (loss function must accomodate for this)\n",
    "elif _testing_type == \"imbalanced\":\n",
    "    X_balanced = data['X']\n",
    "    y_balanced = data['y']\n",
    "else: \n",
    "    raise(\"error\")\n",
    "\n",
    "FEATURE_TYPES = ['mcg', 'atac', 'hic', 'genebody']\n",
    "for k, v in X_balanced.items():\n",
    "        print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a54016c3-de17-4a38-9e8f-40c98f5720bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _feat in FEATURE_TYPES:\n",
    "    Path(f\"../baseline_results/{_ct}/{_feat}\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0524d1b8-06c6-410b-b861-1885af9a6b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  0,  1]), array([ 874, 4115,  557]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data['y']\n",
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "954ee313-83bd-4731-baa5-97e78fe70ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all 0 mcg entires:\n",
      "Down Reg: 745 / 874\n",
      "No Effect: 3422 / 4115\n",
      "Up Reg: 314 / 557\n"
     ]
    }
   ],
   "source": [
    "X_mcg = data['X']['mcg']\n",
    "all0 = X_mcg[0][0]\n",
    "count = {-1 : 0, 0 : 0, 1 : 0}\n",
    "for x, yy in zip(X_mcg, y): \n",
    "    if all0 == x[0]: \n",
    "        count[yy]+=1\n",
    "print(f\"all 0 mcg entires:\")\n",
    "print(f\"Down Reg: {count[-1]} / {(np.asarray(y) == -1).sum()}\")\n",
    "print(f\"No Effect: {count[0]} / {(np.asarray(y) == 0).sum()}\")\n",
    "print(f\"Up Reg: {count[1]} / {(np.asarray(y) == 1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "339dae0c-fe2c-4b39-9418-b2213d07b479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all 0 atac entires:\n",
      "Down Reg: 32 / 874\n",
      "No Effect: 173 / 4115\n",
      "Up Reg: 34 / 557\n"
     ]
    }
   ],
   "source": [
    "X_atac = data['X']['atac']\n",
    "all0 = [0]*11\n",
    "count = {-1 : 0, 0 : 0, 1 : 0}\n",
    "for x, yy in zip(X_atac, y): \n",
    "    for xx in x: \n",
    "        if all0 == xx: \n",
    "            count[yy]+=1\n",
    "print(f\"all 0 atac entires:\")\n",
    "print(f\"Down Reg: {count[-1]} / {(np.asarray(y) == -1).sum()}\")\n",
    "print(f\"No Effect: {count[0]} / {(np.asarray(y) == 0).sum()}\")\n",
    "print(f\"Up Reg: {count[1]} / {(np.asarray(y) == 1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edd941dd-bde2-4e92-9829-5422e519d4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all 0 hic entires:\n",
      "Down Reg: 240 / 874\n",
      "No Effect: 1262 / 4115\n",
      "Up Reg: 151 / 557\n"
     ]
    }
   ],
   "source": [
    "X_hic = data['X']['hic']\n",
    "all0 = [0]*len(X_hic[0][0])\n",
    "count = {-1 : 0, 0 : 0, 1 : 0}\n",
    "for x, yy in zip(X_hic, y): \n",
    "    for xx in x: \n",
    "        if all0 == xx: \n",
    "            count[yy]+=1\n",
    "print(f\"all 0 hic entires:\")\n",
    "print(f\"Down Reg: {count[-1]} / {(np.asarray(y) == -1).sum()}\")\n",
    "print(f\"No Effect: {count[0]} / {(np.asarray(y) == 0).sum()}\")\n",
    "print(f\"Up Reg: {count[1]} / {(np.asarray(y) == 1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6582fd4a-96bf-4400-937d-3be536d719b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all 0 genebody entires:\n",
      "Down Reg: 12 / 874\n",
      "No Effect: 3 / 4115\n",
      "Up Reg: 0 / 557\n"
     ]
    }
   ],
   "source": [
    "X_gb = data['X']['genebody']\n",
    "all0 = [0]*len(X_gb[0][0])\n",
    "count = {-1 : 0, 0 : 0, 1 : 0}\n",
    "for x, yy in zip(X_gb, y): \n",
    "    for xx in x: \n",
    "        if all0 == xx: \n",
    "            count[yy]+=1\n",
    "print(f\"all 0 genebody entires:\")\n",
    "print(f\"Down Reg: {count[-1]} / {(np.asarray(y) == -1).sum()}\")\n",
    "print(f\"No Effect: {count[0]} / {(np.asarray(y) == 0).sum()}\")\n",
    "print(f\"Up Reg: {count[1]} / {(np.asarray(y) == 1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20508baa-2433-44ec-ab16-f89e15af12be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes_mcg = []\n",
    "sizes_atac = []\n",
    "sizes_hic = []\n",
    "sizes_gb = []\n",
    "for i in range(len(y)):\n",
    "    sizes_mcg.append(len(X_mcg[i]))\n",
    "    sizes_atac.append(len(X_atac[i]))\n",
    "    sizes_hic.append(len(X_hic[i]))\n",
    "    sizes_gb.append(len(X_gb[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a5fab2f-51fd-4f60-b6b9-94618989ec75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_sizes = pd.DataFrame([sizes_mcg, sizes_atac, sizes_hic, sizes_gb], index=['mcg DMR', 'atac', 'hic', 'genebody']).T\n",
    "sns.stripplot(df_sizes, s=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac93d140-b451-4ff6-8555-c621ec438693",
   "metadata": {},
   "source": [
    "## Functions for handling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1aae090b-d89a-4f9a-966a-3821b6649efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 64\n",
    "NUM_LAYERS = 4\n",
    "NUM_HEADS = 8\n",
    "DROPOUT = 0.1\n",
    "LR = 1e-4\n",
    "OUTPUT_DIM = 3  # number of classes (-1, 0, 1)\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "class OneHeadTransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, num_heads=1, dropout=0.1):\n",
    "        super(OneHeadTransformerModel, self).__init__()\n",
    "        self.cls_embedding = nn.Parameter(torch.randn(1, 1, hidden_dim))\n",
    "        self.input_embedding = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # TODO: may need to use tanh in attention instead of softmax\n",
    "        encoder_layers = nn.TransformerEncoderLayer(hidden_dim, num_heads, dim_feedforward=hidden_dim*4, dropout=dropout, batch_first=True, norm_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        \n",
    "        # Use self-attention mechanism to combine the outputs of the four heads\n",
    "        self.classifier = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, _x, x_mask):\n",
    "        # print('x in', _x.device)\n",
    "        # print('x mask', x_mask.device)\n",
    "        _x = self.input_embedding(_x)\n",
    "        # print('x 1', _x.device)\n",
    "        \n",
    "\n",
    "        # Add cls embedding, so that we can take the first output as the representation of the sequence\n",
    "        _cls_embedding = self.cls_embedding.repeat(_x.size(0), 1, 1)\n",
    "        # print('cls', _cls_embedding.device)\n",
    "\n",
    "        _z = torch.cat((_cls_embedding, _x), dim=1)\n",
    "        # print('z 1', _z.device)\n",
    "\n",
    "        _mask = torch.cat((torch.ones(x_mask.size(0), 1).to(device), x_mask), dim=1)\n",
    "        # print('mask cat', _mask.device)\n",
    "\n",
    "        _z = self.transformer(_z, src_key_padding_mask=~_mask.bool())\n",
    "        # print('z 2', _z.device)\n",
    "        \n",
    "        # Pooling, we should not use average pooling since the sequence length is also important\n",
    "        _z = _z[:, 0, :]\n",
    "        # print('z 3', _z.device)\n",
    "    \n",
    "        # Combine the output of four heads into a sequence\n",
    "        _z = _z.squeeze(-1) # (batch_size, hidden_dim, 1)\n",
    "        # print('z 4', _z.device)\n",
    "        output = self.classifier(_z)\n",
    "        # print('out', output.device)\n",
    "\n",
    "        del _x, x_mask, _cls_embedding, _mask, _z\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49e12a99-e33a-45ea-b21a-5991ed674fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Object\n",
    "class GeneDataset(Dataset):\n",
    "    def __init__(self, _data, labels):\n",
    "        self.data = _data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        gene_data = torch.FloatTensor(self.data[idx]).to(device)\n",
    "        label = torch.LongTensor([self.labels[idx] + 1]).to(device)  # Add 1 to shift labels to 0, 1, 2\n",
    "        mask = torch.ones(len(gene_data)).to(device)\n",
    "        return gene_data, label, mask\n",
    "\n",
    "## handling the batches \n",
    "def collate_fn(batch):\n",
    "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    gene_sequences, labels, masks, = zip(*batch)\n",
    "    \n",
    "    lengths = [len(seq) for seq in gene_sequences]\n",
    "    max_len = max(lengths)\n",
    "    \n",
    "    padded_seqs = torch.zeros(len(gene_sequences), max_len, gene_sequences[0].size(1))\n",
    "    padded_masks = torch.zeros(len(gene_sequences), max_len)\n",
    "    \n",
    "    for i, (seq, length) in enumerate(zip(gene_sequences, lengths)):\n",
    "        padded_seqs[i, :length] = seq\n",
    "        padded_masks[i, :length] = 1 \n",
    "        \n",
    "    return padded_seqs.to(device), torch.cat(labels), padded_masks.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7db73676-b863-4826-b3b5-24e906a89c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def checkpoint(model, fname):\n",
    "    torch.save(model.state_dict(), fname)\n",
    "    \n",
    "def load_chkpt(model, fname):\n",
    "    model.load_state_dict(torch.load(fname))\n",
    "        \n",
    "def train_model(X_train, y_train, X_test, y_test, exp_name, fold_idx):\n",
    "    #wandb.init(project='gene', group=exp_name, name=f'fold-{fold_idx}')\n",
    "    input_dim = len(X_train[0][0])\n",
    "    \n",
    "    train_dataset = GeneDataset(X_train, y_train)\n",
    "    test_dataset = GeneDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    model = OneHeadTransformerModel(input_dim, HIDDEN_DIM, OUTPUT_DIM, num_layers=NUM_LAYERS, num_heads=NUM_HEADS, dropout=DROPOUT).to(device)\n",
    "\n",
    "    # For imbalanced CEL\n",
    "    class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = torch.tensor(class_weights.astype(np.float32)).to(device)\n",
    "    # weights = torch.tensor(np.unique(y_train, return_counts=True)[1], dtype=float).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    # criterion = FocalLoss(gamma=2, alpha=0.1)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=0.0001)\n",
    "    # Create the OneCycleLR scheduler\n",
    "    # lr_scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=LR, total_steps=NUM_EPOCHS,\n",
    "    #                           pct_start=0.8, anneal_strategy='cos',\n",
    "    #                           cycle_momentum=False, div_factor=5.0,\n",
    "    #                           final_div_factor=10.0)\n",
    "\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    early_stopper = EarlyStopper(patience=5, min_delta=0.1)\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        checkpoint(model, f\"../logging/m_chkpt_{epoch}\")\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        for x1, batch_y, mask1 in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x1, mask1)\n",
    "            loss = criterion(outputs, batch_y.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            train_correct += (outputs.argmax(dim=1) == batch_y.squeeze()).sum().item()\n",
    "            train_total += batch_y.size(0)\n",
    "            del x1, batch_y, mask1, loss, outputs\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x1, batch_y, mask1 in test_loader:\n",
    "                outputs = model(x1, mask1)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                test_loss += criterion(outputs, batch_y.squeeze()).item()\n",
    "                total += batch_y.size(0)\n",
    "                correct += (predicted == batch_y.squeeze()).sum().item()\n",
    "                del x1, batch_y, mask1\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        if early_stopper.early_stop( test_loss / len(test_loader) ): \n",
    "            min_e = np.argmin(test_losses)\n",
    "            load_chkpt(model, f\"../logging/m_chkpt_{min_e}\")\n",
    "            print(\"Early Stop\")\n",
    "            print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {total_loss/len(train_loader):.4f}, Train Accuracy: {train_correct/train_total:.4f}, Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {accuracy:.4f}')\n",
    "            print()\n",
    "            break\n",
    "        train_accuracies.append(train_correct/train_total)\n",
    "        train_losses.append(total_loss/len(train_loader))\n",
    "        test_accuracies.append(accuracy)\n",
    "        test_losses.append(test_loss / len(test_loader))\n",
    "        print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {total_loss/len(train_loader):.4f}, Train Accuracy: {train_correct/train_total:.4f}, Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {accuracy:.4f}')\n",
    "        del total_loss, train_correct, train_total, correct, total, test_loss\n",
    "        \n",
    "        #wandb.log({'epoch': epoch, 'LR': optimizer.param_groups[0]['lr'], 'train_loss': total_loss/len(train_loader), 'train_accuracy': train_correct/train_total, 'test_accuracy': accuracy})\n",
    "\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for x1, batch_y, mask1, in test_loader:\n",
    "            outputs = model(x1, mask1)\n",
    "            _, predicted = torch.max(outputs.detach().cpu().data, 1)\n",
    "            all_predictions.extend(predicted.numpy())\n",
    "            all_labels.extend(batch_y.detach().cpu().numpy())\n",
    "            del x1, batch_y, mask1\n",
    "\n",
    "\n",
    "    # Trying to free up as much memory as possible here\n",
    "    del class_weights, criterion, optimizer, lr_scheduler\n",
    "    del X_train, X_test, y_train, y_test\n",
    "    \n",
    "    return (all_predictions, all_labels), (train_accuracies, train_losses, test_accuracies, test_losses), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b232940-6e64-4ca4-9354-c4cf29f0fe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c2264e43-46a7-4484-bd40-e8c6de67cc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Start -----\n",
      "Allocated Memory: 0.00 MB\n",
      "Cached Memory: 2.00 MB\n",
      "----- Post defining data ------\n",
      "Allocated Memory: 0.00 MB\n",
      "Cached Memory: 2.00 MB\n",
      "----- Post Making Dataset / DataLoader ------\n",
      "Allocated Memory: 0.00 MB\n",
      "Cached Memory: 2.00 MB\n",
      "---- Post allocating model -------\n",
      "Allocated Memory: 0.78 MB\n",
      "Cached Memory: 2.00 MB\n",
      "---- Post allocating model functions (optimizer / loss)... -----\n",
      "Allocated Memory: 0.78 MB\n",
      "Cached Memory: 2.00 MB\n",
      "Epoch [1/50], Train Loss: 1.0647, Train Accuracy: 0.3803, Test Loss: 0.9247, Test Accuracy: 0.5892\n",
      "Allocated Memory: 3.10 MB\n",
      "Cached Memory: 20364.00 MB\n",
      "------ Evaluation Loop -------\n",
      "Allocated Memory: 3.10 MB\n",
      "Cached Memory: 20364.00 MB\n",
      "------ Cleanup -------\n",
      "Allocated Memory: 0.00 MB\n",
      "Cached Memory: 2.00 MB\n",
      "\n",
      "At the End\n",
      "Allocated Memory: 0.00 MB\n",
      "Cached Memory: 2.00 MB\n"
     ]
    }
   ],
   "source": [
    "# ### Testing memory leaks\n",
    "# print('---- Start -----')\n",
    "# print(f\"Allocated Memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "# print(f\"Cached Memory: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "\n",
    "# exp_name = f'b-one-head-{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "# # kf = KFold(n_splits=5, shuffle=True, random_state=25)\n",
    "# for i, (train_index, test_index) in enumerate(kf.split(X_balanced['hic'])):\n",
    "#     X_train_hic, X_test_hic = [X_balanced['hic'][i] for i in train_index], [X_balanced['hic'][i] for i in test_index]\n",
    "#     y_train, y_test = [y_balanced[i] for i in train_index], [y_balanced[i] for i in test_index]\n",
    "#     X_train_hic_normalized, X_test_hic_normalized = normalize_features(X_train_hic, X_test_hic)\n",
    "#     print(\"----- Post defining data ------\")\n",
    "#     print(f\"Allocated Memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "#     print(f\"Cached Memory: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "    \n",
    "#     input_dim = len(X_train_hic[0][0])\n",
    "#     train_dataset = GeneDataset(X_train_hic, y_train)\n",
    "#     test_dataset = GeneDataset(X_test_hic, y_test)\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "#     print(\"----- Post Making Dataset / DataLoader ------\")\n",
    "#     print(f\"Allocated Memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "#     print(f\"Cached Memory: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "\n",
    "#     model = OneHeadTransformerModel(input_dim, HIDDEN_DIM, OUTPUT_DIM, num_layers=NUM_LAYERS, num_heads=NUM_HEADS, dropout=DROPOUT).to(device)\n",
    "#     print(\"---- Post allocating model -------\")\n",
    "#     print(f\"Allocated Memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "#     print(f\"Cached Memory: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "\n",
    "#     # For imbalanced CEL\n",
    "#     class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "#     class_weights = torch.tensor(class_weights.astype(np.float32)).to(device)\n",
    "#     # weights = torch.tensor(np.unique(y_train, return_counts=True)[1], dtype=float).to(device)\n",
    "#     criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "#     # criterion = FocalLoss(gamma=2, alpha=0.1)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "#     lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=0.0001)\n",
    "\n",
    "#     print(\"---- Post allocating model functions (optimizer / loss)... -----\")\n",
    "#     print(f\"Allocated Memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "#     print(f\"Cached Memory: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "\n",
    "\n",
    "#     print(\"------ Training Loop -------\")\n",
    "#     train_accuracies = []\n",
    "#     test_accuracies = []\n",
    "#     train_losses = []\n",
    "#     test_losses = []\n",
    "#     early_stopper = EarlyStopper(patience=5, min_delta=0.1)\n",
    "#     for epoch in range(1):\n",
    "#         checkpoint(model, \"../logging/m_chkpt\")\n",
    "#         model.train()\n",
    "#         total_loss = 0\n",
    "#         train_correct = 0\n",
    "#         train_total = 0\n",
    "#         for x1, batch_y, mask1 in train_loader:\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(x1, mask1)\n",
    "#             loss = criterion(outputs, batch_y.squeeze())\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             total_loss += loss.item()\n",
    "#             train_correct += (outputs.argmax(dim=1) == batch_y.squeeze()).sum().item()\n",
    "#             train_total += batch_y.size(0)\n",
    "#             del x1, batch_y, mask1, loss, outputs\n",
    "#         lr_scheduler.step()\n",
    "        \n",
    "#         model.eval()\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "#         test_loss = 0\n",
    "#         with torch.no_grad():\n",
    "#             for x1, batch_y, mask1 in test_loader:\n",
    "#                 outputs = model(x1, mask1)\n",
    "#                 _, predicted = torch.max(outputs.data, 1)\n",
    "#                 test_loss += criterion(outputs, batch_y.squeeze()).item()\n",
    "#                 total += batch_y.size(0)\n",
    "#                 correct += (predicted == batch_y.squeeze()).sum().item()\n",
    "#                 del x1, batch_y, mask1\n",
    "\n",
    "#         accuracy = correct / total\n",
    "#         if early_stopper.early_stop( test_loss / len(test_loader) ): \n",
    "#             load_chkpt(model, \"../logging/m_chkpt\")\n",
    "#             print(\"Early Stop\")\n",
    "#             print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {total_loss/len(train_loader):.4f}, Train Accuracy: {train_correct/train_total:.4f}, Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {accuracy:.4f}')\n",
    "#             print()\n",
    "#             break\n",
    "#         train_accuracies.append(train_correct/train_total)\n",
    "#         train_losses.append(total_loss/len(train_loader))\n",
    "#         test_accuracies.append(accuracy)\n",
    "#         test_losses.append(test_loss / len(test_loader))\n",
    "#         print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {total_loss/len(train_loader):.4f}, Train Accuracy: {train_correct/train_total:.4f}, Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {accuracy:.4f}')\n",
    "#         del total_loss, train_correct, train_total, correct, total, test_loss\n",
    "\n",
    "#     print(f\"Allocated Memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "#     print(f\"Cached Memory: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "\n",
    "#     print(\"------ Evaluation Loop -------\")\n",
    "#     model.eval()\n",
    "#     all_predictions = []\n",
    "#     all_labels = []\n",
    "#     with torch.no_grad():\n",
    "#         for x1, batch_y, mask1, in test_loader:\n",
    "#             outputs = model(x1, mask1)\n",
    "#             _, predicted = torch.max(outputs.detach().cpu().data, 1)\n",
    "#             all_predictions.extend(predicted.numpy())\n",
    "#             all_labels.extend(batch_y.detach().cpu().numpy())\n",
    "#             del x1, batch_y, mask1\n",
    "#     print(f\"Allocated Memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "#     print(f\"Cached Memory: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "    \n",
    "#     print(\"------ Cleanup -------\")\n",
    "#     del model\n",
    "#     del class_weights, criterion, optimizer, lr_scheduler\n",
    "#     del X_train_hic, X_test_hic, X_train_hic_normalized, X_test_hic_normalized, y_train, y_test\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "#     print(f\"Allocated Memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "#     print(f\"Cached Memory: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    \n",
    "#     break\n",
    "#     # del model\n",
    "#     # del elem_stats, epoch_stats\n",
    "#     # del X_train_hic, X_test_hic, X_train_hic_normalized, X_test_hic_normalized, y_train, y_test\n",
    "\n",
    "#     # gc.collect()\n",
    "#     # torch.cuda.empty_cache()\n",
    "\n",
    "# print()\n",
    "# print(\"At the End\")\n",
    "# print(f\"Allocated Memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "# print(f\"Cached Memory: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f02817eb-0768-4df7-bc14-bdd55505c37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated Memory: 0.00 MB\n",
      "Cached Memory: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Allocated Memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"Cached Memory: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c923f151-8baa-4fd0-a159-d19cfcb2f951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 1.1227, Train Accuracy: 0.4026, Test Loss: 1.0401, Test Accuracy: 0.6793\n",
      "Epoch [2/50], Train Loss: 1.0799, Train Accuracy: 0.4267, Test Loss: 1.0593, Test Accuracy: 0.2225\n",
      "Epoch [3/50], Train Loss: 1.0698, Train Accuracy: 0.4396, Test Loss: 1.0200, Test Accuracy: 0.6793\n",
      "Epoch [4/50], Train Loss: 1.0656, Train Accuracy: 0.4337, Test Loss: 1.1359, Test Accuracy: 0.1847\n",
      "Epoch [5/50], Train Loss: 1.0589, Train Accuracy: 0.4312, Test Loss: 0.9818, Test Accuracy: 0.6919\n",
      "Epoch [6/50], Train Loss: 1.0527, Train Accuracy: 0.4389, Test Loss: 0.9672, Test Accuracy: 0.7117\n",
      "Epoch [7/50], Train Loss: 1.0569, Train Accuracy: 0.4583, Test Loss: 0.9972, Test Accuracy: 0.6775\n",
      "Epoch [8/50], Train Loss: 1.0592, Train Accuracy: 0.4493, Test Loss: 1.0685, Test Accuracy: 0.1892\n",
      "Epoch [9/50], Train Loss: 1.0489, Train Accuracy: 0.4675, Test Loss: 1.0628, Test Accuracy: 0.1856\n",
      "Epoch [10/50], Train Loss: 1.0448, Train Accuracy: 0.4031, Test Loss: 1.0483, Test Accuracy: 0.1838\n",
      "Epoch [11/50], Train Loss: 1.0442, Train Accuracy: 0.5032, Test Loss: 0.9816, Test Accuracy: 0.6937\n",
      "Epoch [12/50], Train Loss: 1.0464, Train Accuracy: 0.4673, Test Loss: 1.1300, Test Accuracy: 0.1108\n",
      "Epoch [13/50], Train Loss: 1.0428, Train Accuracy: 0.4977, Test Loss: 1.0230, Test Accuracy: 0.6820\n",
      "Epoch [14/50], Train Loss: 1.0436, Train Accuracy: 0.5119, Test Loss: 1.0233, Test Accuracy: 0.6874\n",
      "Epoch [15/50], Train Loss: 1.0437, Train Accuracy: 0.4653, Test Loss: 1.0061, Test Accuracy: 0.6865\n",
      "Epoch [16/50], Train Loss: 1.0384, Train Accuracy: 0.4497, Test Loss: 1.0203, Test Accuracy: 0.1910\n",
      "Epoch [17/50], Train Loss: 1.0408, Train Accuracy: 0.5014, Test Loss: 1.0145, Test Accuracy: 0.6856\n",
      "Epoch [18/50], Train Loss: 1.0332, Train Accuracy: 0.4646, Test Loss: 1.0883, Test Accuracy: 0.1189\n",
      "Epoch [19/50], Train Loss: 1.0370, Train Accuracy: 0.4333, Test Loss: 1.0338, Test Accuracy: 0.1901\n",
      "Epoch [20/50], Train Loss: 1.0359, Train Accuracy: 0.4817, Test Loss: 1.0654, Test Accuracy: 0.1946\n",
      "Epoch [21/50], Train Loss: 1.0322, Train Accuracy: 0.4504, Test Loss: 1.0081, Test Accuracy: 0.1901\n",
      "Epoch [22/50], Train Loss: 1.0350, Train Accuracy: 0.4682, Test Loss: 0.9627, Test Accuracy: 0.7009\n",
      "Epoch [23/50], Train Loss: 1.0314, Train Accuracy: 0.5331, Test Loss: 0.9764, Test Accuracy: 0.6919\n",
      "Epoch [24/50], Train Loss: 1.0308, Train Accuracy: 0.4975, Test Loss: 1.0319, Test Accuracy: 0.1964\n",
      "Epoch [25/50], Train Loss: 1.0301, Train Accuracy: 0.5500, Test Loss: 1.0305, Test Accuracy: 0.2108\n",
      "Epoch [26/50], Train Loss: 1.0300, Train Accuracy: 0.4403, Test Loss: 0.9807, Test Accuracy: 0.6973\n",
      "Epoch [27/50], Train Loss: 1.0294, Train Accuracy: 0.6247, Test Loss: 1.0015, Test Accuracy: 0.7018\n",
      "Epoch [28/50], Train Loss: 1.0279, Train Accuracy: 0.4948, Test Loss: 0.9899, Test Accuracy: 0.7036\n",
      "Epoch [29/50], Train Loss: 1.0220, Train Accuracy: 0.5787, Test Loss: 0.9953, Test Accuracy: 0.7036\n",
      "Epoch [30/50], Train Loss: 1.0226, Train Accuracy: 0.4576, Test Loss: 0.9747, Test Accuracy: 0.6892\n",
      "Epoch [31/50], Train Loss: 1.0265, Train Accuracy: 0.5352, Test Loss: 1.0185, Test Accuracy: 0.6991\n",
      "Epoch [32/50], Train Loss: 1.0218, Train Accuracy: 0.5588, Test Loss: 1.0178, Test Accuracy: 0.1973\n",
      "Epoch [33/50], Train Loss: 1.0205, Train Accuracy: 0.5304, Test Loss: 1.0187, Test Accuracy: 0.6991\n",
      "Epoch [34/50], Train Loss: 1.0217, Train Accuracy: 0.6422, Test Loss: 0.9913, Test Accuracy: 0.7018\n",
      "Epoch [35/50], Train Loss: 1.0195, Train Accuracy: 0.5633, Test Loss: 1.0006, Test Accuracy: 0.7027\n",
      "Epoch [36/50], Train Loss: 1.0163, Train Accuracy: 0.5796, Test Loss: 1.0169, Test Accuracy: 0.2009\n",
      "Epoch [37/50], Train Loss: 1.0179, Train Accuracy: 0.4691, Test Loss: 0.9912, Test Accuracy: 0.7009\n",
      "Epoch [38/50], Train Loss: 1.0158, Train Accuracy: 0.6558, Test Loss: 1.0160, Test Accuracy: 0.7045\n",
      "Epoch [39/50], Train Loss: 1.0157, Train Accuracy: 0.5726, Test Loss: 1.0135, Test Accuracy: 0.2018\n",
      "Epoch [40/50], Train Loss: 1.0110, Train Accuracy: 0.4784, Test Loss: 0.9980, Test Accuracy: 0.7018\n",
      "Epoch [41/50], Train Loss: 1.0137, Train Accuracy: 0.6249, Test Loss: 1.0101, Test Accuracy: 0.7054\n",
      "Epoch [42/50], Train Loss: 1.0143, Train Accuracy: 0.6348, Test Loss: 1.0118, Test Accuracy: 0.7036\n",
      "Epoch [43/50], Train Loss: 1.0129, Train Accuracy: 0.5904, Test Loss: 1.0153, Test Accuracy: 0.7027\n",
      "Epoch [44/50], Train Loss: 1.0075, Train Accuracy: 0.5246, Test Loss: 1.0013, Test Accuracy: 0.7054\n",
      "Epoch [45/50], Train Loss: 1.0083, Train Accuracy: 0.6668, Test Loss: 1.0151, Test Accuracy: 0.7063\n",
      "Epoch [46/50], Train Loss: 1.0029, Train Accuracy: 0.5309, Test Loss: 1.0105, Test Accuracy: 0.7090\n",
      "Epoch [47/50], Train Loss: 1.0095, Train Accuracy: 0.5969, Test Loss: 1.0150, Test Accuracy: 0.7054\n",
      "Epoch [48/50], Train Loss: 1.0015, Train Accuracy: 0.5931, Test Loss: 1.0114, Test Accuracy: 0.7063\n",
      "Epoch [49/50], Train Loss: 1.0018, Train Accuracy: 0.5435, Test Loss: 1.0190, Test Accuracy: 0.7063\n",
      "Epoch [50/50], Train Loss: 0.9983, Train Accuracy: 0.5410, Test Loss: 1.0108, Test Accuracy: 0.7054\n",
      "Epoch [1/50], Train Loss: 1.1288, Train Accuracy: 0.3741, Test Loss: 1.0551, Test Accuracy: 0.1912\n",
      "Epoch [2/50], Train Loss: 1.0730, Train Accuracy: 0.4043, Test Loss: 1.0746, Test Accuracy: 0.6510\n",
      "Epoch [3/50], Train Loss: 1.0717, Train Accuracy: 0.3872, Test Loss: 0.9884, Test Accuracy: 0.6555\n",
      "Epoch [4/50], Train Loss: 1.0636, Train Accuracy: 0.4147, Test Loss: 1.0700, Test Accuracy: 0.1858\n",
      "Epoch [5/50], Train Loss: 1.0534, Train Accuracy: 0.4424, Test Loss: 1.0029, Test Accuracy: 0.6528\n",
      "Epoch [6/50], Train Loss: 1.0490, Train Accuracy: 0.4255, Test Loss: 0.9755, Test Accuracy: 0.6880\n",
      "Epoch [7/50], Train Loss: 1.0522, Train Accuracy: 0.4884, Test Loss: 0.9965, Test Accuracy: 0.6907\n",
      "Epoch [8/50], Train Loss: 1.0477, Train Accuracy: 0.4514, Test Loss: 0.9836, Test Accuracy: 0.6790\n",
      "Epoch [9/50], Train Loss: 1.0377, Train Accuracy: 0.4032, Test Loss: 1.0656, Test Accuracy: 0.2011\n",
      "Epoch [10/50], Train Loss: 1.0450, Train Accuracy: 0.4305, Test Loss: 1.0030, Test Accuracy: 0.6682\n",
      "Epoch [11/50], Train Loss: 1.0311, Train Accuracy: 0.4719, Test Loss: 1.0142, Test Accuracy: 0.2056\n",
      "Epoch [12/50], Train Loss: 1.0366, Train Accuracy: 0.4544, Test Loss: 1.0480, Test Accuracy: 0.1984\n",
      "Epoch [13/50], Train Loss: 1.0353, Train Accuracy: 0.5030, Test Loss: 1.0418, Test Accuracy: 0.1957\n",
      "Epoch [14/50], Train Loss: 1.0302, Train Accuracy: 0.4478, Test Loss: 1.0554, Test Accuracy: 0.1993\n",
      "Epoch [15/50], Train Loss: 1.0286, Train Accuracy: 0.4508, Test Loss: 1.0967, Test Accuracy: 0.1939\n",
      "Epoch [16/50], Train Loss: 1.0266, Train Accuracy: 0.4361, Test Loss: 1.0590, Test Accuracy: 0.2011\n",
      "Epoch [17/50], Train Loss: 1.0316, Train Accuracy: 0.4260, Test Loss: 1.0360, Test Accuracy: 0.2020\n",
      "Epoch [18/50], Train Loss: 1.0233, Train Accuracy: 0.4104, Test Loss: 1.0526, Test Accuracy: 0.1921\n",
      "Epoch [19/50], Train Loss: 1.0227, Train Accuracy: 0.4404, Test Loss: 1.0289, Test Accuracy: 0.6655\n",
      "Epoch [20/50], Train Loss: 1.0195, Train Accuracy: 0.4841, Test Loss: 1.0240, Test Accuracy: 0.2038\n",
      "Epoch [21/50], Train Loss: 1.0228, Train Accuracy: 0.5707, Test Loss: 1.0185, Test Accuracy: 0.6691\n",
      "Epoch [22/50], Train Loss: 1.0132, Train Accuracy: 0.4819, Test Loss: 1.0335, Test Accuracy: 0.6628\n",
      "Epoch [23/50], Train Loss: 1.0187, Train Accuracy: 0.4257, Test Loss: 1.0581, Test Accuracy: 0.1993\n",
      "Epoch [24/50], Train Loss: 1.0219, Train Accuracy: 0.4672, Test Loss: 1.0508, Test Accuracy: 0.2020\n",
      "Epoch [25/50], Train Loss: 1.0157, Train Accuracy: 0.4654, Test Loss: 1.0175, Test Accuracy: 0.6664\n",
      "Epoch [26/50], Train Loss: 1.0268, Train Accuracy: 0.4837, Test Loss: 1.0523, Test Accuracy: 0.1984\n",
      "Epoch [27/50], Train Loss: 1.0226, Train Accuracy: 0.4138, Test Loss: 1.0049, Test Accuracy: 0.6718\n",
      "Epoch [28/50], Train Loss: 1.0136, Train Accuracy: 0.4985, Test Loss: 1.0344, Test Accuracy: 0.6682\n",
      "Epoch [29/50], Train Loss: 1.0124, Train Accuracy: 0.4670, Test Loss: 1.0260, Test Accuracy: 0.6736\n",
      "Epoch [30/50], Train Loss: 1.0110, Train Accuracy: 0.4602, Test Loss: 1.0296, Test Accuracy: 0.6709\n",
      "Epoch [31/50], Train Loss: 1.0065, Train Accuracy: 0.4595, Test Loss: 1.0418, Test Accuracy: 0.2038\n",
      "Epoch [32/50], Train Loss: 1.0119, Train Accuracy: 0.4924, Test Loss: 1.0528, Test Accuracy: 0.1993\n",
      "Epoch [33/50], Train Loss: 1.0057, Train Accuracy: 0.5030, Test Loss: 1.0200, Test Accuracy: 0.6727\n",
      "Epoch [34/50], Train Loss: 1.0039, Train Accuracy: 0.5287, Test Loss: 1.0387, Test Accuracy: 0.6673\n",
      "Epoch [35/50], Train Loss: 1.0046, Train Accuracy: 0.4877, Test Loss: 1.0302, Test Accuracy: 0.6601\n",
      "Epoch [36/50], Train Loss: 1.0059, Train Accuracy: 0.4616, Test Loss: 1.0114, Test Accuracy: 0.6736\n",
      "Epoch [37/50], Train Loss: 1.0028, Train Accuracy: 0.5668, Test Loss: 1.0476, Test Accuracy: 0.2056\n",
      "Epoch [38/50], Train Loss: 1.0002, Train Accuracy: 0.4979, Test Loss: 1.0506, Test Accuracy: 0.2092\n",
      "Epoch [39/50], Train Loss: 0.9983, Train Accuracy: 0.4821, Test Loss: 1.0237, Test Accuracy: 0.6817\n",
      "Epoch [40/50], Train Loss: 0.9969, Train Accuracy: 0.5490, Test Loss: 1.0566, Test Accuracy: 0.2056\n",
      "Epoch [41/50], Train Loss: 0.9985, Train Accuracy: 0.4212, Test Loss: 1.0391, Test Accuracy: 0.2092\n",
      "Epoch [42/50], Train Loss: 0.9928, Train Accuracy: 0.4679, Test Loss: 1.0279, Test Accuracy: 0.6745\n",
      "Epoch [43/50], Train Loss: 1.0002, Train Accuracy: 0.6380, Test Loss: 1.0450, Test Accuracy: 0.2110\n",
      "Epoch [44/50], Train Loss: 0.9920, Train Accuracy: 0.4697, Test Loss: 1.0362, Test Accuracy: 0.6763\n",
      "Epoch [45/50], Train Loss: 0.9896, Train Accuracy: 0.4372, Test Loss: 1.0296, Test Accuracy: 0.6763\n",
      "Epoch [46/50], Train Loss: 0.9915, Train Accuracy: 0.6480, Test Loss: 1.0490, Test Accuracy: 0.6754\n",
      "Epoch [47/50], Train Loss: 0.9847, Train Accuracy: 0.4816, Test Loss: 1.0608, Test Accuracy: 0.6736\n",
      "Epoch [48/50], Train Loss: 0.9884, Train Accuracy: 0.5614, Test Loss: 1.0543, Test Accuracy: 0.6754\n",
      "Epoch [49/50], Train Loss: 0.9848, Train Accuracy: 0.6344, Test Loss: 1.0406, Test Accuracy: 0.6754\n",
      "Epoch [50/50], Train Loss: 0.9874, Train Accuracy: 0.3626, Test Loss: 1.0475, Test Accuracy: 0.6772\n",
      "Epoch [1/50], Train Loss: 1.1011, Train Accuracy: 0.4091, Test Loss: 1.3425, Test Accuracy: 0.1641\n",
      "Epoch [2/50], Train Loss: 1.0695, Train Accuracy: 0.3926, Test Loss: 0.9423, Test Accuracy: 0.6628\n",
      "Epoch [3/50], Train Loss: 1.0609, Train Accuracy: 0.4052, Test Loss: 1.0108, Test Accuracy: 0.2101\n",
      "Epoch [4/50], Train Loss: 1.0624, Train Accuracy: 0.4253, Test Loss: 1.0654, Test Accuracy: 0.1767\n",
      "Epoch [5/50], Train Loss: 1.0549, Train Accuracy: 0.4115, Test Loss: 1.1443, Test Accuracy: 0.1767\n",
      "Epoch [6/50], Train Loss: 1.0496, Train Accuracy: 0.4625, Test Loss: 1.0509, Test Accuracy: 0.1867\n",
      "Epoch [7/50], Train Loss: 1.0500, Train Accuracy: 0.3870, Test Loss: 0.9901, Test Accuracy: 0.6691\n",
      "Epoch [8/50], Train Loss: 1.0425, Train Accuracy: 0.4805, Test Loss: 1.0385, Test Accuracy: 0.1867\n",
      "Epoch [9/50], Train Loss: 1.0369, Train Accuracy: 0.4061, Test Loss: 0.9554, Test Accuracy: 0.7187\n",
      "Epoch [10/50], Train Loss: 1.0351, Train Accuracy: 0.4785, Test Loss: 1.1516, Test Accuracy: 0.1858\n",
      "Epoch [11/50], Train Loss: 1.0341, Train Accuracy: 0.4102, Test Loss: 1.0093, Test Accuracy: 0.7169\n",
      "Early Stop\n",
      "Epoch [12/50], Train Loss: 1.0331, Train Accuracy: 0.4722, Test Loss: 1.0549, Test Accuracy: 0.6573\n",
      "\n",
      "Epoch [1/50], Train Loss: 1.1486, Train Accuracy: 0.3953, Test Loss: 1.0536, Test Accuracy: 0.1767\n",
      "Epoch [2/50], Train Loss: 1.0830, Train Accuracy: 0.3973, Test Loss: 1.0940, Test Accuracy: 0.1785\n",
      "Epoch [3/50], Train Loss: 1.0649, Train Accuracy: 0.3919, Test Loss: 0.9685, Test Accuracy: 0.6691\n",
      "Epoch [4/50], Train Loss: 1.0654, Train Accuracy: 0.4415, Test Loss: 1.0523, Test Accuracy: 0.6691\n",
      "Epoch [5/50], Train Loss: 1.0614, Train Accuracy: 0.4440, Test Loss: 0.9464, Test Accuracy: 0.6691\n",
      "Epoch [6/50], Train Loss: 1.0587, Train Accuracy: 0.4600, Test Loss: 1.0043, Test Accuracy: 0.7151\n",
      "Epoch [7/50], Train Loss: 1.0519, Train Accuracy: 0.4801, Test Loss: 0.9870, Test Accuracy: 0.6835\n",
      "Epoch [8/50], Train Loss: 1.0514, Train Accuracy: 0.4305, Test Loss: 1.0266, Test Accuracy: 0.6736\n",
      "Epoch [9/50], Train Loss: 1.0491, Train Accuracy: 0.4672, Test Loss: 1.0444, Test Accuracy: 0.1876\n",
      "Epoch [10/50], Train Loss: 1.0465, Train Accuracy: 0.4248, Test Loss: 1.1388, Test Accuracy: 0.1812\n",
      "Epoch [11/50], Train Loss: 1.0540, Train Accuracy: 0.4625, Test Loss: 1.0058, Test Accuracy: 0.6862\n",
      "Epoch [12/50], Train Loss: 1.0428, Train Accuracy: 0.4787, Test Loss: 1.0234, Test Accuracy: 0.1876\n",
      "Epoch [13/50], Train Loss: 1.0432, Train Accuracy: 0.4598, Test Loss: 1.0463, Test Accuracy: 0.1858\n",
      "Epoch [14/50], Train Loss: 1.0407, Train Accuracy: 0.4350, Test Loss: 1.0196, Test Accuracy: 0.6754\n",
      "Epoch [15/50], Train Loss: 1.0426, Train Accuracy: 0.4246, Test Loss: 1.0471, Test Accuracy: 0.1794\n",
      "Epoch [16/50], Train Loss: 1.0406, Train Accuracy: 0.4663, Test Loss: 1.0255, Test Accuracy: 0.1858\n",
      "Epoch [17/50], Train Loss: 1.0419, Train Accuracy: 0.4271, Test Loss: 0.9895, Test Accuracy: 0.6871\n",
      "Epoch [18/50], Train Loss: 1.0356, Train Accuracy: 0.4832, Test Loss: 1.0053, Test Accuracy: 0.6817\n",
      "Epoch [19/50], Train Loss: 1.0344, Train Accuracy: 0.4983, Test Loss: 1.0107, Test Accuracy: 0.6799\n",
      "Epoch [20/50], Train Loss: 1.0384, Train Accuracy: 0.4888, Test Loss: 1.0462, Test Accuracy: 0.1966\n",
      "Epoch [21/50], Train Loss: 1.0310, Train Accuracy: 0.4990, Test Loss: 1.0527, Test Accuracy: 0.2038\n",
      "Epoch [22/50], Train Loss: 1.0294, Train Accuracy: 0.5145, Test Loss: 1.0063, Test Accuracy: 0.6934\n",
      "Epoch [23/50], Train Loss: 1.0286, Train Accuracy: 0.4848, Test Loss: 1.0036, Test Accuracy: 0.6898\n",
      "Epoch [24/50], Train Loss: 1.0279, Train Accuracy: 0.4949, Test Loss: 1.0277, Test Accuracy: 0.6880\n",
      "Epoch [25/50], Train Loss: 1.0283, Train Accuracy: 0.5808, Test Loss: 1.0667, Test Accuracy: 0.1903\n",
      "Epoch [26/50], Train Loss: 1.0267, Train Accuracy: 0.4931, Test Loss: 0.9945, Test Accuracy: 0.6997\n",
      "Epoch [27/50], Train Loss: 1.0277, Train Accuracy: 0.5231, Test Loss: 1.0063, Test Accuracy: 0.6943\n",
      "Epoch [28/50], Train Loss: 1.0231, Train Accuracy: 0.6362, Test Loss: 1.0431, Test Accuracy: 0.1984\n",
      "Epoch [29/50], Train Loss: 1.0245, Train Accuracy: 0.5308, Test Loss: 1.0170, Test Accuracy: 0.6889\n",
      "Epoch [30/50], Train Loss: 1.0228, Train Accuracy: 0.4652, Test Loss: 1.0012, Test Accuracy: 0.6907\n",
      "Epoch [31/50], Train Loss: 1.0208, Train Accuracy: 0.5846, Test Loss: 1.0015, Test Accuracy: 0.6952\n",
      "Epoch [32/50], Train Loss: 1.0195, Train Accuracy: 0.5181, Test Loss: 0.9918, Test Accuracy: 0.6880\n",
      "Epoch [33/50], Train Loss: 1.0184, Train Accuracy: 0.6099, Test Loss: 0.9981, Test Accuracy: 0.6943\n",
      "Epoch [34/50], Train Loss: 1.0133, Train Accuracy: 0.5535, Test Loss: 1.0212, Test Accuracy: 0.2029\n",
      "Epoch [35/50], Train Loss: 1.0197, Train Accuracy: 0.4512, Test Loss: 1.0056, Test Accuracy: 0.7033\n",
      "Epoch [36/50], Train Loss: 1.0178, Train Accuracy: 0.5069, Test Loss: 0.9842, Test Accuracy: 0.6952\n",
      "Epoch [37/50], Train Loss: 1.0108, Train Accuracy: 0.5864, Test Loss: 1.0275, Test Accuracy: 0.2065\n",
      "Epoch [38/50], Train Loss: 1.0119, Train Accuracy: 0.5497, Test Loss: 1.0336, Test Accuracy: 0.2056\n",
      "Epoch [39/50], Train Loss: 1.0123, Train Accuracy: 0.5806, Test Loss: 1.0016, Test Accuracy: 0.7006\n",
      "Epoch [40/50], Train Loss: 1.0104, Train Accuracy: 0.4909, Test Loss: 0.9968, Test Accuracy: 0.6970\n",
      "Epoch [41/50], Train Loss: 1.0077, Train Accuracy: 0.6558, Test Loss: 1.0233, Test Accuracy: 0.2074\n",
      "Epoch [42/50], Train Loss: 1.0123, Train Accuracy: 0.4819, Test Loss: 1.0092, Test Accuracy: 0.7024\n",
      "Epoch [43/50], Train Loss: 1.0050, Train Accuracy: 0.5921, Test Loss: 1.0098, Test Accuracy: 0.6961\n",
      "Epoch [44/50], Train Loss: 1.0070, Train Accuracy: 0.5619, Test Loss: 1.0262, Test Accuracy: 0.6925\n",
      "Epoch [45/50], Train Loss: 1.0031, Train Accuracy: 0.6369, Test Loss: 1.0201, Test Accuracy: 0.6943\n",
      "Epoch [46/50], Train Loss: 1.0038, Train Accuracy: 0.5984, Test Loss: 1.0261, Test Accuracy: 0.2056\n",
      "Epoch [47/50], Train Loss: 1.0027, Train Accuracy: 0.5432, Test Loss: 1.0199, Test Accuracy: 0.6943\n",
      "Epoch [48/50], Train Loss: 1.0036, Train Accuracy: 0.5114, Test Loss: 1.0235, Test Accuracy: 0.6943\n",
      "Epoch [49/50], Train Loss: 1.0046, Train Accuracy: 0.6371, Test Loss: 1.0361, Test Accuracy: 0.6952\n",
      "Epoch [50/50], Train Loss: 1.0000, Train Accuracy: 0.5583, Test Loss: 1.0344, Test Accuracy: 0.6961\n",
      "Epoch [1/50], Train Loss: 1.1202, Train Accuracy: 0.3928, Test Loss: 0.9706, Test Accuracy: 0.6546\n",
      "Epoch [2/50], Train Loss: 1.0741, Train Accuracy: 0.4176, Test Loss: 1.0208, Test Accuracy: 0.6537\n",
      "Epoch [3/50], Train Loss: 1.0663, Train Accuracy: 0.4399, Test Loss: 1.0771, Test Accuracy: 0.1704\n",
      "Epoch [4/50], Train Loss: 1.0589, Train Accuracy: 0.4158, Test Loss: 1.0337, Test Accuracy: 0.6537\n",
      "Epoch [5/50], Train Loss: 1.0457, Train Accuracy: 0.4433, Test Loss: 1.1125, Test Accuracy: 0.1821\n",
      "Epoch [6/50], Train Loss: 1.0584, Train Accuracy: 0.4339, Test Loss: 1.0424, Test Accuracy: 0.1858\n",
      "Epoch [7/50], Train Loss: 1.0547, Train Accuracy: 0.3931, Test Loss: 1.0539, Test Accuracy: 0.1812\n",
      "Epoch [8/50], Train Loss: 1.0446, Train Accuracy: 0.4237, Test Loss: 1.0725, Test Accuracy: 0.6619\n",
      "Epoch [9/50], Train Loss: 1.0457, Train Accuracy: 0.4717, Test Loss: 0.9791, Test Accuracy: 0.6907\n",
      "Epoch [10/50], Train Loss: 1.0374, Train Accuracy: 0.4269, Test Loss: 1.1441, Test Accuracy: 0.1028\n",
      "Epoch [11/50], Train Loss: 1.0334, Train Accuracy: 0.4251, Test Loss: 1.0079, Test Accuracy: 0.6826\n",
      "Epoch [12/50], Train Loss: 1.0365, Train Accuracy: 0.4740, Test Loss: 1.0618, Test Accuracy: 0.6628\n",
      "Epoch [13/50], Train Loss: 1.0400, Train Accuracy: 0.4566, Test Loss: 0.9943, Test Accuracy: 0.6952\n",
      "Epoch [14/50], Train Loss: 1.0313, Train Accuracy: 0.4541, Test Loss: 1.0032, Test Accuracy: 0.7006\n",
      "Epoch [15/50], Train Loss: 1.0277, Train Accuracy: 0.4215, Test Loss: 0.9632, Test Accuracy: 0.7006\n",
      "Epoch [16/50], Train Loss: 1.0348, Train Accuracy: 0.4494, Test Loss: 1.0339, Test Accuracy: 0.6646\n",
      "Epoch [17/50], Train Loss: 1.0288, Train Accuracy: 0.4546, Test Loss: 1.0291, Test Accuracy: 0.1885\n",
      "Epoch [18/50], Train Loss: 1.0260, Train Accuracy: 0.4257, Test Loss: 1.0688, Test Accuracy: 0.1830\n",
      "Epoch [19/50], Train Loss: 1.0280, Train Accuracy: 0.4269, Test Loss: 1.0420, Test Accuracy: 0.1867\n",
      "Epoch [20/50], Train Loss: 1.0304, Train Accuracy: 0.4936, Test Loss: 1.0165, Test Accuracy: 0.2020\n",
      "Epoch [21/50], Train Loss: 1.0358, Train Accuracy: 0.3910, Test Loss: 1.0141, Test Accuracy: 0.1966\n",
      "Epoch [22/50], Train Loss: 1.0199, Train Accuracy: 0.4949, Test Loss: 1.0581, Test Accuracy: 0.1803\n",
      "Epoch [23/50], Train Loss: 1.0245, Train Accuracy: 0.4519, Test Loss: 1.0382, Test Accuracy: 0.2047\n",
      "Epoch [24/50], Train Loss: 1.0237, Train Accuracy: 0.5109, Test Loss: 1.0522, Test Accuracy: 0.2011\n",
      "Epoch [25/50], Train Loss: 1.0196, Train Accuracy: 0.4012, Test Loss: 1.0315, Test Accuracy: 0.2011\n",
      "Epoch [26/50], Train Loss: 1.0213, Train Accuracy: 0.4767, Test Loss: 1.0567, Test Accuracy: 0.1894\n",
      "Epoch [27/50], Train Loss: 1.0181, Train Accuracy: 0.4834, Test Loss: 1.0327, Test Accuracy: 0.2020\n",
      "Epoch [28/50], Train Loss: 1.0202, Train Accuracy: 0.4291, Test Loss: 1.0340, Test Accuracy: 0.1930\n",
      "Epoch [29/50], Train Loss: 1.0196, Train Accuracy: 0.4952, Test Loss: 1.0543, Test Accuracy: 0.6763\n",
      "Epoch [30/50], Train Loss: 1.0149, Train Accuracy: 0.5281, Test Loss: 1.0380, Test Accuracy: 0.1984\n",
      "Epoch [31/50], Train Loss: 1.0188, Train Accuracy: 0.5535, Test Loss: 1.0123, Test Accuracy: 0.6889\n",
      "Epoch [32/50], Train Loss: 1.0162, Train Accuracy: 0.4613, Test Loss: 1.0264, Test Accuracy: 0.6835\n",
      "Epoch [33/50], Train Loss: 1.0125, Train Accuracy: 0.4573, Test Loss: 1.0123, Test Accuracy: 0.6745\n",
      "Epoch [34/50], Train Loss: 1.0112, Train Accuracy: 0.5353, Test Loss: 1.0310, Test Accuracy: 0.6835\n",
      "Epoch [35/50], Train Loss: 1.0088, Train Accuracy: 0.5308, Test Loss: 1.0452, Test Accuracy: 0.2029\n",
      "Epoch [36/50], Train Loss: 1.0122, Train Accuracy: 0.5033, Test Loss: 1.0426, Test Accuracy: 0.2029\n",
      "Epoch [37/50], Train Loss: 1.0053, Train Accuracy: 0.4918, Test Loss: 1.0447, Test Accuracy: 0.1984\n",
      "Epoch [38/50], Train Loss: 1.0069, Train Accuracy: 0.3856, Test Loss: 1.0263, Test Accuracy: 0.6907\n",
      "Epoch [39/50], Train Loss: 1.0038, Train Accuracy: 0.5053, Test Loss: 1.0461, Test Accuracy: 0.2029\n",
      "Epoch [40/50], Train Loss: 1.0030, Train Accuracy: 0.4503, Test Loss: 1.0163, Test Accuracy: 0.6889\n",
      "Epoch [41/50], Train Loss: 1.0036, Train Accuracy: 0.5402, Test Loss: 1.0294, Test Accuracy: 0.2074\n",
      "Epoch [42/50], Train Loss: 1.0039, Train Accuracy: 0.4796, Test Loss: 1.0320, Test Accuracy: 0.6889\n",
      "Epoch [43/50], Train Loss: 0.9994, Train Accuracy: 0.5217, Test Loss: 1.0424, Test Accuracy: 0.2047\n",
      "Epoch [44/50], Train Loss: 1.0027, Train Accuracy: 0.5506, Test Loss: 1.0328, Test Accuracy: 0.2020\n",
      "Epoch [45/50], Train Loss: 0.9993, Train Accuracy: 0.5887, Test Loss: 1.0203, Test Accuracy: 0.6898\n",
      "Epoch [46/50], Train Loss: 0.9945, Train Accuracy: 0.5623, Test Loss: 1.0267, Test Accuracy: 0.6880\n",
      "Epoch [47/50], Train Loss: 0.9935, Train Accuracy: 0.5860, Test Loss: 1.0366, Test Accuracy: 0.2047\n",
      "Epoch [48/50], Train Loss: 0.9933, Train Accuracy: 0.4553, Test Loss: 1.0238, Test Accuracy: 0.6907\n",
      "Epoch [49/50], Train Loss: 0.9910, Train Accuracy: 0.5794, Test Loss: 1.0436, Test Accuracy: 0.6907\n",
      "Epoch [50/50], Train Loss: 0.9926, Train Accuracy: 0.5709, Test Loss: 1.0313, Test Accuracy: 0.6898\n"
     ]
    }
   ],
   "source": [
    "exp_name = f'b-one-head-{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X_balanced['mcg'])):\n",
    "    \n",
    "    X_train_mcg, X_test_mcg = [X_balanced['mcg'][i] for i in train_index], [X_balanced['mcg'][i] for i in test_index]\n",
    "    \n",
    "    y_train, y_test = [y_balanced[i] for i in train_index], [y_balanced[i] for i in test_index]\n",
    "    \n",
    "    X_train_mcg_normalized, X_test_mcg_normalized = normalize_features(X_train_mcg, X_test_mcg)\n",
    "    \n",
    "    elem_stats, epoch_stats, model = train_model(X_train_mcg_normalized, y_train,\n",
    "                                            X_test_mcg_normalized, y_test,\n",
    "                                            exp_name=exp_name, fold_idx=i)\n",
    "\n",
    "    df_elem = pd.DataFrame({'preds' : elem_stats[0], 'labels' : elem_stats[1]})\n",
    "    df_epoch = pd.DataFrame({'train_acc' : epoch_stats[0], 'train_loss' : epoch_stats[1], \n",
    "                             'test_acc' : epoch_stats[2], 'test_loss' : epoch_stats[3]})\n",
    "\n",
    "    df_elem.to_csv(f\"../baseline_results/{_ct}/mcg/imbal_elem_{i}.csv\")\n",
    "    df_epoch.to_csv(f\"../baseline_results/{_ct}/mcg/imbal_epoch_{i}.csv\")\n",
    "    \n",
    "    torch.save(model, f\"../baseline_results/{_ct}/mcg/imbal_model_{i}.pt\")\n",
    "    \n",
    "    del model\n",
    "    del elem_stats, epoch_stats\n",
    "    del X_train_mcg, X_test_mcg, X_train_mcg_normalized, X_test_mcg_normalized, y_train, y_test\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c5e6f44-0d91-45db-ba0b-e9bb5f75e2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 0.9968, Train Accuracy: 0.4083, Test Loss: 1.0025, Test Accuracy: 0.3712\n",
      "Epoch [2/50], Train Loss: 0.9062, Train Accuracy: 0.4502, Test Loss: 0.8521, Test Accuracy: 0.5919\n",
      "Epoch [3/50], Train Loss: 0.8533, Train Accuracy: 0.4840, Test Loss: 0.9850, Test Accuracy: 0.4712\n",
      "Epoch [4/50], Train Loss: 0.8235, Train Accuracy: 0.4966, Test Loss: 0.8246, Test Accuracy: 0.5505\n",
      "Epoch [5/50], Train Loss: 0.8088, Train Accuracy: 0.5038, Test Loss: 0.8382, Test Accuracy: 0.5559\n",
      "Epoch [6/50], Train Loss: 0.8097, Train Accuracy: 0.5104, Test Loss: 0.8622, Test Accuracy: 0.4793\n",
      "Epoch [7/50], Train Loss: 0.8004, Train Accuracy: 0.4964, Test Loss: 0.8381, Test Accuracy: 0.5297\n",
      "Epoch [8/50], Train Loss: 0.7730, Train Accuracy: 0.5345, Test Loss: 0.8238, Test Accuracy: 0.5694\n",
      "Epoch [9/50], Train Loss: 0.8016, Train Accuracy: 0.5119, Test Loss: 0.7657, Test Accuracy: 0.6559\n",
      "Epoch [10/50], Train Loss: 0.7781, Train Accuracy: 0.5214, Test Loss: 1.0000, Test Accuracy: 0.4225\n",
      "Epoch [11/50], Train Loss: 0.7742, Train Accuracy: 0.5417, Test Loss: 0.8943, Test Accuracy: 0.5604\n",
      "Epoch [12/50], Train Loss: 0.7753, Train Accuracy: 0.5300, Test Loss: 0.9456, Test Accuracy: 0.4541\n",
      "Epoch [13/50], Train Loss: 0.7658, Train Accuracy: 0.5309, Test Loss: 0.9016, Test Accuracy: 0.5243\n",
      "Epoch [14/50], Train Loss: 0.7562, Train Accuracy: 0.5476, Test Loss: 0.7543, Test Accuracy: 0.6450\n",
      "Epoch [15/50], Train Loss: 0.7585, Train Accuracy: 0.5518, Test Loss: 0.9306, Test Accuracy: 0.4784\n",
      "Epoch [16/50], Train Loss: 0.7429, Train Accuracy: 0.5595, Test Loss: 0.8695, Test Accuracy: 0.5180\n",
      "Epoch [17/50], Train Loss: 0.7541, Train Accuracy: 0.5568, Test Loss: 0.8330, Test Accuracy: 0.5468\n",
      "Epoch [18/50], Train Loss: 0.7517, Train Accuracy: 0.5579, Test Loss: 0.8267, Test Accuracy: 0.5297\n",
      "Epoch [19/50], Train Loss: 0.7420, Train Accuracy: 0.5647, Test Loss: 0.8837, Test Accuracy: 0.5153\n",
      "Epoch [20/50], Train Loss: 0.7377, Train Accuracy: 0.5528, Test Loss: 0.8388, Test Accuracy: 0.5297\n",
      "Epoch [21/50], Train Loss: 0.7419, Train Accuracy: 0.5489, Test Loss: 0.7696, Test Accuracy: 0.6198\n",
      "Epoch [22/50], Train Loss: 0.7362, Train Accuracy: 0.5629, Test Loss: 0.8295, Test Accuracy: 0.5468\n",
      "Epoch [23/50], Train Loss: 0.7433, Train Accuracy: 0.5543, Test Loss: 0.8490, Test Accuracy: 0.5532\n",
      "Epoch [24/50], Train Loss: 0.7288, Train Accuracy: 0.5597, Test Loss: 0.8879, Test Accuracy: 0.5667\n",
      "Epoch [25/50], Train Loss: 0.7349, Train Accuracy: 0.5640, Test Loss: 0.7976, Test Accuracy: 0.5991\n",
      "Epoch [26/50], Train Loss: 0.7254, Train Accuracy: 0.5665, Test Loss: 0.7906, Test Accuracy: 0.5658\n",
      "Epoch [27/50], Train Loss: 0.7173, Train Accuracy: 0.5514, Test Loss: 0.8127, Test Accuracy: 0.5414\n",
      "Epoch [28/50], Train Loss: 0.7211, Train Accuracy: 0.5726, Test Loss: 0.8470, Test Accuracy: 0.5486\n",
      "Early Stop\n",
      "Epoch [29/50], Train Loss: 0.7218, Train Accuracy: 0.5573, Test Loss: 0.8787, Test Accuracy: 0.5396\n",
      "\n",
      "Epoch [1/50], Train Loss: 1.0480, Train Accuracy: 0.3942, Test Loss: 1.1563, Test Accuracy: 0.2480\n",
      "Epoch [2/50], Train Loss: 0.8987, Train Accuracy: 0.4402, Test Loss: 0.9625, Test Accuracy: 0.4833\n",
      "Epoch [3/50], Train Loss: 0.8474, Train Accuracy: 0.5085, Test Loss: 1.1314, Test Accuracy: 0.2254\n",
      "Epoch [4/50], Train Loss: 0.8384, Train Accuracy: 0.4895, Test Loss: 0.8572, Test Accuracy: 0.4950\n",
      "Epoch [5/50], Train Loss: 0.7950, Train Accuracy: 0.5132, Test Loss: 0.9570, Test Accuracy: 0.4373\n",
      "Epoch [6/50], Train Loss: 0.7765, Train Accuracy: 0.5260, Test Loss: 1.0476, Test Accuracy: 0.4554\n",
      "Epoch [7/50], Train Loss: 0.7732, Train Accuracy: 0.5470, Test Loss: 0.8369, Test Accuracy: 0.5320\n",
      "Epoch [8/50], Train Loss: 0.7869, Train Accuracy: 0.5375, Test Loss: 0.8559, Test Accuracy: 0.5104\n",
      "Epoch [9/50], Train Loss: 0.7690, Train Accuracy: 0.5247, Test Loss: 0.8232, Test Accuracy: 0.5518\n",
      "Epoch [10/50], Train Loss: 0.7570, Train Accuracy: 0.5344, Test Loss: 0.9748, Test Accuracy: 0.4824\n",
      "Epoch [11/50], Train Loss: 0.7690, Train Accuracy: 0.5389, Test Loss: 0.9418, Test Accuracy: 0.4536\n",
      "Epoch [12/50], Train Loss: 0.7489, Train Accuracy: 0.5380, Test Loss: 0.7755, Test Accuracy: 0.6393\n",
      "Epoch [13/50], Train Loss: 0.7566, Train Accuracy: 0.5513, Test Loss: 0.8061, Test Accuracy: 0.5879\n",
      "Epoch [14/50], Train Loss: 0.7513, Train Accuracy: 0.5549, Test Loss: 0.8264, Test Accuracy: 0.5311\n",
      "Epoch [15/50], Train Loss: 0.7462, Train Accuracy: 0.5655, Test Loss: 0.8413, Test Accuracy: 0.5122\n",
      "Epoch [16/50], Train Loss: 0.7330, Train Accuracy: 0.5519, Test Loss: 0.8289, Test Accuracy: 0.5464\n",
      "Epoch [17/50], Train Loss: 0.7395, Train Accuracy: 0.5585, Test Loss: 0.8462, Test Accuracy: 0.5573\n",
      "Epoch [18/50], Train Loss: 0.7423, Train Accuracy: 0.5634, Test Loss: 0.8487, Test Accuracy: 0.5068\n",
      "Epoch [19/50], Train Loss: 0.7323, Train Accuracy: 0.5646, Test Loss: 0.8995, Test Accuracy: 0.4923\n",
      "Epoch [20/50], Train Loss: 0.7283, Train Accuracy: 0.5632, Test Loss: 0.9135, Test Accuracy: 0.4491\n",
      "Epoch [21/50], Train Loss: 0.7304, Train Accuracy: 0.5607, Test Loss: 0.8581, Test Accuracy: 0.5546\n",
      "Epoch [22/50], Train Loss: 0.7142, Train Accuracy: 0.5725, Test Loss: 0.8333, Test Accuracy: 0.5951\n",
      "Epoch [23/50], Train Loss: 0.7274, Train Accuracy: 0.5567, Test Loss: 0.7773, Test Accuracy: 0.5906\n",
      "Epoch [24/50], Train Loss: 0.7105, Train Accuracy: 0.5767, Test Loss: 0.8476, Test Accuracy: 0.5753\n",
      "Epoch [25/50], Train Loss: 0.7212, Train Accuracy: 0.5711, Test Loss: 0.7945, Test Accuracy: 0.6186\n",
      "Epoch [26/50], Train Loss: 0.7082, Train Accuracy: 0.5781, Test Loss: 0.8346, Test Accuracy: 0.5293\n",
      "Epoch [27/50], Train Loss: 0.7095, Train Accuracy: 0.5729, Test Loss: 0.8790, Test Accuracy: 0.5401\n",
      "Epoch [28/50], Train Loss: 0.6927, Train Accuracy: 0.5736, Test Loss: 0.8274, Test Accuracy: 0.5681\n",
      "Epoch [29/50], Train Loss: 0.6996, Train Accuracy: 0.5752, Test Loss: 0.8475, Test Accuracy: 0.5302\n",
      "Epoch [30/50], Train Loss: 0.6985, Train Accuracy: 0.5815, Test Loss: 0.9134, Test Accuracy: 0.5068\n",
      "Epoch [31/50], Train Loss: 0.6866, Train Accuracy: 0.5718, Test Loss: 0.8586, Test Accuracy: 0.5491\n",
      "Epoch [32/50], Train Loss: 0.6868, Train Accuracy: 0.5704, Test Loss: 0.8212, Test Accuracy: 0.5618\n",
      "Epoch [33/50], Train Loss: 0.6832, Train Accuracy: 0.5765, Test Loss: 0.8575, Test Accuracy: 0.5600\n",
      "Epoch [34/50], Train Loss: 0.6749, Train Accuracy: 0.5781, Test Loss: 0.8215, Test Accuracy: 0.5933\n",
      "Epoch [35/50], Train Loss: 0.6638, Train Accuracy: 0.5907, Test Loss: 0.8121, Test Accuracy: 0.5879\n",
      "Epoch [36/50], Train Loss: 0.6717, Train Accuracy: 0.5790, Test Loss: 0.8013, Test Accuracy: 0.6096\n",
      "Epoch [37/50], Train Loss: 0.6565, Train Accuracy: 0.5882, Test Loss: 0.7937, Test Accuracy: 0.6096\n",
      "Early Stop\n",
      "Epoch [38/50], Train Loss: 0.6642, Train Accuracy: 0.5921, Test Loss: 0.8811, Test Accuracy: 0.5230\n",
      "\n",
      "Epoch [1/50], Train Loss: 1.0784, Train Accuracy: 0.3771, Test Loss: 1.0229, Test Accuracy: 0.3580\n",
      "Epoch [2/50], Train Loss: 0.8765, Train Accuracy: 0.4708, Test Loss: 0.8335, Test Accuracy: 0.5600\n",
      "Epoch [3/50], Train Loss: 0.8455, Train Accuracy: 0.4789, Test Loss: 1.0427, Test Accuracy: 0.3634\n",
      "Epoch [4/50], Train Loss: 0.8194, Train Accuracy: 0.4828, Test Loss: 0.8587, Test Accuracy: 0.5149\n",
      "Epoch [5/50], Train Loss: 0.8064, Train Accuracy: 0.5172, Test Loss: 0.8535, Test Accuracy: 0.5419\n",
      "Epoch [6/50], Train Loss: 0.8062, Train Accuracy: 0.5132, Test Loss: 0.8655, Test Accuracy: 0.5113\n",
      "Epoch [7/50], Train Loss: 0.7879, Train Accuracy: 0.5114, Test Loss: 0.7848, Test Accuracy: 0.6105\n",
      "Epoch [8/50], Train Loss: 0.7666, Train Accuracy: 0.5339, Test Loss: 0.8560, Test Accuracy: 0.5762\n",
      "Epoch [9/50], Train Loss: 0.7689, Train Accuracy: 0.5346, Test Loss: 0.8250, Test Accuracy: 0.5654\n",
      "Epoch [10/50], Train Loss: 0.7556, Train Accuracy: 0.5569, Test Loss: 0.9079, Test Accuracy: 0.4950\n",
      "Epoch [11/50], Train Loss: 0.7650, Train Accuracy: 0.5526, Test Loss: 0.8968, Test Accuracy: 0.4698\n",
      "Epoch [12/50], Train Loss: 0.7585, Train Accuracy: 0.5549, Test Loss: 0.7736, Test Accuracy: 0.6258\n",
      "Epoch [13/50], Train Loss: 0.7576, Train Accuracy: 0.5490, Test Loss: 0.7656, Test Accuracy: 0.6069\n",
      "Epoch [14/50], Train Loss: 0.7519, Train Accuracy: 0.5619, Test Loss: 0.8170, Test Accuracy: 0.5482\n",
      "Epoch [15/50], Train Loss: 0.7542, Train Accuracy: 0.5535, Test Loss: 0.8643, Test Accuracy: 0.5113\n",
      "Epoch [16/50], Train Loss: 0.7528, Train Accuracy: 0.5438, Test Loss: 0.9462, Test Accuracy: 0.4509\n",
      "Epoch [17/50], Train Loss: 0.7448, Train Accuracy: 0.5544, Test Loss: 0.8394, Test Accuracy: 0.5663\n",
      "Epoch [18/50], Train Loss: 0.7433, Train Accuracy: 0.5510, Test Loss: 0.8817, Test Accuracy: 0.4977\n",
      "Epoch [19/50], Train Loss: 0.7447, Train Accuracy: 0.5529, Test Loss: 0.7946, Test Accuracy: 0.5708\n",
      "Epoch [20/50], Train Loss: 0.7368, Train Accuracy: 0.5540, Test Loss: 0.8213, Test Accuracy: 0.5645\n",
      "Epoch [21/50], Train Loss: 0.7369, Train Accuracy: 0.5576, Test Loss: 0.8509, Test Accuracy: 0.5176\n",
      "Epoch [22/50], Train Loss: 0.7200, Train Accuracy: 0.5634, Test Loss: 0.9191, Test Accuracy: 0.4545\n",
      "Epoch [23/50], Train Loss: 0.7287, Train Accuracy: 0.5492, Test Loss: 0.9031, Test Accuracy: 0.4923\n",
      "Early Stop\n",
      "Epoch [24/50], Train Loss: 0.7272, Train Accuracy: 0.5657, Test Loss: 0.8843, Test Accuracy: 0.5023\n",
      "\n",
      "Epoch [1/50], Train Loss: 1.0137, Train Accuracy: 0.4291, Test Loss: 0.9937, Test Accuracy: 0.4463\n",
      "Epoch [2/50], Train Loss: 0.9139, Train Accuracy: 0.4496, Test Loss: 0.9293, Test Accuracy: 0.4572\n",
      "Epoch [3/50], Train Loss: 0.8532, Train Accuracy: 0.4751, Test Loss: 0.7594, Test Accuracy: 0.6682\n",
      "Epoch [4/50], Train Loss: 0.8203, Train Accuracy: 0.5204, Test Loss: 0.8987, Test Accuracy: 0.4337\n",
      "Epoch [5/50], Train Loss: 0.7961, Train Accuracy: 0.5148, Test Loss: 0.7970, Test Accuracy: 0.5978\n",
      "Epoch [6/50], Train Loss: 0.8212, Train Accuracy: 0.4999, Test Loss: 0.8126, Test Accuracy: 0.5365\n",
      "Epoch [7/50], Train Loss: 0.7984, Train Accuracy: 0.5215, Test Loss: 1.0230, Test Accuracy: 0.4328\n",
      "Epoch [8/50], Train Loss: 0.7879, Train Accuracy: 0.5227, Test Loss: 0.8791, Test Accuracy: 0.5149\n",
      "Epoch [9/50], Train Loss: 0.7826, Train Accuracy: 0.5463, Test Loss: 0.8910, Test Accuracy: 0.4824\n",
      "Early Stop\n",
      "Epoch [10/50], Train Loss: 0.7875, Train Accuracy: 0.5238, Test Loss: 1.0747, Test Accuracy: 0.2831\n",
      "\n",
      "Epoch [1/50], Train Loss: 1.0408, Train Accuracy: 0.3816, Test Loss: 0.8468, Test Accuracy: 0.5870\n",
      "Epoch [2/50], Train Loss: 0.8687, Train Accuracy: 0.4737, Test Loss: 0.7615, Test Accuracy: 0.5960\n",
      "Epoch [3/50], Train Loss: 0.8447, Train Accuracy: 0.4958, Test Loss: 0.7459, Test Accuracy: 0.6339\n",
      "Epoch [4/50], Train Loss: 0.8280, Train Accuracy: 0.5087, Test Loss: 0.7451, Test Accuracy: 0.6339\n",
      "Epoch [5/50], Train Loss: 0.7832, Train Accuracy: 0.5166, Test Loss: 0.8073, Test Accuracy: 0.6267\n",
      "Epoch [6/50], Train Loss: 0.7802, Train Accuracy: 0.5188, Test Loss: 0.7743, Test Accuracy: 0.5960\n",
      "Epoch [7/50], Train Loss: 0.8022, Train Accuracy: 0.5175, Test Loss: 0.7440, Test Accuracy: 0.6240\n",
      "Epoch [8/50], Train Loss: 0.7740, Train Accuracy: 0.5427, Test Loss: 0.9380, Test Accuracy: 0.4463\n",
      "Epoch [9/50], Train Loss: 0.7682, Train Accuracy: 0.5341, Test Loss: 0.9216, Test Accuracy: 0.4256\n",
      "Epoch [10/50], Train Loss: 0.7584, Train Accuracy: 0.5393, Test Loss: 0.8234, Test Accuracy: 0.5212\n",
      "Epoch [11/50], Train Loss: 0.7743, Train Accuracy: 0.5391, Test Loss: 0.7538, Test Accuracy: 0.6123\n",
      "Epoch [12/50], Train Loss: 0.7585, Train Accuracy: 0.5592, Test Loss: 0.8145, Test Accuracy: 0.5383\n",
      "Epoch [13/50], Train Loss: 0.7526, Train Accuracy: 0.5657, Test Loss: 0.8118, Test Accuracy: 0.5356\n",
      "Epoch [14/50], Train Loss: 0.7522, Train Accuracy: 0.5560, Test Loss: 0.7672, Test Accuracy: 0.5888\n",
      "Epoch [15/50], Train Loss: 0.7646, Train Accuracy: 0.5524, Test Loss: 0.8382, Test Accuracy: 0.5203\n",
      "Epoch [16/50], Train Loss: 0.7485, Train Accuracy: 0.5341, Test Loss: 0.8049, Test Accuracy: 0.5726\n",
      "Epoch [17/50], Train Loss: 0.7371, Train Accuracy: 0.5630, Test Loss: 0.8430, Test Accuracy: 0.4995\n",
      "Epoch [18/50], Train Loss: 0.7353, Train Accuracy: 0.5675, Test Loss: 0.7823, Test Accuracy: 0.5717\n",
      "Epoch [19/50], Train Loss: 0.7430, Train Accuracy: 0.5625, Test Loss: 0.7400, Test Accuracy: 0.6060\n",
      "Epoch [20/50], Train Loss: 0.7268, Train Accuracy: 0.5637, Test Loss: 0.7587, Test Accuracy: 0.6096\n",
      "Epoch [21/50], Train Loss: 0.7331, Train Accuracy: 0.5810, Test Loss: 0.6858, Test Accuracy: 0.6610\n",
      "Epoch [22/50], Train Loss: 0.7210, Train Accuracy: 0.5761, Test Loss: 0.8014, Test Accuracy: 0.5843\n",
      "Epoch [23/50], Train Loss: 0.7265, Train Accuracy: 0.5693, Test Loss: 0.7448, Test Accuracy: 0.6014\n",
      "Epoch [24/50], Train Loss: 0.7263, Train Accuracy: 0.5643, Test Loss: 0.8287, Test Accuracy: 0.5212\n",
      "Epoch [25/50], Train Loss: 0.7288, Train Accuracy: 0.5709, Test Loss: 0.7897, Test Accuracy: 0.5690\n",
      "Epoch [26/50], Train Loss: 0.7127, Train Accuracy: 0.5772, Test Loss: 0.7593, Test Accuracy: 0.5933\n",
      "Epoch [27/50], Train Loss: 0.7087, Train Accuracy: 0.5592, Test Loss: 0.8417, Test Accuracy: 0.5203\n",
      "Early Stop\n",
      "Epoch [28/50], Train Loss: 0.7221, Train Accuracy: 0.5614, Test Loss: 0.7865, Test Accuracy: 0.5888\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp_name = f'b-one-head-{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "# kf = KFold(n_splits=5, shuffle=True, random_state=25)\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X_balanced['atac'])):\n",
    "    \n",
    "    X_train_atac, X_test_atac = [X_balanced['atac'][i] for i in train_index], [X_balanced['atac'][i] for i in test_index]\n",
    "    \n",
    "    y_train, y_test = [y_balanced[i] for i in train_index], [y_balanced[i] for i in test_index]\n",
    "    \n",
    "    X_train_atac_normalized, X_test_atac_normalized = normalize_features(X_train_atac, X_test_atac)\n",
    "\n",
    "    elem_stats, epoch_stats, model = train_model(X_train_atac_normalized, y_train,\n",
    "                                            X_test_atac_normalized, y_test,\n",
    "                                            exp_name=exp_name, fold_idx=i)\n",
    "\n",
    "    df_elem = pd.DataFrame({'preds' : elem_stats[0], 'labels' : elem_stats[1]})\n",
    "    df_epoch = pd.DataFrame({'train_acc' : epoch_stats[0], 'train_loss' : epoch_stats[1], \n",
    "                             'test_acc' : epoch_stats[2], 'test_loss' : epoch_stats[3]})\n",
    "\n",
    "    df_elem.to_csv(f\"../baseline_results/{_ct}/atac/imbal_elem_{i}.csv\")\n",
    "    df_epoch.to_csv(f\"../baseline_results/{_ct}/atac/imbal_epoch_{i}.csv\")\n",
    "    \n",
    "    torch.save(model, f\"../baseline_results/{_ct}/atac/imbal_model_{i}.pt\")\n",
    "    del model \n",
    "    del elem_stats, epoch_stats\n",
    "    del X_train_atac, X_test_atac, X_train_atac_normalized, X_test_atac_normalized, y_train, y_test\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0cd5a75f-02cc-44a3-81bf-e8d98e6b3b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated Memory: 39056.07 MB\n",
      "Cached Memory: 39532.00 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Allocated Memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"Cached Memory: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c653c9d0-32fb-4061-beb9-ee5d67a28f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 1.1078, Train Accuracy: 0.3877, Test Loss: 1.1813, Test Accuracy: 0.1532\n",
      "Epoch [2/50], Train Loss: 1.0621, Train Accuracy: 0.3972, Test Loss: 1.0461, Test Accuracy: 0.1784\n",
      "Epoch [3/50], Train Loss: 0.9734, Train Accuracy: 0.4225, Test Loss: 0.9303, Test Accuracy: 0.6243\n",
      "Epoch [4/50], Train Loss: 0.9586, Train Accuracy: 0.4743, Test Loss: 0.8936, Test Accuracy: 0.6144\n",
      "Epoch [5/50], Train Loss: 0.9600, Train Accuracy: 0.4926, Test Loss: 0.9599, Test Accuracy: 0.5550\n",
      "Epoch [6/50], Train Loss: 0.9574, Train Accuracy: 0.4689, Test Loss: 1.1035, Test Accuracy: 0.1919\n",
      "Epoch [7/50], Train Loss: 0.9430, Train Accuracy: 0.4707, Test Loss: 0.9470, Test Accuracy: 0.5378\n",
      "Epoch [8/50], Train Loss: 0.9428, Train Accuracy: 0.5232, Test Loss: 0.9012, Test Accuracy: 0.6586\n",
      "Epoch [9/50], Train Loss: 0.9413, Train Accuracy: 0.4687, Test Loss: 0.9271, Test Accuracy: 0.3811\n",
      "Epoch [10/50], Train Loss: 0.9315, Train Accuracy: 0.5090, Test Loss: 0.9123, Test Accuracy: 0.6477\n",
      "Epoch [11/50], Train Loss: 0.9247, Train Accuracy: 0.5426, Test Loss: 0.8797, Test Accuracy: 0.4459\n",
      "Epoch [12/50], Train Loss: 0.9211, Train Accuracy: 0.5428, Test Loss: 1.0003, Test Accuracy: 0.5550\n",
      "Epoch [13/50], Train Loss: 0.9205, Train Accuracy: 0.4986, Test Loss: 0.9478, Test Accuracy: 0.5243\n",
      "Epoch [14/50], Train Loss: 0.9314, Train Accuracy: 0.4966, Test Loss: 0.8957, Test Accuracy: 0.4703\n",
      "Epoch [15/50], Train Loss: 0.9154, Train Accuracy: 0.5059, Test Loss: 0.8815, Test Accuracy: 0.4405\n",
      "Epoch [16/50], Train Loss: 0.9225, Train Accuracy: 0.5318, Test Loss: 0.9637, Test Accuracy: 0.5910\n",
      "Epoch [17/50], Train Loss: 0.9193, Train Accuracy: 0.5311, Test Loss: 1.0851, Test Accuracy: 0.5270\n",
      "Epoch [18/50], Train Loss: 0.9197, Train Accuracy: 0.5406, Test Loss: 0.9039, Test Accuracy: 0.5955\n",
      "Epoch [19/50], Train Loss: 0.9046, Train Accuracy: 0.5322, Test Loss: 0.9357, Test Accuracy: 0.5505\n",
      "Epoch [20/50], Train Loss: 0.9145, Train Accuracy: 0.5255, Test Loss: 0.8975, Test Accuracy: 0.5973\n",
      "Epoch [21/50], Train Loss: 0.9068, Train Accuracy: 0.5099, Test Loss: 0.9082, Test Accuracy: 0.4703\n",
      "Epoch [22/50], Train Loss: 0.9054, Train Accuracy: 0.5329, Test Loss: 0.8874, Test Accuracy: 0.6495\n",
      "Epoch [23/50], Train Loss: 0.8996, Train Accuracy: 0.5561, Test Loss: 0.9527, Test Accuracy: 0.5432\n",
      "Epoch [24/50], Train Loss: 0.8965, Train Accuracy: 0.5440, Test Loss: 0.9326, Test Accuracy: 0.5847\n",
      "Epoch [25/50], Train Loss: 0.8932, Train Accuracy: 0.5751, Test Loss: 0.9460, Test Accuracy: 0.5712\n",
      "Epoch [26/50], Train Loss: 0.8942, Train Accuracy: 0.5564, Test Loss: 1.0119, Test Accuracy: 0.3532\n",
      "Epoch [27/50], Train Loss: 0.8864, Train Accuracy: 0.5108, Test Loss: 0.8779, Test Accuracy: 0.6333\n",
      "Epoch [28/50], Train Loss: 0.8839, Train Accuracy: 0.5584, Test Loss: 0.9341, Test Accuracy: 0.6063\n",
      "Epoch [29/50], Train Loss: 0.8875, Train Accuracy: 0.5487, Test Loss: 0.9231, Test Accuracy: 0.4216\n",
      "Epoch [30/50], Train Loss: 0.8825, Train Accuracy: 0.5514, Test Loss: 0.9260, Test Accuracy: 0.5667\n",
      "Epoch [31/50], Train Loss: 0.8847, Train Accuracy: 0.5821, Test Loss: 0.9530, Test Accuracy: 0.5396\n",
      "Epoch [32/50], Train Loss: 0.8793, Train Accuracy: 0.5857, Test Loss: 0.9794, Test Accuracy: 0.3216\n",
      "Epoch [33/50], Train Loss: 0.8727, Train Accuracy: 0.5356, Test Loss: 0.9692, Test Accuracy: 0.3441\n",
      "Epoch [34/50], Train Loss: 0.8652, Train Accuracy: 0.5886, Test Loss: 0.9175, Test Accuracy: 0.5946\n",
      "Epoch [35/50], Train Loss: 0.8575, Train Accuracy: 0.5933, Test Loss: 0.9547, Test Accuracy: 0.5279\n",
      "Epoch [36/50], Train Loss: 0.8647, Train Accuracy: 0.5618, Test Loss: 0.9720, Test Accuracy: 0.3297\n",
      "Epoch [37/50], Train Loss: 0.8572, Train Accuracy: 0.5739, Test Loss: 0.9364, Test Accuracy: 0.5703\n",
      "Epoch [38/50], Train Loss: 0.8441, Train Accuracy: 0.5836, Test Loss: 0.9552, Test Accuracy: 0.5649\n",
      "Epoch [39/50], Train Loss: 0.8423, Train Accuracy: 0.6023, Test Loss: 0.9177, Test Accuracy: 0.6189\n",
      "Epoch [40/50], Train Loss: 0.8438, Train Accuracy: 0.5818, Test Loss: 0.9190, Test Accuracy: 0.6144\n",
      "Epoch [41/50], Train Loss: 0.8364, Train Accuracy: 0.5976, Test Loss: 0.9632, Test Accuracy: 0.5748\n",
      "Epoch [42/50], Train Loss: 0.8292, Train Accuracy: 0.5663, Test Loss: 0.9998, Test Accuracy: 0.5468\n",
      "Epoch [43/50], Train Loss: 0.8190, Train Accuracy: 0.5857, Test Loss: 0.9798, Test Accuracy: 0.5414\n",
      "Epoch [44/50], Train Loss: 0.8154, Train Accuracy: 0.6039, Test Loss: 0.9654, Test Accuracy: 0.6018\n",
      "Epoch [45/50], Train Loss: 0.8080, Train Accuracy: 0.5902, Test Loss: 0.9699, Test Accuracy: 0.5721\n",
      "Epoch [46/50], Train Loss: 0.8044, Train Accuracy: 0.6010, Test Loss: 0.9836, Test Accuracy: 0.5838\n",
      "Early Stop\n",
      "Epoch [47/50], Train Loss: 0.8047, Train Accuracy: 0.6046, Test Loss: 1.0447, Test Accuracy: 0.5414\n",
      "\n",
      "Allocated Memory: 0.00 MB\n",
      "Cached Memory: 2.00 MB\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 5.48 GiB (GPU 0; 44.56 GiB total capacity; 38.14 GiB already allocated; 4.99 GiB free; 38.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[138], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m y_train, y_test \u001b[38;5;241m=\u001b[39m [y_balanced[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m train_index], [y_balanced[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m test_index]\n\u001b[1;32m     10\u001b[0m X_train_hic_normalized, X_test_hic_normalized \u001b[38;5;241m=\u001b[39m normalize_features(X_train_hic, X_test_hic)\n\u001b[0;32m---> 12\u001b[0m elem_stats, epoch_stats, model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_hic_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mX_test_hic_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mexp_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m df_elem \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreds\u001b[39m\u001b[38;5;124m'\u001b[39m : elem_stats[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m : elem_stats[\u001b[38;5;241m1\u001b[39m]})\n\u001b[1;32m     17\u001b[0m df_epoch \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m'\u001b[39m : epoch_stats[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m : epoch_stats[\u001b[38;5;241m1\u001b[39m], \n\u001b[1;32m     18\u001b[0m                          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_acc\u001b[39m\u001b[38;5;124m'\u001b[39m : epoch_stats[\u001b[38;5;241m2\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_loss\u001b[39m\u001b[38;5;124m'\u001b[39m : epoch_stats[\u001b[38;5;241m3\u001b[39m]})\n",
      "Cell \u001b[0;32mIn[16], line 63\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(X_train, y_train, X_test, y_test, exp_name, fold_idx)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x1, batch_y, mask1 \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     62\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 63\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y\u001b[38;5;241m.\u001b[39msqueeze())\n\u001b[1;32m     65\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/agp/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[14], line 40\u001b[0m, in \u001b[0;36mOneHeadTransformerModel.forward\u001b[0;34m(self, _x, x_mask)\u001b[0m\n\u001b[1;32m     37\u001b[0m _mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((torch\u001b[38;5;241m.\u001b[39mones(x_mask\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device), x_mask), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# print('mask cat', _mask.device)\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m _z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_z\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43m_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# print('z 2', _z.device)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Pooling, we should not use average pooling since the sequence length is also important\u001b[39;00m\n\u001b[1;32m     44\u001b[0m _z \u001b[38;5;241m=\u001b[39m _z[:, \u001b[38;5;241m0\u001b[39m, :]\n",
      "File \u001b[0;32m~/miniconda3/envs/agp/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/agp/lib/python3.10/site-packages/torch/nn/modules/transformer.py:238\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    236\u001b[0m         output \u001b[38;5;241m=\u001b[39m mod(output, src_mask\u001b[38;5;241m=\u001b[39mmask)\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 238\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    241\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/agp/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/agp/lib/python3.10/site-packages/torch/nn/modules/transformer.py:460\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    458\u001b[0m x \u001b[38;5;241m=\u001b[39m src\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_first:\n\u001b[0;32m--> 460\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/agp/lib/python3.10/site-packages/torch/nn/modules/transformer.py:471\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor,\n\u001b[1;32m    470\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 471\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/agp/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/agp/lib/python3.10/site-packages/torch/nn/modules/activation.py:1153\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m   1142\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1143\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         q_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj_weight, k_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj_weight,\n\u001b[1;32m   1151\u001b[0m         v_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj_weight, average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights)\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1153\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/miniconda3/envs/agp/lib/python3.10/site-packages/torch/nn/functional.py:5179\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[1;32m   5174\u001b[0m     dropout_p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m   5176\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   5177\u001b[0m \u001b[38;5;66;03m# (deep breath) calculate attention and out projection\u001b[39;00m\n\u001b[1;32m   5178\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m-> 5179\u001b[0m attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43m_scaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5180\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(tgt_len \u001b[38;5;241m*\u001b[39m bsz, embed_dim)\n\u001b[1;32m   5181\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "File \u001b[0;32m~/miniconda3/envs/agp/lib/python3.10/site-packages/torch/nn/functional.py:4852\u001b[0m, in \u001b[0;36m_scaled_dot_product_attention\u001b[0;34m(q, k, v, attn_mask, dropout_p)\u001b[0m\n\u001b[1;32m   4850\u001b[0m \u001b[38;5;66;03m# (B, Nt, E) x (B, E, Ns) -> (B, Nt, Ns)\u001b[39;00m\n\u001b[1;32m   4851\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4852\u001b[0m     attn \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbaddbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4853\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4854\u001b[0m     attn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(q, k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 5.48 GiB (GPU 0; 44.56 GiB total capacity; 38.14 GiB already allocated; 4.99 GiB free; 38.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "exp_name = f'b-one-head-{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "# kf = KFold(n_splits=5, shuffle=True, random_state=25)\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X_balanced['hic'])):\n",
    "\n",
    "    X_train_hic, X_test_hic = [X_balanced['hic'][i] for i in train_index], [X_balanced['hic'][i] for i in test_index]\n",
    "    \n",
    "    y_train, y_test = [y_balanced[i] for i in train_index], [y_balanced[i] for i in test_index]\n",
    "\n",
    "    X_train_hic_normalized, X_test_hic_normalized = normalize_features(X_train_hic, X_test_hic)\n",
    "    \n",
    "    elem_stats, epoch_stats, model = train_model(X_train_hic_normalized, y_train,\n",
    "                                            X_test_hic_normalized, y_test,\n",
    "                                            exp_name=exp_name, fold_idx=i)\n",
    "\n",
    "    df_elem = pd.DataFrame({'preds' : elem_stats[0], 'labels' : elem_stats[1]})\n",
    "    df_epoch = pd.DataFrame({'train_acc' : epoch_stats[0], 'train_loss' : epoch_stats[1], \n",
    "                             'test_acc' : epoch_stats[2], 'test_loss' : epoch_stats[3]})\n",
    "\n",
    "    df_elem.to_csv(f\"../baseline_results/{_ct}/hic/imbal_elem_{i}.csv\")\n",
    "    df_epoch.to_csv(f\"../baseline_results/{_ct}/hic/imbal_epoch_{i}.csv\")\n",
    "    \n",
    "    torch.save(model, f\"../baseline_results/{_ct}/hic/imbal_model_{i}.pt\")\n",
    "    \n",
    "    del model\n",
    "    del elem_stats, epoch_stats\n",
    "    del X_train_hic, X_test_hic, X_train_hic_normalized, X_test_hic_normalized, y_train, y_test\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Allocated Memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "    print(f\"Cached Memory: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "871269b1-76cf-4b63-8ba0-c282780dd631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 1.0247, Train Accuracy: 0.4270, Test Loss: 0.8308, Test Accuracy: 0.6874\n",
      "Epoch [2/50], Train Loss: 0.9186, Train Accuracy: 0.4729, Test Loss: 0.9740, Test Accuracy: 0.4324\n",
      "Epoch [3/50], Train Loss: 0.9201, Train Accuracy: 0.4802, Test Loss: 0.9846, Test Accuracy: 0.4189\n",
      "Epoch [4/50], Train Loss: 0.9116, Train Accuracy: 0.4829, Test Loss: 0.9308, Test Accuracy: 0.4739\n",
      "Epoch [5/50], Train Loss: 0.8823, Train Accuracy: 0.4820, Test Loss: 0.8247, Test Accuracy: 0.6324\n",
      "Epoch [6/50], Train Loss: 0.8851, Train Accuracy: 0.4743, Test Loss: 0.9143, Test Accuracy: 0.4523\n",
      "Epoch [7/50], Train Loss: 0.8891, Train Accuracy: 0.4899, Test Loss: 1.0345, Test Accuracy: 0.3874\n",
      "Epoch [8/50], Train Loss: 0.8906, Train Accuracy: 0.4995, Test Loss: 0.9349, Test Accuracy: 0.5090\n",
      "Epoch [9/50], Train Loss: 0.8665, Train Accuracy: 0.4935, Test Loss: 0.9688, Test Accuracy: 0.3874\n",
      "Epoch [10/50], Train Loss: 0.8746, Train Accuracy: 0.5104, Test Loss: 0.9041, Test Accuracy: 0.5523\n",
      "Epoch [11/50], Train Loss: 0.8729, Train Accuracy: 0.5138, Test Loss: 0.8986, Test Accuracy: 0.5477\n",
      "Epoch [12/50], Train Loss: 0.8682, Train Accuracy: 0.5158, Test Loss: 0.9253, Test Accuracy: 0.5315\n",
      "Epoch [13/50], Train Loss: 0.8643, Train Accuracy: 0.5097, Test Loss: 0.8152, Test Accuracy: 0.6000\n",
      "Epoch [14/50], Train Loss: 0.8703, Train Accuracy: 0.5201, Test Loss: 0.9205, Test Accuracy: 0.4865\n",
      "Epoch [15/50], Train Loss: 0.8700, Train Accuracy: 0.5128, Test Loss: 0.8540, Test Accuracy: 0.5523\n",
      "Epoch [16/50], Train Loss: 0.8562, Train Accuracy: 0.5205, Test Loss: 0.9767, Test Accuracy: 0.4856\n",
      "Epoch [17/50], Train Loss: 0.8647, Train Accuracy: 0.5133, Test Loss: 0.9570, Test Accuracy: 0.4162\n",
      "Epoch [18/50], Train Loss: 0.8680, Train Accuracy: 0.4971, Test Loss: 0.8544, Test Accuracy: 0.5937\n",
      "Epoch [19/50], Train Loss: 0.8610, Train Accuracy: 0.5230, Test Loss: 0.8596, Test Accuracy: 0.5847\n",
      "Epoch [20/50], Train Loss: 0.8533, Train Accuracy: 0.5092, Test Loss: 0.8559, Test Accuracy: 0.5694\n",
      "Epoch [21/50], Train Loss: 0.8490, Train Accuracy: 0.5196, Test Loss: 0.9159, Test Accuracy: 0.4495\n",
      "Epoch [22/50], Train Loss: 0.8506, Train Accuracy: 0.5174, Test Loss: 0.8618, Test Accuracy: 0.5324\n",
      "Epoch [23/50], Train Loss: 0.8470, Train Accuracy: 0.5293, Test Loss: 0.8906, Test Accuracy: 0.5207\n",
      "Epoch [24/50], Train Loss: 0.8542, Train Accuracy: 0.5234, Test Loss: 0.8838, Test Accuracy: 0.5559\n",
      "Early Stop\n",
      "Epoch [25/50], Train Loss: 0.8437, Train Accuracy: 0.5449, Test Loss: 0.9245, Test Accuracy: 0.4279\n",
      "\n",
      "Allocated Memory: 0.00 MB\n",
      "Cached Memory: 2.00 MB\n",
      "Epoch [1/50], Train Loss: 1.0385, Train Accuracy: 0.4097, Test Loss: 1.0406, Test Accuracy: 0.3003\n",
      "Epoch [2/50], Train Loss: 0.9346, Train Accuracy: 0.4481, Test Loss: 1.0635, Test Accuracy: 0.3318\n",
      "Epoch [3/50], Train Loss: 0.9313, Train Accuracy: 0.4686, Test Loss: 1.0520, Test Accuracy: 0.4202\n",
      "Epoch [4/50], Train Loss: 0.9060, Train Accuracy: 0.4699, Test Loss: 1.0740, Test Accuracy: 0.4094\n",
      "Epoch [5/50], Train Loss: 0.8846, Train Accuracy: 0.4780, Test Loss: 0.9488, Test Accuracy: 0.4094\n",
      "Epoch [6/50], Train Loss: 0.8779, Train Accuracy: 0.4877, Test Loss: 0.8610, Test Accuracy: 0.5924\n",
      "Epoch [7/50], Train Loss: 0.8784, Train Accuracy: 0.4810, Test Loss: 0.9024, Test Accuracy: 0.5041\n",
      "Epoch [8/50], Train Loss: 0.8608, Train Accuracy: 0.4931, Test Loss: 0.8757, Test Accuracy: 0.4977\n",
      "Epoch [9/50], Train Loss: 0.8749, Train Accuracy: 0.4981, Test Loss: 0.8967, Test Accuracy: 0.5338\n",
      "Epoch [10/50], Train Loss: 0.8410, Train Accuracy: 0.4936, Test Loss: 0.9136, Test Accuracy: 0.4905\n",
      "Epoch [11/50], Train Loss: 0.8647, Train Accuracy: 0.4911, Test Loss: 1.0326, Test Accuracy: 0.4139\n",
      "Epoch [12/50], Train Loss: 0.8490, Train Accuracy: 0.4902, Test Loss: 0.9299, Test Accuracy: 0.5861\n",
      "Epoch [13/50], Train Loss: 0.8667, Train Accuracy: 0.4852, Test Loss: 0.9197, Test Accuracy: 0.4445\n",
      "Epoch [14/50], Train Loss: 0.8517, Train Accuracy: 0.5051, Test Loss: 0.8367, Test Accuracy: 0.6249\n",
      "Epoch [15/50], Train Loss: 0.8450, Train Accuracy: 0.5168, Test Loss: 0.8983, Test Accuracy: 0.5176\n",
      "Epoch [16/50], Train Loss: 0.8512, Train Accuracy: 0.4990, Test Loss: 0.8930, Test Accuracy: 0.5798\n",
      "Epoch [17/50], Train Loss: 0.8439, Train Accuracy: 0.5107, Test Loss: 0.8695, Test Accuracy: 0.5645\n",
      "Epoch [18/50], Train Loss: 0.8404, Train Accuracy: 0.5085, Test Loss: 0.8440, Test Accuracy: 0.5987\n",
      "Epoch [19/50], Train Loss: 0.8443, Train Accuracy: 0.5204, Test Loss: 0.8943, Test Accuracy: 0.5591\n",
      "Epoch [20/50], Train Loss: 0.8423, Train Accuracy: 0.5130, Test Loss: 0.8684, Test Accuracy: 0.5293\n",
      "Epoch [21/50], Train Loss: 0.8460, Train Accuracy: 0.5224, Test Loss: 1.0081, Test Accuracy: 0.4427\n",
      "Epoch [22/50], Train Loss: 0.8299, Train Accuracy: 0.5254, Test Loss: 0.9381, Test Accuracy: 0.4689\n",
      "Epoch [23/50], Train Loss: 0.8282, Train Accuracy: 0.5118, Test Loss: 0.8997, Test Accuracy: 0.4869\n",
      "Epoch [24/50], Train Loss: 0.8320, Train Accuracy: 0.5166, Test Loss: 0.9182, Test Accuracy: 0.4626\n",
      "Epoch [25/50], Train Loss: 0.8321, Train Accuracy: 0.5263, Test Loss: 0.8835, Test Accuracy: 0.5014\n",
      "Epoch [26/50], Train Loss: 0.8257, Train Accuracy: 0.5019, Test Loss: 0.8443, Test Accuracy: 0.5573\n",
      "Epoch [27/50], Train Loss: 0.8289, Train Accuracy: 0.5107, Test Loss: 0.8835, Test Accuracy: 0.5221\n",
      "Epoch [28/50], Train Loss: 0.8327, Train Accuracy: 0.5233, Test Loss: 0.8693, Test Accuracy: 0.5807\n",
      "Epoch [29/50], Train Loss: 0.8246, Train Accuracy: 0.5089, Test Loss: 0.9114, Test Accuracy: 0.5401\n",
      "Epoch [30/50], Train Loss: 0.8170, Train Accuracy: 0.5202, Test Loss: 0.9285, Test Accuracy: 0.5113\n",
      "Epoch [31/50], Train Loss: 0.8214, Train Accuracy: 0.5242, Test Loss: 0.8316, Test Accuracy: 0.5852\n",
      "Epoch [32/50], Train Loss: 0.8133, Train Accuracy: 0.5236, Test Loss: 0.8566, Test Accuracy: 0.5780\n",
      "Epoch [33/50], Train Loss: 0.8130, Train Accuracy: 0.5249, Test Loss: 0.8792, Test Accuracy: 0.5428\n",
      "Epoch [34/50], Train Loss: 0.8184, Train Accuracy: 0.5308, Test Loss: 0.8970, Test Accuracy: 0.5230\n",
      "Epoch [35/50], Train Loss: 0.8101, Train Accuracy: 0.5217, Test Loss: 0.8775, Test Accuracy: 0.5284\n",
      "Epoch [36/50], Train Loss: 0.8099, Train Accuracy: 0.5393, Test Loss: 0.8709, Test Accuracy: 0.5365\n",
      "Epoch [37/50], Train Loss: 0.8037, Train Accuracy: 0.5319, Test Loss: 0.8991, Test Accuracy: 0.5401\n",
      "Epoch [38/50], Train Loss: 0.8065, Train Accuracy: 0.5402, Test Loss: 0.8565, Test Accuracy: 0.5753\n",
      "Epoch [39/50], Train Loss: 0.8023, Train Accuracy: 0.5438, Test Loss: 0.8980, Test Accuracy: 0.5185\n",
      "Epoch [40/50], Train Loss: 0.8015, Train Accuracy: 0.5247, Test Loss: 0.8822, Test Accuracy: 0.5446\n",
      "Epoch [41/50], Train Loss: 0.7958, Train Accuracy: 0.5339, Test Loss: 0.9209, Test Accuracy: 0.5383\n",
      "Epoch [42/50], Train Loss: 0.7973, Train Accuracy: 0.5350, Test Loss: 0.8644, Test Accuracy: 0.5546\n",
      "Epoch [43/50], Train Loss: 0.7963, Train Accuracy: 0.5432, Test Loss: 0.9106, Test Accuracy: 0.5347\n",
      "Epoch [44/50], Train Loss: 0.7857, Train Accuracy: 0.5409, Test Loss: 0.8728, Test Accuracy: 0.5636\n",
      "Epoch [45/50], Train Loss: 0.7927, Train Accuracy: 0.5382, Test Loss: 0.8365, Test Accuracy: 0.5636\n",
      "Epoch [46/50], Train Loss: 0.7895, Train Accuracy: 0.5393, Test Loss: 0.8889, Test Accuracy: 0.5446\n",
      "Epoch [47/50], Train Loss: 0.7798, Train Accuracy: 0.5434, Test Loss: 0.8986, Test Accuracy: 0.5194\n",
      "Epoch [48/50], Train Loss: 0.7823, Train Accuracy: 0.5227, Test Loss: 0.8497, Test Accuracy: 0.5600\n",
      "Epoch [49/50], Train Loss: 0.7783, Train Accuracy: 0.5456, Test Loss: 0.8794, Test Accuracy: 0.5428\n",
      "Epoch [50/50], Train Loss: 0.7869, Train Accuracy: 0.5427, Test Loss: 0.8690, Test Accuracy: 0.5464\n",
      "Allocated Memory: 0.00 MB\n",
      "Cached Memory: 2.00 MB\n",
      "Epoch [1/50], Train Loss: 1.0118, Train Accuracy: 0.4361, Test Loss: 1.0226, Test Accuracy: 0.2876\n",
      "Epoch [2/50], Train Loss: 0.9113, Train Accuracy: 0.4645, Test Loss: 1.0359, Test Accuracy: 0.3192\n",
      "Epoch [3/50], Train Loss: 0.9004, Train Accuracy: 0.4819, Test Loss: 0.9757, Test Accuracy: 0.3859\n",
      "Epoch [4/50], Train Loss: 0.9083, Train Accuracy: 0.4762, Test Loss: 0.9098, Test Accuracy: 0.4761\n",
      "Epoch [5/50], Train Loss: 0.8890, Train Accuracy: 0.4924, Test Loss: 0.9557, Test Accuracy: 0.4463\n",
      "Epoch [6/50], Train Loss: 0.8773, Train Accuracy: 0.5089, Test Loss: 0.9236, Test Accuracy: 0.4788\n",
      "Epoch [7/50], Train Loss: 0.8655, Train Accuracy: 0.4920, Test Loss: 0.8748, Test Accuracy: 0.5473\n",
      "Epoch [8/50], Train Loss: 0.8672, Train Accuracy: 0.4979, Test Loss: 1.0232, Test Accuracy: 0.4463\n",
      "Epoch [9/50], Train Loss: 0.8647, Train Accuracy: 0.5078, Test Loss: 0.9192, Test Accuracy: 0.5140\n",
      "Epoch [10/50], Train Loss: 0.8604, Train Accuracy: 0.5265, Test Loss: 0.9906, Test Accuracy: 0.4049\n",
      "Epoch [11/50], Train Loss: 0.8498, Train Accuracy: 0.5121, Test Loss: 0.8033, Test Accuracy: 0.6745\n",
      "Epoch [12/50], Train Loss: 0.8621, Train Accuracy: 0.5037, Test Loss: 0.8866, Test Accuracy: 0.5293\n",
      "Epoch [13/50], Train Loss: 0.8365, Train Accuracy: 0.5479, Test Loss: 1.0019, Test Accuracy: 0.4238\n",
      "Epoch [14/50], Train Loss: 0.8479, Train Accuracy: 0.4949, Test Loss: 0.8645, Test Accuracy: 0.5401\n",
      "Epoch [15/50], Train Loss: 0.8511, Train Accuracy: 0.5204, Test Loss: 0.9462, Test Accuracy: 0.5194\n",
      "Epoch [16/50], Train Loss: 0.8443, Train Accuracy: 0.5285, Test Loss: 0.8956, Test Accuracy: 0.5482\n",
      "Epoch [17/50], Train Loss: 0.8409, Train Accuracy: 0.5366, Test Loss: 0.9141, Test Accuracy: 0.5230\n",
      "Epoch [18/50], Train Loss: 0.8348, Train Accuracy: 0.5326, Test Loss: 0.8652, Test Accuracy: 0.5726\n",
      "Epoch [19/50], Train Loss: 0.8330, Train Accuracy: 0.5355, Test Loss: 0.8940, Test Accuracy: 0.5239\n",
      "Epoch [20/50], Train Loss: 0.8395, Train Accuracy: 0.5393, Test Loss: 0.9351, Test Accuracy: 0.4743\n",
      "Epoch [21/50], Train Loss: 0.8355, Train Accuracy: 0.5337, Test Loss: 0.8676, Test Accuracy: 0.5735\n",
      "Early Stop\n",
      "Epoch [22/50], Train Loss: 0.8283, Train Accuracy: 0.5425, Test Loss: 0.9720, Test Accuracy: 0.4013\n",
      "\n",
      "Allocated Memory: 0.00 MB\n",
      "Cached Memory: 2.00 MB\n",
      "Epoch [1/50], Train Loss: 1.0142, Train Accuracy: 0.4203, Test Loss: 1.0984, Test Accuracy: 0.2038\n",
      "Epoch [2/50], Train Loss: 0.9167, Train Accuracy: 0.4449, Test Loss: 0.9269, Test Accuracy: 0.5960\n",
      "Epoch [3/50], Train Loss: 0.9051, Train Accuracy: 0.4708, Test Loss: 0.9426, Test Accuracy: 0.4716\n",
      "Epoch [4/50], Train Loss: 0.8981, Train Accuracy: 0.4857, Test Loss: 0.9864, Test Accuracy: 0.4139\n",
      "Epoch [5/50], Train Loss: 0.8804, Train Accuracy: 0.4983, Test Loss: 0.9464, Test Accuracy: 0.4491\n",
      "Epoch [6/50], Train Loss: 0.8962, Train Accuracy: 0.4733, Test Loss: 0.9550, Test Accuracy: 0.4869\n",
      "Epoch [7/50], Train Loss: 0.8802, Train Accuracy: 0.4855, Test Loss: 0.9457, Test Accuracy: 0.4563\n",
      "Epoch [8/50], Train Loss: 0.8680, Train Accuracy: 0.5035, Test Loss: 0.9893, Test Accuracy: 0.3895\n",
      "Epoch [9/50], Train Loss: 0.8662, Train Accuracy: 0.5053, Test Loss: 0.9225, Test Accuracy: 0.5852\n",
      "Epoch [10/50], Train Loss: 0.8661, Train Accuracy: 0.5103, Test Loss: 0.9330, Test Accuracy: 0.4743\n",
      "Epoch [11/50], Train Loss: 0.8679, Train Accuracy: 0.5051, Test Loss: 1.0201, Test Accuracy: 0.3751\n",
      "Epoch [12/50], Train Loss: 0.8591, Train Accuracy: 0.4934, Test Loss: 0.9134, Test Accuracy: 0.5158\n",
      "Epoch [13/50], Train Loss: 0.8593, Train Accuracy: 0.5003, Test Loss: 0.9315, Test Accuracy: 0.5546\n",
      "Epoch [14/50], Train Loss: 0.8562, Train Accuracy: 0.5258, Test Loss: 0.9853, Test Accuracy: 0.4301\n",
      "Epoch [15/50], Train Loss: 0.8535, Train Accuracy: 0.4988, Test Loss: 0.9537, Test Accuracy: 0.4716\n",
      "Epoch [16/50], Train Loss: 0.8554, Train Accuracy: 0.5044, Test Loss: 0.8612, Test Accuracy: 0.5807\n",
      "Epoch [17/50], Train Loss: 0.8438, Train Accuracy: 0.5245, Test Loss: 0.9054, Test Accuracy: 0.5176\n",
      "Epoch [18/50], Train Loss: 0.8403, Train Accuracy: 0.5281, Test Loss: 0.9555, Test Accuracy: 0.4391\n",
      "Epoch [19/50], Train Loss: 0.8382, Train Accuracy: 0.5208, Test Loss: 0.9451, Test Accuracy: 0.4418\n",
      "Epoch [20/50], Train Loss: 0.8495, Train Accuracy: 0.5148, Test Loss: 1.0084, Test Accuracy: 0.4373\n",
      "Epoch [21/50], Train Loss: 0.8451, Train Accuracy: 0.5096, Test Loss: 0.9135, Test Accuracy: 0.5176\n",
      "Epoch [22/50], Train Loss: 0.8484, Train Accuracy: 0.5152, Test Loss: 0.8519, Test Accuracy: 0.6087\n",
      "Epoch [23/50], Train Loss: 0.8427, Train Accuracy: 0.5215, Test Loss: 0.9576, Test Accuracy: 0.4797\n",
      "Epoch [24/50], Train Loss: 0.8297, Train Accuracy: 0.5204, Test Loss: 0.8541, Test Accuracy: 0.5627\n",
      "Epoch [25/50], Train Loss: 0.8313, Train Accuracy: 0.5181, Test Loss: 0.9686, Test Accuracy: 0.4364\n",
      "Epoch [26/50], Train Loss: 0.8316, Train Accuracy: 0.5348, Test Loss: 0.8611, Test Accuracy: 0.5717\n",
      "Epoch [27/50], Train Loss: 0.8326, Train Accuracy: 0.5299, Test Loss: 0.8663, Test Accuracy: 0.5410\n",
      "Epoch [28/50], Train Loss: 0.8409, Train Accuracy: 0.5240, Test Loss: 0.9141, Test Accuracy: 0.5374\n",
      "Epoch [29/50], Train Loss: 0.8277, Train Accuracy: 0.5364, Test Loss: 0.8773, Test Accuracy: 0.5410\n",
      "Epoch [30/50], Train Loss: 0.8258, Train Accuracy: 0.5378, Test Loss: 0.8871, Test Accuracy: 0.5266\n",
      "Epoch [31/50], Train Loss: 0.8247, Train Accuracy: 0.5384, Test Loss: 0.9418, Test Accuracy: 0.4644\n",
      "Epoch [32/50], Train Loss: 0.8221, Train Accuracy: 0.5540, Test Loss: 0.9245, Test Accuracy: 0.4923\n",
      "Epoch [33/50], Train Loss: 0.8341, Train Accuracy: 0.5420, Test Loss: 0.9638, Test Accuracy: 0.4644\n",
      "Epoch [34/50], Train Loss: 0.8238, Train Accuracy: 0.5314, Test Loss: 0.8909, Test Accuracy: 0.5275\n",
      "Epoch [35/50], Train Loss: 0.8177, Train Accuracy: 0.5387, Test Loss: 0.9157, Test Accuracy: 0.4950\n",
      "Epoch [36/50], Train Loss: 0.8227, Train Accuracy: 0.5337, Test Loss: 0.9598, Test Accuracy: 0.4536\n",
      "Epoch [37/50], Train Loss: 0.8234, Train Accuracy: 0.5355, Test Loss: 0.8600, Test Accuracy: 0.5537\n",
      "Epoch [38/50], Train Loss: 0.8170, Train Accuracy: 0.5368, Test Loss: 0.9311, Test Accuracy: 0.5122\n",
      "Epoch [39/50], Train Loss: 0.8153, Train Accuracy: 0.5409, Test Loss: 0.8958, Test Accuracy: 0.5284\n",
      "Epoch [40/50], Train Loss: 0.8206, Train Accuracy: 0.5341, Test Loss: 0.8240, Test Accuracy: 0.6078\n",
      "Epoch [41/50], Train Loss: 0.8117, Train Accuracy: 0.5337, Test Loss: 0.9073, Test Accuracy: 0.5149\n",
      "Epoch [42/50], Train Loss: 0.8105, Train Accuracy: 0.5227, Test Loss: 0.8578, Test Accuracy: 0.5690\n",
      "Epoch [43/50], Train Loss: 0.8143, Train Accuracy: 0.5488, Test Loss: 0.8481, Test Accuracy: 0.5699\n",
      "Epoch [44/50], Train Loss: 0.8129, Train Accuracy: 0.5483, Test Loss: 0.8622, Test Accuracy: 0.5573\n",
      "Epoch [45/50], Train Loss: 0.8071, Train Accuracy: 0.5391, Test Loss: 0.8890, Test Accuracy: 0.5131\n",
      "Epoch [46/50], Train Loss: 0.8098, Train Accuracy: 0.5350, Test Loss: 0.8646, Test Accuracy: 0.5392\n",
      "Epoch [47/50], Train Loss: 0.8062, Train Accuracy: 0.5445, Test Loss: 0.8981, Test Accuracy: 0.5059\n",
      "Epoch [48/50], Train Loss: 0.8022, Train Accuracy: 0.5452, Test Loss: 0.9035, Test Accuracy: 0.4923\n",
      "Epoch [49/50], Train Loss: 0.8048, Train Accuracy: 0.5326, Test Loss: 0.8920, Test Accuracy: 0.5212\n",
      "Epoch [50/50], Train Loss: 0.8003, Train Accuracy: 0.5378, Test Loss: 0.8861, Test Accuracy: 0.5239\n",
      "Allocated Memory: 0.00 MB\n",
      "Cached Memory: 2.00 MB\n",
      "Epoch [1/50], Train Loss: 1.0289, Train Accuracy: 0.4084, Test Loss: 0.8106, Test Accuracy: 0.7015\n",
      "Epoch [2/50], Train Loss: 0.9237, Train Accuracy: 0.4444, Test Loss: 0.9396, Test Accuracy: 0.4229\n",
      "Epoch [3/50], Train Loss: 0.8877, Train Accuracy: 0.4514, Test Loss: 0.8472, Test Accuracy: 0.6817\n",
      "Epoch [4/50], Train Loss: 0.8900, Train Accuracy: 0.4690, Test Loss: 0.8509, Test Accuracy: 0.6276\n",
      "Epoch [5/50], Train Loss: 0.8818, Train Accuracy: 0.4990, Test Loss: 0.8275, Test Accuracy: 0.6979\n",
      "Epoch [6/50], Train Loss: 0.8804, Train Accuracy: 0.4803, Test Loss: 0.8636, Test Accuracy: 0.5816\n",
      "Epoch [7/50], Train Loss: 0.8630, Train Accuracy: 0.5098, Test Loss: 0.9261, Test Accuracy: 0.4130\n",
      "Epoch [8/50], Train Loss: 0.8649, Train Accuracy: 0.4810, Test Loss: 0.8572, Test Accuracy: 0.5744\n",
      "Epoch [9/50], Train Loss: 0.8687, Train Accuracy: 0.5166, Test Loss: 0.8736, Test Accuracy: 0.5104\n",
      "Epoch [10/50], Train Loss: 0.8522, Train Accuracy: 0.5195, Test Loss: 0.9000, Test Accuracy: 0.5681\n",
      "Epoch [11/50], Train Loss: 0.8560, Train Accuracy: 0.4983, Test Loss: 0.8993, Test Accuracy: 0.5140\n",
      "Epoch [12/50], Train Loss: 0.8480, Train Accuracy: 0.5143, Test Loss: 0.8743, Test Accuracy: 0.5401\n",
      "Epoch [13/50], Train Loss: 0.8503, Train Accuracy: 0.5062, Test Loss: 0.9176, Test Accuracy: 0.4851\n",
      "Epoch [14/50], Train Loss: 0.8566, Train Accuracy: 0.5170, Test Loss: 0.8530, Test Accuracy: 0.5392\n",
      "Epoch [15/50], Train Loss: 0.8459, Train Accuracy: 0.5125, Test Loss: 0.9786, Test Accuracy: 0.4842\n",
      "Epoch [16/50], Train Loss: 0.8491, Train Accuracy: 0.5132, Test Loss: 0.8311, Test Accuracy: 0.5374\n",
      "Epoch [17/50], Train Loss: 0.8428, Train Accuracy: 0.5204, Test Loss: 0.8435, Test Accuracy: 0.5816\n",
      "Epoch [18/50], Train Loss: 0.8438, Train Accuracy: 0.5199, Test Loss: 0.9036, Test Accuracy: 0.4671\n",
      "Epoch [19/50], Train Loss: 0.8361, Train Accuracy: 0.5127, Test Loss: 0.8519, Test Accuracy: 0.5473\n",
      "Epoch [20/50], Train Loss: 0.8497, Train Accuracy: 0.5202, Test Loss: 0.8911, Test Accuracy: 0.5555\n",
      "Epoch [21/50], Train Loss: 0.8397, Train Accuracy: 0.5240, Test Loss: 0.8430, Test Accuracy: 0.5744\n",
      "Epoch [22/50], Train Loss: 0.8419, Train Accuracy: 0.5190, Test Loss: 0.8698, Test Accuracy: 0.5248\n",
      "Epoch [23/50], Train Loss: 0.8308, Train Accuracy: 0.5179, Test Loss: 0.8381, Test Accuracy: 0.6159\n",
      "Epoch [24/50], Train Loss: 0.8328, Train Accuracy: 0.5213, Test Loss: 0.8796, Test Accuracy: 0.5104\n",
      "Early Stop\n",
      "Epoch [25/50], Train Loss: 0.8342, Train Accuracy: 0.5305, Test Loss: 0.9342, Test Accuracy: 0.4328\n",
      "\n",
      "Allocated Memory: 0.00 MB\n",
      "Cached Memory: 2.00 MB\n"
     ]
    }
   ],
   "source": [
    "exp_name = f'b-one-head-{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "# kf = KFold(n_splits=5, shuffle=True, random_state=25)\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X_balanced['genebody'])):\n",
    "    \n",
    "    X_train_genebody, X_test_genebody = [X_balanced['genebody'][i] for i in train_index], [X_balanced['genebody'][i] for i in test_index]\n",
    "    \n",
    "    y_train, y_test = [y_balanced[i] for i in train_index], [y_balanced[i] for i in test_index]\n",
    "    \n",
    "    X_train_genebody_normalized, X_test_genebody_normalized = normalize_features(X_train_genebody, X_test_genebody)\n",
    "\n",
    "    elem_stats, epoch_stats, model = train_model(X_train_genebody_normalized, y_train,\n",
    "                                            X_test_genebody_normalized, y_test,\n",
    "                                            exp_name=exp_name, fold_idx=i)\n",
    "\n",
    "    df_elem = pd.DataFrame({'preds' : elem_stats[0], 'labels' : elem_stats[1]})\n",
    "    df_epoch = pd.DataFrame({'train_acc' : epoch_stats[0], 'train_loss' : epoch_stats[1], \n",
    "                             'test_acc' : epoch_stats[2], 'test_loss' : epoch_stats[3]})\n",
    "\n",
    "    df_elem.to_csv(f\"../baseline_results/{_ct}/genebody/imbal_elem_{i}.csv\")\n",
    "    df_epoch.to_csv(f\"../baseline_results/{_ct}/genebody/imbal_epoch_{i}.csv\")\n",
    "    \n",
    "    torch.save(model, f\"../baseline_results/{_ct}/genebody/imbal_model_{i}.pt\")\n",
    "    del model\n",
    "    del elem_stats, epoch_stats\n",
    "    del X_train_genebody, X_test_genebody, X_train_genebody_normalized, X_test_genebody_normalized, y_train, y_test\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Allocated Memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "    print(f\"Cached Memory: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35d1fd4-7dac-4d86-a3f5-e22b4bb5077c",
   "metadata": {},
   "source": [
    "## Clear GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5251625f-cf70-4d30-a6bc-990c48c9ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11637d7c-d1bb-4be5-b54a-4d756d300727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated Memory: 0.00 MB\n",
      "Cached Memory: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Allocated Memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"Cached Memory: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cfbc3c-8d33-47dd-abca-016b21bf3f63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
