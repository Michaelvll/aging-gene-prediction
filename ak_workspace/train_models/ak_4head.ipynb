{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1344dcf7-1b1c-42ab-809c-947c2bf513dc",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28350337-3020-4577-ba6d-4bc5570c76d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gale/netapp/home2/aklein/miniconda3/envs/agp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from data_loader import load_data, get_balanced_data, normalize_features\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# import wandb\n",
    "# wandb.init(project='gene')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56e2aff-adfd-46c8-8927-81d717be36f6",
   "metadata": {},
   "source": [
    "## Session Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc00e1b1-318c-46a0-9528-06185f599a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.15\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ee864cc-a9b9-43af-956a-85d8d7be4552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n",
      "1.26.4\n",
      "True\n",
      "11.3\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__), print(np.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a381bde2-4b73-42ed-a1c5-92c6d0e31786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 NVIDIA A40\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.current_device(), torch.cuda.device_count(), torch.cuda.get_device_name(0))\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc95c83-da76-49c0-8648-b9c1c3e413a6",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b7ffd90-ec2b-4e3e-8445-5bdd7cd5e93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed mcg data\n",
      "Processed genebody data\n",
      "Processed atac data\n",
      "Processed hic data\n",
      "zero: 4115, down: 874, up: 557\n",
      "mcg 1671\n",
      "genebody 1671\n",
      "atac 1671\n",
      "hic 1671\n"
     ]
    }
   ],
   "source": [
    "data = load_data(y_val = \"DEG\", ct=\"Oligo_NN\")\n",
    "X_balanced, y_balanced = get_balanced_data(data, method='balanced', y_val=\"DEG\")\n",
    "FEATURE_TYPES = ['mcg', 'atac', 'hic', 'genebody']\n",
    "for k, v in X_balanced.items():\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971a929a-821e-4dac-8d33-4197bc81dd57",
   "metadata": {},
   "source": [
    "## Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5609c5bf-94fc-4cfd-9a03-c1adc1e511d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 64\n",
    "NUM_LAYERS = 4\n",
    "NUM_HEADS = 8\n",
    "DROPOUT = 0.1\n",
    "LR = 0.001\n",
    "OUTPUT_DIM = 3  # number of classes (-1, 0, 1)\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "class FourHeadTransformerModel(nn.Module):\n",
    "    def __init__(self, input_1_dim, input_2_dim, input_3_dim, input_4_dim, hidden_dim, output_dim, num_layers=2, num_heads=1, dropout=0.1):\n",
    "        super(FourHeadTransformerModel, self).__init__()\n",
    "        # For the tokens\n",
    "        self.cls_1_embedding = nn.Parameter(torch.randn(1, 1, hidden_dim))\n",
    "        self.cls_2_embedding = nn.Parameter(torch.randn(1, 1, hidden_dim))\n",
    "        self.cls_3_embedding = nn.Parameter(torch.randn(1, 1, hidden_dim))\n",
    "        self.cls_4_embedding = nn.Parameter(torch.randn(1, 1, hidden_dim))\n",
    "        self.combined_cls_embedding = nn.Parameter(torch.randn(1, 1, hidden_dim))\n",
    "        # embedding layer of the inputs\n",
    "        self.input_1_embedding = nn.Linear(input_1_dim, hidden_dim)\n",
    "        self.input_2_embedding = nn.Linear(input_2_dim, hidden_dim)\n",
    "        self.input_3_embedding = nn.Linear(input_3_dim, hidden_dim)\n",
    "        self.input_4_embedding = nn.Linear(input_4_dim, hidden_dim)\n",
    "\n",
    "        # Transformer layers\n",
    "        # TODO: may need to use tanh in attention instead of softmax\n",
    "        encoder_layers = nn.TransformerEncoderLayer(hidden_dim, num_heads, dim_feedforward=hidden_dim*4, dropout=dropout, batch_first=True, norm_first=True)\n",
    "        self.transformer_1 = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.transformer_2 = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.transformer_3 = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.transformer_4 = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        \n",
    "        # Use self-attention mechanism to combine the outputs of the four heads\n",
    "        self.linear = nn.Linear(hidden_dim * 4, hidden_dim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.classifier = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, _x1, _x2, _x3, _x4, x1_mask, x2_mask, x3_mask, x4_mask):\n",
    "        _x1 = self.input_1_embedding(_x1)\n",
    "        _x2 = self.input_2_embedding(_x2)\n",
    "        _x3 = self.input_3_embedding(_x3)\n",
    "        _x4 = self.input_4_embedding(_x4)\n",
    "\n",
    "        # Add cls embedding, so that we can take the first output as the representation of the sequence\n",
    "        _cls_1_embedding = self.cls_1_embedding.repeat(_x1.size(0), 1, 1)\n",
    "        _cls_2_embedding = self.cls_2_embedding.repeat(_x2.size(0), 1, 1)\n",
    "        _cls_3_embedding = self.cls_3_embedding.repeat(_x3.size(0), 1, 1)\n",
    "        _cls_4_embedding = self.cls_4_embedding.repeat(_x4.size(0), 1, 1)\n",
    "\n",
    "        _z1 = torch.cat((_cls_1_embedding, _x1), dim=1)\n",
    "        _z2 = torch.cat((_cls_2_embedding, _x2), dim=1)\n",
    "        _z3 = torch.cat((_cls_3_embedding, _x3), dim=1)\n",
    "        _z4 = torch.cat((_cls_4_embedding, _x4), dim=1)\n",
    "        \n",
    "        _mask_1 = torch.cat((torch.ones(x1_mask.size(0), 1).to(device), x1_mask), dim=1)\n",
    "        _mask_2 = torch.cat((torch.ones(x2_mask.size(0), 1).to(device), x2_mask), dim=1)\n",
    "        _mask_3 = torch.cat((torch.ones(x3_mask.size(0), 1).to(device), x3_mask), dim=1)\n",
    "        _mask_4 = torch.cat((torch.ones(x4_mask.size(0), 1).to(device), x4_mask), dim=1)\n",
    "        \n",
    "        _z1 = self.transformer_1(_z1, src_key_padding_mask=~_mask_1.bool())\n",
    "        _z2 = self.transformer_2(_z2, src_key_padding_mask=~_mask_2.bool())\n",
    "        _z3 = self.transformer_3(_z3, src_key_padding_mask=~_mask_3.bool())\n",
    "        _z4 = self.transformer_3(_z4, src_key_padding_mask=~_mask_4.bool())\n",
    "    \n",
    "        \n",
    "        # Pooling, we should not use average pooling since the sequence length is also important\n",
    "        _z1 = _z1[:, 0, :]\n",
    "        _z2 = _z2[:, 0, :]\n",
    "        _z3 = _z3[:, 0, :]\n",
    "        _z4 = _z4[:, 0, :]\n",
    "        \n",
    "        # Combine the output of three heads into a sequence\n",
    "        _z = torch.concat((_z1, _z2, _z3, _z4), dim=1)\n",
    "        _z = self.linear(_z)\n",
    "        _z = _z.squeeze(-1) # (batch_size, hidden_dim, 1)\n",
    "        _z = self.relu(_z)\n",
    "        # Final classifier\n",
    "        output = self.classifier(_z)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d88750ba-a3e7-4d56-9bcf-81ab408e32ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Object\n",
    "class FourGeneDataset(Dataset):\n",
    "    def __init__(self, _data1, _data2, _data3, _data4, labels):\n",
    "        self.data1 = _data1\n",
    "        self.data2 = _data2\n",
    "        self.data3 = _data3\n",
    "        self.data4 = _data4\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data1 = torch.FloatTensor(self.data1[idx]).to(device)\n",
    "        data2 = torch.FloatTensor(self.data2[idx]).to(device)\n",
    "        data3 = torch.FloatTensor(self.data3[idx]).to(device)\n",
    "        data4 = torch.FloatTensor(self.data4[idx]).to(device)\n",
    "        \n",
    "        label = torch.LongTensor([self.labels[idx] + 1]).to(device)  # Add 1 to shift labels to 0, 1, 2\n",
    "        \n",
    "        mask1 = torch.ones(len(data1)).to(device)\n",
    "        mask2 = torch.ones(len(data2)).to(device)\n",
    "        mask3 = torch.ones(len(data3)).to(device)\n",
    "        mask4 = torch.ones(len(data4)).to(device)\n",
    "        \n",
    "        return data1, data2, data3, data4, label, mask1, mask2, mask3, mask4\n",
    "\n",
    "## handling the batches \n",
    "def comb_collate_fn(batch):\n",
    "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    d1_sequences, d2_sequences, d3_sequences, d4_sequences, labels, d1_masks, d2_masks, d3_masks, d4_masks = zip(*batch)\n",
    "    \n",
    "    d1_lengths = [len(seq) for seq in d1_sequences]\n",
    "    d2_lengths = [len(seq) for seq in d2_sequences]\n",
    "    d3_lengths = [len(seq) for seq in d3_sequences]\n",
    "    d4_lengths = [len(seq) for seq in d4_sequences]\n",
    "    d1_max_len = max(d1_lengths)\n",
    "    d2_max_len = max(d2_lengths)\n",
    "    d3_max_len = max(d3_lengths)\n",
    "    d4_max_len = max(d4_lengths)\n",
    "    \n",
    "    d1_padded_seqs = torch.zeros(len(d1_sequences), d1_max_len, d1_sequences[0].size(1))\n",
    "    d2_padded_seqs = torch.zeros(len(d2_sequences), d2_max_len, d2_sequences[0].size(1))\n",
    "    d3_padded_seqs = torch.zeros(len(d3_sequences), d3_max_len, d3_sequences[0].size(1))\n",
    "    d4_padded_seqs = torch.zeros(len(d4_sequences), d4_max_len, d4_sequences[0].size(1))\n",
    "    \n",
    "    d1_padded_masks = torch.zeros(len(d1_sequences), d1_max_len)\n",
    "    d2_padded_masks = torch.zeros(len(d2_sequences), d2_max_len)\n",
    "    d3_padded_masks = torch.zeros(len(d3_sequences), d3_max_len)\n",
    "    d4_padded_masks = torch.zeros(len(d4_sequences), d4_max_len)\n",
    "    \n",
    "    for i, (d1_seq, d2_seq, d3_seq, d4_seq, d1_length, d2_length, d3_length, d4_length) in enumerate(zip(d1_sequences, d2_sequences, d3_sequences, d4_sequences, d1_lengths, d2_lengths, d3_lengths, d4_lengths)):\n",
    "        d1_padded_seqs[i, :d1_length] = d1_seq\n",
    "        d2_padded_seqs[i, :d2_length] = d2_seq\n",
    "        d3_padded_seqs[i, :d3_length] = d3_seq\n",
    "        d4_padded_seqs[i, :d4_length] = d4_seq\n",
    "        \n",
    "        d1_padded_masks[i, :d1_length] = 1 \n",
    "        d2_padded_masks[i, :d2_length] = 1 \n",
    "        d3_padded_masks[i, :d3_length] = 1 \n",
    "        d4_padded_masks[i, :d4_length] = 1 \n",
    "        \n",
    "    return d1_padded_seqs.to(device), d2_padded_seqs.to(device), d3_padded_seqs.to(device), d4_padded_seqs.to(device), torch.cat(labels), d1_padded_masks.to(device), d2_padded_masks.to(device), d3_padded_masks.to(device), d4_padded_masks.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89e3f634-cd7e-484d-9223-63e7c1b6b152",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "def train_4head_model(X_train_1, X_train_2, X_train_3, X_train_4, y_train, X_test_1, X_test_2, X_test_3, X_test_4, y_test, exp_name, fold_idx):\n",
    "    #wandb.init(project='gene', group=exp_name, name=f'fold-{fold_idx}')\n",
    "    input_1_dim = len(X_train_1[0][0])\n",
    "    input_2_dim = len(X_train_2[0][0])\n",
    "    input_3_dim = len(X_train_3[0][0])\n",
    "    input_4_dim = len(X_train_4[0][0])\n",
    "    \n",
    "    train_dataset = FourGeneDataset(X_train_1, X_train_2, X_train_3, X_train_4, y_train)\n",
    "    test_dataset = FourGeneDataset(X_test_1, X_test_2, X_test_3, X_test_4, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=comb_collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=comb_collate_fn)\n",
    "\n",
    "    model = FourHeadTransformerModel(input_1_dim, input_2_dim, input_3_dim, input_4_dim, HIDDEN_DIM, OUTPUT_DIM, num_layers=NUM_LAYERS, num_heads=NUM_HEADS, dropout=DROPOUT).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=0.0001)\n",
    "    # Create the OneCycleLR scheduler\n",
    "    # lr_scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=LR, total_steps=NUM_EPOCHS,\n",
    "    #                           pct_start=0.8, anneal_strategy='cos',\n",
    "    #                           cycle_momentum=False, div_factor=5.0,\n",
    "    #                           final_div_factor=10.0)\n",
    "\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    early_stopper = EarlyStopper(patience=6, min_delta=0.05)\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        for x1, x2, x3, x4, batch_y, mask1, mask2, mask3, mask4 in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x1, x2, x3, x4, mask1, mask2, mask3, mask4)\n",
    "            loss = criterion(outputs, batch_y.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            train_correct += (outputs.argmax(dim=1) == batch_y.squeeze()).sum().item()\n",
    "            train_total += batch_y.size(0)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x1, x2, x3, x4, batch_y, mask1, mask2, mask3, mask4 in test_loader:\n",
    "                outputs = model(x1, x2, x3, x4, mask1, mask2, mask3, mask4)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                test_loss += criterion(outputs, batch_y.squeeze()).item()\n",
    "                total += batch_y.size(0)\n",
    "                correct += (predicted == batch_y.squeeze()).sum().item()\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        train_accuracies.append(train_correct/train_total)\n",
    "        train_losses.append(total_loss/len(train_loader))\n",
    "        test_accuracies.append(accuracy)\n",
    "        test_losses.append(test_loss / len(test_loader))\n",
    "        print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {total_loss/len(train_loader):.4f}, Train Accuracy: {train_correct/train_total:.4f}, Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {accuracy:.4f}')\n",
    "        #wandb.log({'epoch': epoch, 'LR': optimizer.param_groups[0]['lr'], 'train_loss': total_loss/len(train_loader), 'train_accuracy': train_correct/train_total, 'test_accuracy': accuracy})\n",
    "        if early_stopper.early_stop( test_loss / len(test_loader) ): \n",
    "            print(\"Early Stop\")\n",
    "            break\n",
    "\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, x3, x4, batch_y, mask1, mask2, mask3, mask4 in test_loader:\n",
    "            outputs = model(x1, x2, x3, x4, mask1, mask2, mask3, mask4)\n",
    "            _, predicted = torch.max(outputs.detach().cpu().data, 1)\n",
    "            all_predictions.extend(predicted.numpy())\n",
    "            all_labels.extend(batch_y.detach().cpu().numpy())\n",
    "\n",
    "    # final_accuracy = sum(np.array(all_predictions) == np.array(all_labels).squeeze()) / len(all_labels)\n",
    "    # print(f'Final Test Accuracy: {final_accuracy:.4f}')\n",
    "    return (all_predictions, all_labels), (train_accuracies, train_losses, test_accuracies, test_losses), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c2d6fcf-d302-49a3-959b-0d8592415f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4026c5eb-2fac-49ee-a45e-96b2e92aa608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 1.0589, Train Accuracy: 0.4379, Test Loss: 0.9267, Test Accuracy: 0.5552\n",
      "Epoch [2/50], Train Loss: 0.9058, Train Accuracy: 0.5576, Test Loss: 0.8421, Test Accuracy: 0.6388\n",
      "Epoch [3/50], Train Loss: 0.8508, Train Accuracy: 0.5793, Test Loss: 0.8127, Test Accuracy: 0.5910\n",
      "Epoch [4/50], Train Loss: 0.8123, Train Accuracy: 0.6048, Test Loss: 0.8046, Test Accuracy: 0.6478\n",
      "Epoch [5/50], Train Loss: 0.8428, Train Accuracy: 0.5988, Test Loss: 0.8029, Test Accuracy: 0.6060\n",
      "Epoch [6/50], Train Loss: 0.7962, Train Accuracy: 0.6213, Test Loss: 0.8318, Test Accuracy: 0.6299\n",
      "Epoch [7/50], Train Loss: 0.7803, Train Accuracy: 0.6280, Test Loss: 0.7730, Test Accuracy: 0.6358\n",
      "Epoch [8/50], Train Loss: 0.7851, Train Accuracy: 0.6070, Test Loss: 0.8330, Test Accuracy: 0.6209\n",
      "Epoch [9/50], Train Loss: 0.7643, Train Accuracy: 0.6385, Test Loss: 0.7589, Test Accuracy: 0.6537\n",
      "Epoch [10/50], Train Loss: 0.7464, Train Accuracy: 0.6407, Test Loss: 0.7610, Test Accuracy: 0.6299\n",
      "Epoch [11/50], Train Loss: 0.7262, Train Accuracy: 0.6647, Test Loss: 0.7836, Test Accuracy: 0.6507\n",
      "Epoch [12/50], Train Loss: 0.7398, Train Accuracy: 0.6385, Test Loss: 0.7361, Test Accuracy: 0.6448\n",
      "Epoch [13/50], Train Loss: 0.7496, Train Accuracy: 0.6407, Test Loss: 0.7937, Test Accuracy: 0.5881\n",
      "Epoch [14/50], Train Loss: 0.7483, Train Accuracy: 0.6497, Test Loss: 0.7810, Test Accuracy: 0.6478\n",
      "Epoch [15/50], Train Loss: 0.7452, Train Accuracy: 0.6280, Test Loss: 0.7803, Test Accuracy: 0.6179\n",
      "Epoch [16/50], Train Loss: 0.7238, Train Accuracy: 0.6624, Test Loss: 0.7576, Test Accuracy: 0.6448\n",
      "Epoch [17/50], Train Loss: 0.7142, Train Accuracy: 0.6542, Test Loss: 0.7583, Test Accuracy: 0.6537\n",
      "Epoch [18/50], Train Loss: 0.7210, Train Accuracy: 0.6602, Test Loss: 0.7595, Test Accuracy: 0.6448\n",
      "Epoch [19/50], Train Loss: 0.7085, Train Accuracy: 0.6737, Test Loss: 0.7386, Test Accuracy: 0.6537\n",
      "Epoch [20/50], Train Loss: 0.6875, Train Accuracy: 0.6796, Test Loss: 0.8493, Test Accuracy: 0.6000\n",
      "Epoch [21/50], Train Loss: 0.7146, Train Accuracy: 0.6632, Test Loss: 0.7701, Test Accuracy: 0.6328\n",
      "Epoch [22/50], Train Loss: 0.6971, Train Accuracy: 0.6729, Test Loss: 0.7991, Test Accuracy: 0.6269\n",
      "Epoch [32/50], Train Loss: 0.6296, Train Accuracy: 0.6978, Test Loss: 0.7569, Test Accuracy: 0.6557\n",
      "Epoch [33/50], Train Loss: 0.6287, Train Accuracy: 0.7016, Test Loss: 0.7372, Test Accuracy: 0.6617\n",
      "Epoch [34/50], Train Loss: 0.6199, Train Accuracy: 0.7053, Test Loss: 0.7429, Test Accuracy: 0.6737\n",
      "Epoch [35/50], Train Loss: 0.6225, Train Accuracy: 0.7240, Test Loss: 0.7386, Test Accuracy: 0.6317\n",
      "Epoch [36/50], Train Loss: 0.6063, Train Accuracy: 0.7091, Test Loss: 0.7691, Test Accuracy: 0.6647\n",
      "Epoch [37/50], Train Loss: 0.5976, Train Accuracy: 0.7128, Test Loss: 0.7373, Test Accuracy: 0.6377\n",
      "Epoch [38/50], Train Loss: 0.6008, Train Accuracy: 0.7218, Test Loss: 0.7473, Test Accuracy: 0.6557\n",
      "Epoch [39/50], Train Loss: 0.5917, Train Accuracy: 0.7165, Test Loss: 0.7530, Test Accuracy: 0.6647\n",
      "Epoch [40/50], Train Loss: 0.5758, Train Accuracy: 0.7225, Test Loss: 0.7733, Test Accuracy: 0.6587\n",
      "Epoch [41/50], Train Loss: 0.5727, Train Accuracy: 0.7322, Test Loss: 0.7718, Test Accuracy: 0.6737\n",
      "Early Stop\n",
      "\n",
      "Epoch [1/50], Train Loss: 1.0639, Train Accuracy: 0.4428, Test Loss: 0.9332, Test Accuracy: 0.5659\n",
      "Epoch [2/50], Train Loss: 0.9466, Train Accuracy: 0.5542, Test Loss: 0.8230, Test Accuracy: 0.5988\n",
      "Epoch [3/50], Train Loss: 0.9028, Train Accuracy: 0.5580, Test Loss: 0.9136, Test Accuracy: 0.5389\n",
      "Epoch [4/50], Train Loss: 0.8781, Train Accuracy: 0.5625, Test Loss: 0.8452, Test Accuracy: 0.5778\n",
      "Epoch [5/50], Train Loss: 0.8600, Train Accuracy: 0.5864, Test Loss: 0.7654, Test Accuracy: 0.6287\n",
      "Epoch [6/50], Train Loss: 0.8388, Train Accuracy: 0.5826, Test Loss: 0.7539, Test Accuracy: 0.6287\n",
      "Epoch [7/50], Train Loss: 0.8400, Train Accuracy: 0.5946, Test Loss: 0.7698, Test Accuracy: 0.6287\n",
      "Epoch [8/50], Train Loss: 0.7966, Train Accuracy: 0.6208, Test Loss: 0.7527, Test Accuracy: 0.6287\n",
      "Epoch [9/50], Train Loss: 0.7820, Train Accuracy: 0.6305, Test Loss: 0.7489, Test Accuracy: 0.6377\n",
      "Epoch [10/50], Train Loss: 0.7877, Train Accuracy: 0.6178, Test Loss: 0.7359, Test Accuracy: 0.6527\n",
      "Epoch [11/50], Train Loss: 0.7650, Train Accuracy: 0.6485, Test Loss: 0.8157, Test Accuracy: 0.6078\n",
      "Epoch [12/50], Train Loss: 0.7798, Train Accuracy: 0.6283, Test Loss: 0.7792, Test Accuracy: 0.6527\n",
      "Epoch [13/50], Train Loss: 0.7622, Train Accuracy: 0.6372, Test Loss: 0.7186, Test Accuracy: 0.6707\n",
      "Epoch [14/50], Train Loss: 0.7634, Train Accuracy: 0.6380, Test Loss: 0.7268, Test Accuracy: 0.6497\n",
      "Epoch [15/50], Train Loss: 0.7322, Train Accuracy: 0.6545, Test Loss: 0.7299, Test Accuracy: 0.6407\n",
      "Epoch [16/50], Train Loss: 0.7471, Train Accuracy: 0.6559, Test Loss: 0.7262, Test Accuracy: 0.6347\n",
      "Epoch [17/50], Train Loss: 0.7366, Train Accuracy: 0.6440, Test Loss: 0.7550, Test Accuracy: 0.6347\n",
      "Epoch [18/50], Train Loss: 0.7132, Train Accuracy: 0.6537, Test Loss: 0.7312, Test Accuracy: 0.6796\n",
      "Epoch [19/50], Train Loss: 0.7295, Train Accuracy: 0.6694, Test Loss: 0.7348, Test Accuracy: 0.6557\n",
      "Epoch [20/50], Train Loss: 0.7183, Train Accuracy: 0.6709, Test Loss: 0.6985, Test Accuracy: 0.6826\n",
      "Epoch [21/50], Train Loss: 0.7289, Train Accuracy: 0.6500, Test Loss: 0.7233, Test Accuracy: 0.6587\n",
      "Epoch [22/50], Train Loss: 0.7319, Train Accuracy: 0.6440, Test Loss: 0.7166, Test Accuracy: 0.6437\n",
      "Epoch [23/50], Train Loss: 0.6909, Train Accuracy: 0.6821, Test Loss: 0.7128, Test Accuracy: 0.6707\n",
      "Epoch [24/50], Train Loss: 0.6897, Train Accuracy: 0.6739, Test Loss: 0.7220, Test Accuracy: 0.6587\n",
      "Epoch [25/50], Train Loss: 0.6865, Train Accuracy: 0.6829, Test Loss: 0.7506, Test Accuracy: 0.6287\n",
      "Epoch [26/50], Train Loss: 0.6785, Train Accuracy: 0.6829, Test Loss: 0.7372, Test Accuracy: 0.6228\n",
      "Epoch [27/50], Train Loss: 0.6874, Train Accuracy: 0.6821, Test Loss: 0.7382, Test Accuracy: 0.6557\n",
      "Epoch [28/50], Train Loss: 0.6909, Train Accuracy: 0.6679, Test Loss: 0.7450, Test Accuracy: 0.6108\n",
      "Epoch [29/50], Train Loss: 0.6784, Train Accuracy: 0.6896, Test Loss: 0.7217, Test Accuracy: 0.6677\n",
      "Epoch [30/50], Train Loss: 0.6715, Train Accuracy: 0.6874, Test Loss: 0.7180, Test Accuracy: 0.6377\n",
      "Epoch [31/50], Train Loss: 0.6546, Train Accuracy: 0.6986, Test Loss: 0.7441, Test Accuracy: 0.6707\n",
      "Epoch [32/50], Train Loss: 0.6526, Train Accuracy: 0.6963, Test Loss: 0.7309, Test Accuracy: 0.6557\n",
      "Epoch [33/50], Train Loss: 0.6494, Train Accuracy: 0.7016, Test Loss: 0.7619, Test Accuracy: 0.6617\n",
      "Epoch [34/50], Train Loss: 0.6528, Train Accuracy: 0.7001, Test Loss: 0.7288, Test Accuracy: 0.6707\n",
      "Epoch [35/50], Train Loss: 0.6368, Train Accuracy: 0.7031, Test Loss: 0.7229, Test Accuracy: 0.6497\n",
      "Epoch [36/50], Train Loss: 0.6351, Train Accuracy: 0.7038, Test Loss: 0.7285, Test Accuracy: 0.6557\n",
      "Epoch [37/50], Train Loss: 0.6329, Train Accuracy: 0.7053, Test Loss: 0.7325, Test Accuracy: 0.6317\n",
      "Epoch [38/50], Train Loss: 0.6274, Train Accuracy: 0.7098, Test Loss: 0.7907, Test Accuracy: 0.6078\n",
      "Epoch [39/50], Train Loss: 0.6320, Train Accuracy: 0.7128, Test Loss: 0.7445, Test Accuracy: 0.6527\n",
      "Epoch [40/50], Train Loss: 0.6201, Train Accuracy: 0.7016, Test Loss: 0.7166, Test Accuracy: 0.6437\n",
      "Epoch [41/50], Train Loss: 0.6166, Train Accuracy: 0.7188, Test Loss: 0.7269, Test Accuracy: 0.6377\n",
      "Epoch [42/50], Train Loss: 0.6137, Train Accuracy: 0.7158, Test Loss: 0.7250, Test Accuracy: 0.6407\n",
      "Epoch [43/50], Train Loss: 0.6085, Train Accuracy: 0.7053, Test Loss: 0.7428, Test Accuracy: 0.6317\n",
      "Epoch [44/50], Train Loss: 0.6053, Train Accuracy: 0.7188, Test Loss: 0.7243, Test Accuracy: 0.6527\n",
      "Epoch [45/50], Train Loss: 0.6023, Train Accuracy: 0.7128, Test Loss: 0.7386, Test Accuracy: 0.6617\n",
      "Epoch [46/50], Train Loss: 0.6003, Train Accuracy: 0.7188, Test Loss: 0.7285, Test Accuracy: 0.6407\n",
      "Epoch [47/50], Train Loss: 0.5930, Train Accuracy: 0.7345, Test Loss: 0.7304, Test Accuracy: 0.6587\n",
      "Epoch [48/50], Train Loss: 0.5964, Train Accuracy: 0.7225, Test Loss: 0.7280, Test Accuracy: 0.6407\n",
      "Epoch [49/50], Train Loss: 0.5900, Train Accuracy: 0.7210, Test Loss: 0.7224, Test Accuracy: 0.6437\n",
      "Epoch [50/50], Train Loss: 0.5917, Train Accuracy: 0.7188, Test Loss: 0.7297, Test Accuracy: 0.6587\n",
      "\n",
      "Epoch [1/50], Train Loss: 0.9932, Train Accuracy: 0.5086, Test Loss: 0.9082, Test Accuracy: 0.5838\n",
      "Epoch [2/50], Train Loss: 0.9185, Train Accuracy: 0.5669, Test Loss: 0.9691, Test Accuracy: 0.5329\n",
      "Epoch [3/50], Train Loss: 0.8800, Train Accuracy: 0.5819, Test Loss: 0.9724, Test Accuracy: 0.5479\n",
      "Epoch [4/50], Train Loss: 0.8647, Train Accuracy: 0.5886, Test Loss: 0.9559, Test Accuracy: 0.5599\n",
      "Epoch [5/50], Train Loss: 0.8339, Train Accuracy: 0.5782, Test Loss: 0.8698, Test Accuracy: 0.5599\n",
      "Epoch [6/50], Train Loss: 0.7986, Train Accuracy: 0.6215, Test Loss: 0.7535, Test Accuracy: 0.6617\n",
      "Epoch [7/50], Train Loss: 0.8122, Train Accuracy: 0.6178, Test Loss: 0.7860, Test Accuracy: 0.6228\n",
      "Epoch [8/50], Train Loss: 0.7735, Train Accuracy: 0.6507, Test Loss: 0.9312, Test Accuracy: 0.5449\n",
      "Epoch [9/50], Train Loss: 0.7788, Train Accuracy: 0.6320, Test Loss: 0.7443, Test Accuracy: 0.6677\n",
      "Epoch [10/50], Train Loss: 0.7761, Train Accuracy: 0.6178, Test Loss: 0.7530, Test Accuracy: 0.6257\n",
      "Epoch [11/50], Train Loss: 0.7637, Train Accuracy: 0.6432, Test Loss: 0.7499, Test Accuracy: 0.6557\n",
      "Epoch [12/50], Train Loss: 0.7401, Train Accuracy: 0.6462, Test Loss: 0.7388, Test Accuracy: 0.6766\n",
      "Epoch [13/50], Train Loss: 0.7276, Train Accuracy: 0.6612, Test Loss: 0.7333, Test Accuracy: 0.6856\n",
      "Epoch [14/50], Train Loss: 0.7351, Train Accuracy: 0.6402, Test Loss: 0.7456, Test Accuracy: 0.6647\n",
      "Epoch [15/50], Train Loss: 0.7345, Train Accuracy: 0.6589, Test Loss: 0.7336, Test Accuracy: 0.6677\n",
      "Epoch [16/50], Train Loss: 0.7284, Train Accuracy: 0.6574, Test Loss: 0.7559, Test Accuracy: 0.6557\n",
      "Epoch [17/50], Train Loss: 0.7172, Train Accuracy: 0.6559, Test Loss: 0.7521, Test Accuracy: 0.6317\n",
      "Epoch [18/50], Train Loss: 0.7264, Train Accuracy: 0.6709, Test Loss: 0.8881, Test Accuracy: 0.6228\n",
      "Epoch [19/50], Train Loss: 0.7200, Train Accuracy: 0.6657, Test Loss: 0.7652, Test Accuracy: 0.6497\n",
      "Epoch [20/50], Train Loss: 0.7052, Train Accuracy: 0.6627, Test Loss: 0.7403, Test Accuracy: 0.6617\n",
      "Epoch [21/50], Train Loss: 0.7043, Train Accuracy: 0.6597, Test Loss: 0.7718, Test Accuracy: 0.6707\n",
      "Epoch [22/50], Train Loss: 0.7087, Train Accuracy: 0.6649, Test Loss: 0.7622, Test Accuracy: 0.6347\n",
      "Epoch [23/50], Train Loss: 0.6812, Train Accuracy: 0.6724, Test Loss: 0.7540, Test Accuracy: 0.6677\n",
      "Epoch [24/50], Train Loss: 0.6977, Train Accuracy: 0.6687, Test Loss: 0.7550, Test Accuracy: 0.6437\n",
      "Epoch [25/50], Train Loss: 0.6748, Train Accuracy: 0.6889, Test Loss: 0.7314, Test Accuracy: 0.6407\n",
      "Epoch [26/50], Train Loss: 0.6829, Train Accuracy: 0.6649, Test Loss: 0.7457, Test Accuracy: 0.6467\n",
      "Epoch [27/50], Train Loss: 0.6697, Train Accuracy: 0.6821, Test Loss: 0.7518, Test Accuracy: 0.6377\n",
      "Epoch [28/50], Train Loss: 0.6679, Train Accuracy: 0.6933, Test Loss: 0.7435, Test Accuracy: 0.6497\n",
      "Epoch [29/50], Train Loss: 0.6644, Train Accuracy: 0.6836, Test Loss: 0.7497, Test Accuracy: 0.6527\n",
      "Epoch [30/50], Train Loss: 0.6569, Train Accuracy: 0.6911, Test Loss: 0.7414, Test Accuracy: 0.6467\n",
      "Epoch [31/50], Train Loss: 0.6506, Train Accuracy: 0.6993, Test Loss: 0.8119, Test Accuracy: 0.6467\n",
      "Epoch [32/50], Train Loss: 0.6424, Train Accuracy: 0.7016, Test Loss: 0.7848, Test Accuracy: 0.6587\n",
      "Epoch [33/50], Train Loss: 0.6304, Train Accuracy: 0.7143, Test Loss: 0.7432, Test Accuracy: 0.6287\n",
      "Epoch [34/50], Train Loss: 0.6222, Train Accuracy: 0.7255, Test Loss: 0.7517, Test Accuracy: 0.6647\n",
      "Epoch [35/50], Train Loss: 0.6125, Train Accuracy: 0.7188, Test Loss: 0.7666, Test Accuracy: 0.6467\n",
      "Epoch [36/50], Train Loss: 0.6303, Train Accuracy: 0.7091, Test Loss: 0.7526, Test Accuracy: 0.6317\n",
      "Epoch [37/50], Train Loss: 0.6181, Train Accuracy: 0.7225, Test Loss: 0.7590, Test Accuracy: 0.6527\n",
      "Epoch [38/50], Train Loss: 0.5989, Train Accuracy: 0.7300, Test Loss: 0.7635, Test Accuracy: 0.6317\n",
      "Epoch [39/50], Train Loss: 0.5942, Train Accuracy: 0.7270, Test Loss: 0.7843, Test Accuracy: 0.6407\n",
      "Epoch [40/50], Train Loss: 0.5960, Train Accuracy: 0.7307, Test Loss: 0.7757, Test Accuracy: 0.6377\n",
      "Epoch [41/50], Train Loss: 0.5852, Train Accuracy: 0.7360, Test Loss: 0.7727, Test Accuracy: 0.6527\n",
      "Epoch [42/50], Train Loss: 0.5868, Train Accuracy: 0.7524, Test Loss: 0.8120, Test Accuracy: 0.6557\n",
      "Epoch [43/50], Train Loss: 0.5661, Train Accuracy: 0.7539, Test Loss: 0.7840, Test Accuracy: 0.6527\n",
      "Epoch [44/50], Train Loss: 0.5725, Train Accuracy: 0.7487, Test Loss: 0.8031, Test Accuracy: 0.6737\n",
      "Early Stop\n",
      "\n",
      "Epoch [1/50], Train Loss: 1.0750, Train Accuracy: 0.4428, Test Loss: 1.0714, Test Accuracy: 0.4731\n",
      "Epoch [2/50], Train Loss: 0.9231, Train Accuracy: 0.5654, Test Loss: 0.9939, Test Accuracy: 0.4850\n",
      "Epoch [3/50], Train Loss: 0.9034, Train Accuracy: 0.5587, Test Loss: 0.8969, Test Accuracy: 0.5599\n",
      "Epoch [4/50], Train Loss: 0.8663, Train Accuracy: 0.5864, Test Loss: 0.8862, Test Accuracy: 0.5689\n",
      "Epoch [5/50], Train Loss: 0.8445, Train Accuracy: 0.5939, Test Loss: 0.8643, Test Accuracy: 0.5808\n",
      "Epoch [6/50], Train Loss: 0.7985, Train Accuracy: 0.6320, Test Loss: 0.7878, Test Accuracy: 0.5898\n",
      "Epoch [7/50], Train Loss: 0.7758, Train Accuracy: 0.6335, Test Loss: 0.7805, Test Accuracy: 0.5838\n",
      "Epoch [8/50], Train Loss: 0.7621, Train Accuracy: 0.6477, Test Loss: 0.7887, Test Accuracy: 0.6018\n",
      "Epoch [9/50], Train Loss: 0.7762, Train Accuracy: 0.6402, Test Loss: 0.7775, Test Accuracy: 0.6048\n",
      "Epoch [10/50], Train Loss: 0.7585, Train Accuracy: 0.6372, Test Loss: 0.8230, Test Accuracy: 0.5838\n",
      "Epoch [11/50], Train Loss: 0.7451, Train Accuracy: 0.6432, Test Loss: 0.7868, Test Accuracy: 0.5988\n",
      "Epoch [12/50], Train Loss: 0.7296, Train Accuracy: 0.6694, Test Loss: 0.7877, Test Accuracy: 0.6018\n",
      "Epoch [13/50], Train Loss: 0.7553, Train Accuracy: 0.6619, Test Loss: 0.8039, Test Accuracy: 0.5838\n",
      "Epoch [14/50], Train Loss: 0.7516, Train Accuracy: 0.6649, Test Loss: 0.7662, Test Accuracy: 0.6108\n",
      "Epoch [15/50], Train Loss: 0.7202, Train Accuracy: 0.6589, Test Loss: 0.7892, Test Accuracy: 0.5749\n",
      "Epoch [16/50], Train Loss: 0.7436, Train Accuracy: 0.6619, Test Loss: 0.7806, Test Accuracy: 0.6048\n",
      "Epoch [17/50], Train Loss: 0.7119, Train Accuracy: 0.6717, Test Loss: 0.7918, Test Accuracy: 0.5629\n",
      "Epoch [18/50], Train Loss: 0.7317, Train Accuracy: 0.6634, Test Loss: 0.7769, Test Accuracy: 0.5868\n",
      "Epoch [19/50], Train Loss: 0.7227, Train Accuracy: 0.6485, Test Loss: 0.7666, Test Accuracy: 0.6108\n",
      "Epoch [20/50], Train Loss: 0.6912, Train Accuracy: 0.6821, Test Loss: 0.7808, Test Accuracy: 0.6138\n",
      "Epoch [21/50], Train Loss: 0.6993, Train Accuracy: 0.6806, Test Loss: 0.7606, Test Accuracy: 0.6138\n",
      "Epoch [22/50], Train Loss: 0.7125, Train Accuracy: 0.6679, Test Loss: 0.7641, Test Accuracy: 0.5749\n",
      "Epoch [23/50], Train Loss: 0.7017, Train Accuracy: 0.6717, Test Loss: 0.7787, Test Accuracy: 0.6138\n",
      "Epoch [24/50], Train Loss: 0.6729, Train Accuracy: 0.6814, Test Loss: 0.8064, Test Accuracy: 0.6168\n",
      "Epoch [25/50], Train Loss: 0.6659, Train Accuracy: 0.6821, Test Loss: 0.7720, Test Accuracy: 0.6048\n",
      "Epoch [26/50], Train Loss: 0.6608, Train Accuracy: 0.6911, Test Loss: 0.7776, Test Accuracy: 0.6198\n",
      "Epoch [27/50], Train Loss: 0.6516, Train Accuracy: 0.6948, Test Loss: 0.7883, Test Accuracy: 0.6347\n",
      "Epoch [28/50], Train Loss: 0.6535, Train Accuracy: 0.7023, Test Loss: 0.8041, Test Accuracy: 0.6228\n",
      "Epoch [29/50], Train Loss: 0.6583, Train Accuracy: 0.7023, Test Loss: 0.8255, Test Accuracy: 0.5838\n",
      "Epoch [30/50], Train Loss: 0.6665, Train Accuracy: 0.6761, Test Loss: 0.7969, Test Accuracy: 0.6078\n",
      "Epoch [31/50], Train Loss: 0.6467, Train Accuracy: 0.6993, Test Loss: 0.7799, Test Accuracy: 0.6257\n",
      "Epoch [32/50], Train Loss: 0.6452, Train Accuracy: 0.7061, Test Loss: 0.7865, Test Accuracy: 0.5988\n",
      "Epoch [33/50], Train Loss: 0.6402, Train Accuracy: 0.6986, Test Loss: 0.8288, Test Accuracy: 0.5928\n",
      "Epoch [34/50], Train Loss: 0.6259, Train Accuracy: 0.7098, Test Loss: 0.8098, Test Accuracy: 0.6257\n",
      "Epoch [35/50], Train Loss: 0.6308, Train Accuracy: 0.7031, Test Loss: 0.7974, Test Accuracy: 0.6108\n",
      "Epoch [36/50], Train Loss: 0.6122, Train Accuracy: 0.7173, Test Loss: 0.8706, Test Accuracy: 0.6138\n",
      "Epoch [37/50], Train Loss: 0.6118, Train Accuracy: 0.7195, Test Loss: 0.8021, Test Accuracy: 0.6078\n",
      "Epoch [38/50], Train Loss: 0.5976, Train Accuracy: 0.7143, Test Loss: 0.8229, Test Accuracy: 0.6048\n",
      "Epoch [39/50], Train Loss: 0.5882, Train Accuracy: 0.7255, Test Loss: 0.8475, Test Accuracy: 0.6138\n",
      "Epoch [40/50], Train Loss: 0.5926, Train Accuracy: 0.7188, Test Loss: 0.8367, Test Accuracy: 0.6228\n",
      "Early Stop\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp_name = f'four-head-{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "accuracies = []\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X_balanced['mcg'])):\n",
    "    X_train_mcg, X_test_mcg = [X_balanced['mcg'][i] for i in train_index], [X_balanced['mcg'][i] for i in test_index]\n",
    "    X_train_atac, X_test_atac = [X_balanced['atac'][i] for i in train_index], [X_balanced['atac'][i] for i in test_index]\n",
    "    X_train_hic, X_test_hic = [X_balanced['hic'][i] for i in train_index], [X_balanced['hic'][i] for i in test_index]\n",
    "    X_train_genebody, X_test_genebody = [X_balanced['genebody'][i] for i in train_index], [X_balanced['genebody'][i] for i in test_index]\n",
    "    \n",
    "    y_train, y_test = [y_balanced[i] for i in train_index], [y_balanced[i] for i in test_index]\n",
    "    \n",
    "    X_train_mcg_normalized, X_test_mcg_normalized = normalize_features(X_train_mcg, X_test_mcg)\n",
    "    X_train_atac_normalized, X_test_atac_normalized = normalize_features(X_train_atac, X_test_atac)\n",
    "    X_train_hic_normalized, X_test_hic_normalized = normalize_features(X_train_hic, X_test_hic)\n",
    "    X_train_genebody_normalized, X_test_genebody_normalized = normalize_features(X_train_genebody, X_test_genebody)\n",
    "    \n",
    "    elem_stats, epoch_stats, model = train_4head_model(X_train_mcg_normalized, X_train_atac_normalized, X_train_hic_normalized, X_train_genebody_normalized, y_train,\n",
    "                                                       X_test_mcg_normalized, X_test_atac_normalized, X_test_hic_normalized, X_test_genebody_normalized, y_test,\n",
    "                                                       exp_name=exp_name, fold_idx=i)\n",
    "    print()\n",
    "\n",
    "    df_elem = pd.DataFrame({'preds' : elem_stats[0], 'labels' : elem_stats[1]})\n",
    "    df_epoch = pd.DataFrame({'train_acc' : epoch_stats[0], 'train_loss' : epoch_stats[1], \n",
    "                             'test_acc' : epoch_stats[2], 'test_loss' : epoch_stats[3]})\n",
    "    df_elem.to_csv(f\"../4head_results/2_res_bal_elem_{i}.csv\")\n",
    "    df_epoch.to_csv(f\"../4head_results/2_res_bal_epoch_{i}.csv\")\n",
    "    torch.save(model, f\"../4head_results/2_model_{i}.pt\")\n",
    "\n",
    "    del model\n",
    "    del X_train_mcg, X_test_mcg, X_train_mcg_normalized, X_test_mcg_normalized\n",
    "    del X_train_atac, X_test_atac, X_train_atac_normalized, X_test_atac_normalized\n",
    "    del X_train_hic, X_test_hic, X_train_hic_normalized, X_test_hic_normalized\n",
    "    del X_train_genebody, X_test_genebody, X_train_genebody_normalized, X_test_genebody_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d3c766-1944-4616-8786-8bf5fb6dfa83",
   "metadata": {},
   "source": [
    "## Clear GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b206e01-5074-41b5-a967-d43d837ca81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "# del optimizer\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35eb48ef-a552-4ec8-9c5a-527524c72c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated Memory: 0.00 MB\n",
      "Cached Memory: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Allocated Memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"Cached Memory: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3534bba-9960-4212-9a66-44d2a15e5cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
