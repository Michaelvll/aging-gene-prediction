{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed mcg data\n",
      "Processed genebody data\n",
      "Processed atac data\n",
      "Processed hic data\n",
      "zero: 9941, non-zero: 3183\n",
      "mcg 4774\n",
      "genebody 4774\n",
      "atac 4774\n",
      "hic 4774\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from data_loader import load_data, get_balanced_data, normalize_features\n",
    "\n",
    "# import wandb\n",
    "# wandb.init(project='gene')\n",
    "\n",
    "data = load_data()\n",
    "X_balanced, y_balanced = get_balanced_data(data)\n",
    "FEATURE_TYPES = ['mcg', 'atac', 'hic', 'genebody']\n",
    "for k, v in X_balanced.items():\n",
    "    print(k, len(v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 63%\n",
    "HIDDEN_DIM = 64\n",
    "NUM_LAYERS = 2\n",
    "NUM_HEADS = 4\n",
    "DROPOUT = 0.1\n",
    "LR = 0.01\n",
    "OUTPUT_DIM = 3  # number of classes (-1, 0, 1)\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# HIDDEN_DIM = 64\n",
    "# NUM_LAYERS = 4\n",
    "# NUM_HEADS = 8\n",
    "# DROPOUT = 0.0\n",
    "# LR = 0.03\n",
    "# OUTPUT_DIM = 3  # number of classes (-1, 0, 1)\n",
    "# NUM_EPOCHS = 20\n",
    "# BATCH_SIZE = 32\n",
    "\n",
    "# HIDDEN_DIM = 64\n",
    "# NUM_LAYERS = 1\n",
    "# NUM_HEADS = 1\n",
    "# DROPOUT = 0.3\n",
    "# LR = 1e-4\n",
    "# OUTPUT_DIM = 3  # number of classes (-1, 0, 1)\n",
    "# NUM_EPOCHS = 200\n",
    "# BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "class FourHeadTransformerModel(nn.Module):\n",
    "    def __init__(self, mcg_input_dim, atac_input_dim, hic_input_dim, genebody_input_dim, hidden_dim, output_dim, num_layers=2, num_heads=1, dropout=0.1):\n",
    "        super(FourHeadTransformerModel, self).__init__()\n",
    "        self.mcg_embedding = nn.Linear(mcg_input_dim, hidden_dim)\n",
    "        self.atac_embedding = nn.Linear(atac_input_dim, hidden_dim)\n",
    "        self.hic_embedding = nn.Linear(hic_input_dim, hidden_dim)\n",
    "        self.genebody_embedding = nn.Linear(genebody_input_dim, hidden_dim)\n",
    "\n",
    "        # TODO: may need to use tanh in attention instead of softmax\n",
    "        encoder_layers = nn.TransformerEncoderLayer(hidden_dim, num_heads, dim_feedforward=hidden_dim*4, dropout=dropout, batch_first=True, norm_first=True)\n",
    "        self.mcg_transformer = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.atac_transformer = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.hic_transformer = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.genebody_transformer = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_dim * 4, output_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, mcg_x, mcg_mask, atac_x, atac_mask, hic_x, hic_mask, genebody_x, genebody_mask):\n",
    "        mcg_x = self.mcg_embedding(mcg_x)\n",
    "        atac_x = self.atac_embedding(atac_x)\n",
    "        hic_x = self.hic_embedding(hic_x)\n",
    "        genebody_x = self.genebody_embedding(genebody_x)\n",
    "        \n",
    "        mcg_x = self.mcg_transformer(mcg_x, src_key_padding_mask=~mcg_mask.bool())\n",
    "        atac_x = self.atac_transformer(atac_x, src_key_padding_mask=~atac_mask.bool())\n",
    "        hic_x = self.hic_transformer(hic_x, src_key_padding_mask=~hic_mask.bool())\n",
    "        genebody_x = self.genebody_transformer(genebody_x, src_key_padding_mask=~genebody_mask.bool())\n",
    "        \n",
    "        # Global average pooling\n",
    "        mcg_x = mcg_x.mean(dim=1)\n",
    "        atac_x = atac_x.mean(dim=1)\n",
    "        hic_x = hic_x.mean(dim=1)\n",
    "        genebody_x = genebody_x.mean(dim=1)\n",
    "\n",
    "        # Concatenate MCG and ATAC embeddings\n",
    "        combined_x = torch.cat((mcg_x, atac_x, hic_x, genebody_x), dim=1)\n",
    "        \n",
    "        output = self.classifier(combined_x)\n",
    "        return output\n",
    "\n",
    "class CombinedGeneDataset(Dataset):\n",
    "    def __init__(self, mcg_data, atac_data, hic_data, genebody_data, labels):\n",
    "        self.mcg_data = mcg_data\n",
    "        self.atac_data = atac_data\n",
    "        self.hic_data = hic_data\n",
    "        self.genebody_data = genebody_data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        mcg_gene_data = torch.FloatTensor(self.mcg_data[idx])\n",
    "        atac_gene_data = torch.FloatTensor(self.atac_data[idx])\n",
    "        hic_gene_data = torch.FloatTensor(self.hic_data[idx])\n",
    "        genebody_gene_data = torch.FloatTensor(self.genebody_data[idx])\n",
    "        label = torch.LongTensor([self.labels[idx] + 1])  # Add 1 to shift labels to 0, 1, 2\n",
    "        mcg_mask = torch.ones(len(mcg_gene_data))\n",
    "        atac_mask = torch.ones(len(atac_gene_data))\n",
    "        hic_mask = torch.ones(len(hic_gene_data))\n",
    "        genebody_mask = torch.ones(len(genebody_gene_data))\n",
    "        return mcg_gene_data, atac_gene_data, hic_gene_data, genebody_gene_data, label, mcg_mask, atac_mask, hic_mask, genebody_mask\n",
    "\n",
    "def combined_collate_fn(batch):\n",
    "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    mcg_sequences, atac_sequences, hic_sequences, genebody_sequences, labels, mcg_masks, atac_masks, hic_masks, genebody_masks = zip(*batch)\n",
    "    \n",
    "    mcg_lengths = [len(seq) for seq in mcg_sequences]\n",
    "    atac_lengths = [len(seq) for seq in atac_sequences]\n",
    "    hic_lengths = [len(seq) for seq in hic_sequences]\n",
    "    genebody_lengths = [len(seq) for seq in genebody_sequences]\n",
    "    mcg_max_len = max(mcg_lengths)\n",
    "    atac_max_len = max(atac_lengths)\n",
    "    hic_max_len = max(hic_lengths)\n",
    "    genebody_max_len = max(genebody_lengths)\n",
    "    \n",
    "    padded_mcg_seqs = torch.zeros(len(mcg_sequences), mcg_max_len, mcg_sequences[0].size(1))\n",
    "    padded_atac_seqs = torch.zeros(len(atac_sequences), atac_max_len, atac_sequences[0].size(1))\n",
    "    padded_hic_seqs = torch.zeros(len(hic_sequences), hic_max_len, hic_sequences[0].size(1))\n",
    "    padded_genebody_seqs = torch.zeros(len(genebody_sequences), genebody_max_len, genebody_sequences[0].size(1))\n",
    "    padded_mcg_masks = torch.zeros(len(mcg_sequences), mcg_max_len)\n",
    "    padded_atac_masks = torch.zeros(len(atac_sequences), atac_max_len)\n",
    "    padded_hic_masks = torch.zeros(len(hic_sequences), hic_max_len)\n",
    "    padded_genebody_masks = torch.zeros(len(genebody_sequences), genebody_max_len)\n",
    "    \n",
    "    for i, (mcg_seq, atac_seq, hic_seq, genebody_seq, mcg_length, atac_length, hic_length, genebody_length) in enumerate(zip(mcg_sequences, atac_sequences, hic_sequences, genebody_sequences, mcg_lengths, atac_lengths, hic_lengths, genebody_lengths)):\n",
    "        padded_mcg_seqs[i, :mcg_length] = mcg_seq\n",
    "        padded_atac_seqs[i, :atac_length] = atac_seq\n",
    "        padded_hic_seqs[i, :hic_length] = hic_seq\n",
    "        padded_genebody_seqs[i, :genebody_length] = genebody_seq\n",
    "        padded_mcg_masks[i, :mcg_length] = 1\n",
    "        padded_atac_masks[i, :atac_length] = 1\n",
    "        padded_hic_masks[i, :hic_length] = 1\n",
    "        padded_genebody_masks[i, :genebody_length] = 1\n",
    "    \n",
    "    return padded_mcg_seqs, padded_atac_seqs, padded_hic_seqs, padded_genebody_seqs, torch.cat(labels), padded_mcg_masks, padded_atac_masks, padded_hic_masks, padded_genebody_masks\n",
    "\n",
    "def train_combined_model(X_train_mcg, X_train_atac, X_train_hic, X_train_genebody, y_train, X_test_mcg, X_test_atac, X_test_hic, X_test_genebody, y_test, exp_name, fold_idx):\n",
    "    #wandb.init(project='gene', group=exp_name, name=f'fold-{fold_idx}')\n",
    "    mcg_input_dim = len(X_train_mcg[0][0])\n",
    "    atac_input_dim = len(X_train_atac[0][0])\n",
    "    hic_input_dim = len(X_train_hic[0][0])\n",
    "    genebody_input_dim = len(X_train_genebody[0][0])\n",
    "    \n",
    "    train_dataset = CombinedGeneDataset(X_train_mcg, X_train_atac, X_train_hic, X_train_genebody, y_train)\n",
    "    test_dataset = CombinedGeneDataset(X_test_mcg, X_test_atac, X_test_hic, X_test_genebody, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=combined_collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=combined_collate_fn)\n",
    "\n",
    "    model = FourHeadTransformerModel(mcg_input_dim, atac_input_dim, hic_input_dim, genebody_input_dim, HIDDEN_DIM, OUTPUT_DIM, num_layers=NUM_LAYERS, num_heads=NUM_HEADS, dropout=DROPOUT)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=0.0001)\n",
    "    # Create the OneCycleLR scheduler\n",
    "    # lr_scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=LR, total_steps=NUM_EPOCHS,\n",
    "    #                           pct_start=0.8, anneal_strategy='cos',\n",
    "    #                           cycle_momentum=False, div_factor=5.0,\n",
    "    #                           final_div_factor=10.0)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        for mcg_x, atac_x, hic_x, genebody_x, batch_y, mcg_mask, atac_mask, hic_mask, genebody_mask in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(mcg_x, mcg_mask, atac_x, atac_mask, hic_x, hic_mask, genebody_x, genebody_mask)\n",
    "            loss = criterion(outputs, batch_y.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            train_correct += (outputs.argmax(dim=1) == batch_y.squeeze()).sum().item()\n",
    "            train_total += batch_y.size(0)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for mcg_x, atac_x, hic_x, genebody_x, batch_y, mcg_mask, atac_mask, hic_mask, genebody_mask in test_loader:\n",
    "                outputs = model(mcg_x, mcg_mask, atac_x, atac_mask, hic_x, hic_mask, genebody_x, genebody_mask)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += batch_y.size(0)\n",
    "                correct += (predicted == batch_y.squeeze()).sum().item()\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {total_loss/len(train_loader):.4f}, Train Accuracy: {train_correct/train_total:.4f}, Test Accuracy: {accuracy:.4f}')\n",
    "        #wandb.log({'epoch': epoch, 'LR': optimizer.param_groups[0]['lr'], 'train_loss': total_loss/len(train_loader), 'train_accuracy': train_correct/train_total, 'test_accuracy': accuracy})\n",
    "\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for mcg_x, atac_x, hic_x, genebody_x, batch_y, mcg_mask, atac_mask, hic_mask, genebody_mask in test_loader:\n",
    "            outputs = model(mcg_x, mcg_mask, atac_x, atac_mask, hic_x, hic_mask, genebody_x, genebody_mask)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    final_accuracy = sum(np.array(all_predictions) == np.array(all_labels).squeeze()) / len(all_labels)\n",
    "    print(f'Final Test Accuracy: {final_accuracy:.4f}')\n",
    "    return final_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 1.1948, Train Accuracy: 0.4910, Test Accuracy: 0.4848\n",
      "Epoch [2/20], Train Loss: 0.9693, Train Accuracy: 0.5122, Test Accuracy: 0.5225\n",
      "Epoch [3/20], Train Loss: 0.9316, Train Accuracy: 0.5300, Test Accuracy: 0.4974\n",
      "Epoch [4/20], Train Loss: 0.9094, Train Accuracy: 0.5491, Test Accuracy: 0.5696\n",
      "Epoch [5/20], Train Loss: 0.8966, Train Accuracy: 0.5598, Test Accuracy: 0.5539\n",
      "Epoch [6/20], Train Loss: 0.8844, Train Accuracy: 0.5693, Test Accuracy: 0.5110\n",
      "Epoch [7/20], Train Loss: 0.9760, Train Accuracy: 0.5499, Test Accuracy: 0.5204\n",
      "Epoch [8/20], Train Loss: 0.8754, Train Accuracy: 0.5766, Test Accuracy: 0.5728\n",
      "Epoch [9/20], Train Loss: 0.8565, Train Accuracy: 0.5690, Test Accuracy: 0.5717\n",
      "Epoch [10/20], Train Loss: 0.8524, Train Accuracy: 0.5858, Test Accuracy: 0.5832\n",
      "Epoch [11/20], Train Loss: 0.8563, Train Accuracy: 0.5766, Test Accuracy: 0.5948\n",
      "Epoch [12/20], Train Loss: 0.8415, Train Accuracy: 0.5996, Test Accuracy: 0.5634\n",
      "Epoch [13/20], Train Loss: 0.8316, Train Accuracy: 0.5994, Test Accuracy: 0.5822\n",
      "Epoch [14/20], Train Loss: 0.8305, Train Accuracy: 0.6023, Test Accuracy: 0.5927\n",
      "Epoch [15/20], Train Loss: 0.8204, Train Accuracy: 0.6119, Test Accuracy: 0.5990\n",
      "Epoch [16/20], Train Loss: 0.8135, Train Accuracy: 0.6164, Test Accuracy: 0.6042\n",
      "Epoch [17/20], Train Loss: 0.8092, Train Accuracy: 0.6216, Test Accuracy: 0.6084\n",
      "Epoch [18/20], Train Loss: 0.8039, Train Accuracy: 0.6216, Test Accuracy: 0.6126\n",
      "Epoch [19/20], Train Loss: 0.8003, Train Accuracy: 0.6190, Test Accuracy: 0.6063\n",
      "Epoch [20/20], Train Loss: 0.7904, Train Accuracy: 0.6263, Test Accuracy: 0.6063\n",
      "Final Test Accuracy: 0.6063\n",
      "Epoch [1/20], Train Loss: 1.2517, Train Accuracy: 0.4580, Test Accuracy: 0.5288\n",
      "Epoch [2/20], Train Loss: 0.9537, Train Accuracy: 0.5093, Test Accuracy: 0.5026\n",
      "Epoch [3/20], Train Loss: 0.9442, Train Accuracy: 0.5245, Test Accuracy: 0.5403\n",
      "Epoch [4/20], Train Loss: 0.9161, Train Accuracy: 0.5431, Test Accuracy: 0.5099\n",
      "Epoch [5/20], Train Loss: 0.9037, Train Accuracy: 0.5601, Test Accuracy: 0.5592\n",
      "Epoch [6/20], Train Loss: 0.8925, Train Accuracy: 0.5535, Test Accuracy: 0.5874\n",
      "Epoch [7/20], Train Loss: 0.8637, Train Accuracy: 0.5645, Test Accuracy: 0.5476\n",
      "Epoch [8/20], Train Loss: 0.8700, Train Accuracy: 0.5792, Test Accuracy: 0.5979\n",
      "Epoch [9/20], Train Loss: 0.8599, Train Accuracy: 0.5758, Test Accuracy: 0.5654\n",
      "Epoch [10/20], Train Loss: 0.8487, Train Accuracy: 0.5928, Test Accuracy: 0.6126\n",
      "Epoch [11/20], Train Loss: 0.8458, Train Accuracy: 0.5991, Test Accuracy: 0.5749\n",
      "Epoch [12/20], Train Loss: 0.8390, Train Accuracy: 0.5952, Test Accuracy: 0.5843\n",
      "Epoch [13/20], Train Loss: 0.8391, Train Accuracy: 0.5975, Test Accuracy: 0.5843\n",
      "Epoch [14/20], Train Loss: 0.8309, Train Accuracy: 0.6096, Test Accuracy: 0.6031\n",
      "Epoch [15/20], Train Loss: 0.8189, Train Accuracy: 0.6159, Test Accuracy: 0.6084\n",
      "Epoch [16/20], Train Loss: 0.8108, Train Accuracy: 0.6187, Test Accuracy: 0.6115\n",
      "Epoch [17/20], Train Loss: 0.8052, Train Accuracy: 0.6190, Test Accuracy: 0.6021\n",
      "Epoch [18/20], Train Loss: 0.7990, Train Accuracy: 0.6263, Test Accuracy: 0.6199\n",
      "Epoch [19/20], Train Loss: 0.7931, Train Accuracy: 0.6203, Test Accuracy: 0.6136\n",
      "Epoch [20/20], Train Loss: 0.7871, Train Accuracy: 0.6287, Test Accuracy: 0.6115\n",
      "Final Test Accuracy: 0.6115\n",
      "Epoch [1/20], Train Loss: 1.2591, Train Accuracy: 0.4828, Test Accuracy: 0.4796\n",
      "Epoch [2/20], Train Loss: 0.9575, Train Accuracy: 0.5279, Test Accuracy: 0.4827\n",
      "Epoch [3/20], Train Loss: 0.9287, Train Accuracy: 0.5431, Test Accuracy: 0.4942\n",
      "Epoch [4/20], Train Loss: 0.9222, Train Accuracy: 0.5538, Test Accuracy: 0.5487\n",
      "Epoch [5/20], Train Loss: 0.8972, Train Accuracy: 0.5554, Test Accuracy: 0.5560\n",
      "Epoch [6/20], Train Loss: 0.8838, Train Accuracy: 0.5666, Test Accuracy: 0.5298\n",
      "Epoch [7/20], Train Loss: 0.8761, Train Accuracy: 0.5837, Test Accuracy: 0.5644\n",
      "Epoch [8/20], Train Loss: 0.8724, Train Accuracy: 0.5753, Test Accuracy: 0.5791\n",
      "Epoch [9/20], Train Loss: 0.8474, Train Accuracy: 0.5873, Test Accuracy: 0.5592\n",
      "Epoch [10/20], Train Loss: 0.8383, Train Accuracy: 0.5957, Test Accuracy: 0.5634\n",
      "Epoch [11/20], Train Loss: 0.8386, Train Accuracy: 0.6036, Test Accuracy: 0.5906\n",
      "Epoch [12/20], Train Loss: 0.8359, Train Accuracy: 0.6046, Test Accuracy: 0.5550\n",
      "Epoch [13/20], Train Loss: 0.8308, Train Accuracy: 0.6193, Test Accuracy: 0.5885\n",
      "Epoch [14/20], Train Loss: 0.8136, Train Accuracy: 0.6148, Test Accuracy: 0.5770\n",
      "Epoch [15/20], Train Loss: 0.8161, Train Accuracy: 0.6180, Test Accuracy: 0.6084\n",
      "Epoch [16/20], Train Loss: 0.8097, Train Accuracy: 0.6122, Test Accuracy: 0.5927\n",
      "Epoch [17/20], Train Loss: 0.8054, Train Accuracy: 0.6222, Test Accuracy: 0.6010\n",
      "Epoch [18/20], Train Loss: 0.7951, Train Accuracy: 0.6266, Test Accuracy: 0.5969\n",
      "Epoch [19/20], Train Loss: 0.7879, Train Accuracy: 0.6274, Test Accuracy: 0.6063\n",
      "Epoch [20/20], Train Loss: 0.7858, Train Accuracy: 0.6284, Test Accuracy: 0.5979\n",
      "Final Test Accuracy: 0.5979\n",
      "Epoch [1/20], Train Loss: 1.2416, Train Accuracy: 0.4747, Test Accuracy: 0.5173\n",
      "Epoch [2/20], Train Loss: 0.9682, Train Accuracy: 0.5151, Test Accuracy: 0.5005\n",
      "Epoch [3/20], Train Loss: 1.4414, Train Accuracy: 0.4836, Test Accuracy: 0.5236\n",
      "Epoch [4/20], Train Loss: 0.9275, Train Accuracy: 0.5418, Test Accuracy: 0.5592\n",
      "Epoch [5/20], Train Loss: 0.9234, Train Accuracy: 0.5551, Test Accuracy: 0.5351\n",
      "Epoch [6/20], Train Loss: 0.9155, Train Accuracy: 0.5399, Test Accuracy: 0.3204\n",
      "Epoch [7/20], Train Loss: 0.9606, Train Accuracy: 0.5533, Test Accuracy: 0.5058\n",
      "Epoch [8/20], Train Loss: 0.8751, Train Accuracy: 0.5742, Test Accuracy: 0.5455\n",
      "Epoch [9/20], Train Loss: 0.8531, Train Accuracy: 0.5818, Test Accuracy: 0.6010\n",
      "Epoch [10/20], Train Loss: 0.8498, Train Accuracy: 0.5873, Test Accuracy: 0.5539\n",
      "Epoch [11/20], Train Loss: 0.8649, Train Accuracy: 0.5852, Test Accuracy: 0.5770\n",
      "Epoch [12/20], Train Loss: 0.8559, Train Accuracy: 0.5905, Test Accuracy: 0.5696\n",
      "Epoch [13/20], Train Loss: 0.8399, Train Accuracy: 0.5983, Test Accuracy: 0.5916\n",
      "Epoch [14/20], Train Loss: 0.8272, Train Accuracy: 0.6062, Test Accuracy: 0.5927\n",
      "Epoch [15/20], Train Loss: 0.8210, Train Accuracy: 0.6004, Test Accuracy: 0.6126\n",
      "Epoch [16/20], Train Loss: 0.8182, Train Accuracy: 0.6174, Test Accuracy: 0.6084\n",
      "Epoch [17/20], Train Loss: 0.8098, Train Accuracy: 0.6138, Test Accuracy: 0.5979\n",
      "Epoch [18/20], Train Loss: 0.8057, Train Accuracy: 0.6093, Test Accuracy: 0.6115\n",
      "Epoch [19/20], Train Loss: 0.8079, Train Accuracy: 0.6117, Test Accuracy: 0.6157\n",
      "Epoch [20/20], Train Loss: 0.8015, Train Accuracy: 0.6219, Test Accuracy: 0.6136\n",
      "Final Test Accuracy: 0.6136\n",
      "Epoch [1/20], Train Loss: 1.2480, Train Accuracy: 0.4681, Test Accuracy: 0.4906\n",
      "Epoch [2/20], Train Loss: 0.9738, Train Accuracy: 0.5136, Test Accuracy: 0.3176\n",
      "Epoch [3/20], Train Loss: 0.9794, Train Accuracy: 0.5071, Test Accuracy: 0.4518\n",
      "Epoch [4/20], Train Loss: 0.9373, Train Accuracy: 0.5186, Test Accuracy: 0.5346\n",
      "Epoch [5/20], Train Loss: 0.8971, Train Accuracy: 0.5387, Test Accuracy: 0.5084\n",
      "Epoch [6/20], Train Loss: 0.9111, Train Accuracy: 0.5437, Test Accuracy: 0.5975\n",
      "Epoch [7/20], Train Loss: 0.8807, Train Accuracy: 0.5599, Test Accuracy: 0.5618\n",
      "Epoch [8/20], Train Loss: 0.8758, Train Accuracy: 0.5662, Test Accuracy: 0.5388\n",
      "Epoch [9/20], Train Loss: 0.8678, Train Accuracy: 0.5791, Test Accuracy: 0.5943\n",
      "Epoch [10/20], Train Loss: 0.8481, Train Accuracy: 0.5890, Test Accuracy: 0.6069\n",
      "Epoch [11/20], Train Loss: 0.8466, Train Accuracy: 0.5830, Test Accuracy: 0.5818\n",
      "Epoch [12/20], Train Loss: 0.8545, Train Accuracy: 0.5783, Test Accuracy: 0.6237\n",
      "Epoch [13/20], Train Loss: 0.8396, Train Accuracy: 0.5880, Test Accuracy: 0.5996\n",
      "Epoch [14/20], Train Loss: 0.8373, Train Accuracy: 0.5958, Test Accuracy: 0.6164\n",
      "Epoch [15/20], Train Loss: 0.8348, Train Accuracy: 0.5906, Test Accuracy: 0.6258\n",
      "Epoch [16/20], Train Loss: 0.8154, Train Accuracy: 0.6134, Test Accuracy: 0.6195\n",
      "Epoch [17/20], Train Loss: 0.8122, Train Accuracy: 0.6178, Test Accuracy: 0.6394\n",
      "Epoch [18/20], Train Loss: 0.8090, Train Accuracy: 0.6128, Test Accuracy: 0.6310\n",
      "Epoch [19/20], Train Loss: 0.8034, Train Accuracy: 0.6160, Test Accuracy: 0.6478\n",
      "Epoch [20/20], Train Loss: 0.8007, Train Accuracy: 0.6212, Test Accuracy: 0.6426\n",
      "Final Test Accuracy: 0.6426\n",
      "Mean Accuracy: 0.6144 ± 0.0151\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "exp_name = f'two-head-{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "# wandb.config.update({\n",
    "#     'hidden_dim': HIDDEN_DIM,\n",
    "#     'num_layers': NUM_LAYERS,\n",
    "#     'num_heads': NUM_HEADS,\n",
    "#     'dropout': DROPOUT,\n",
    "#     'lr': LR,\n",
    "#     'output_dim': OUTPUT_DIM,\n",
    "#     'num_epochs': NUM_EPOCHS\n",
    "# })\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=25)\n",
    "accuracies = []\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X_balanced['mcg'])):\n",
    "    X_train_mcg, X_test_mcg = [X_balanced['mcg'][i] for i in train_index], [X_balanced['mcg'][i] for i in test_index]\n",
    "    X_train_atac, X_test_atac = [X_balanced['atac'][i] for i in train_index], [X_balanced['atac'][i] for i in test_index]\n",
    "    X_train_hic, X_test_hic = [X_balanced['hic'][i] for i in train_index], [X_balanced['hic'][i] for i in test_index]\n",
    "    X_train_genebody, X_test_genebody = [X_balanced['genebody'][i] for i in train_index], [X_balanced['genebody'][i] for i in test_index]\n",
    "    y_train, y_test = [y_balanced[i] for i in train_index], [y_balanced[i] for i in test_index]\n",
    "    \n",
    "    X_train_mcg_normalized, X_test_mcg_normalized = normalize_features(X_train_mcg, X_test_mcg)\n",
    "    X_train_atac_normalized, X_test_atac_normalized = normalize_features(X_train_atac, X_test_atac)\n",
    "    X_train_hic_normalized, X_test_hic_normalized = normalize_features(X_train_hic, X_test_hic)\n",
    "    X_train_genebody_normalized, X_test_genebody_normalized = normalize_features(X_train_genebody, X_test_genebody)\n",
    "    \n",
    "    accuracies.append(train_combined_model(X_train_mcg_normalized, X_train_atac_normalized, X_train_hic_normalized, X_train_genebody_normalized, y_train, \n",
    "                                           X_test_mcg_normalized, X_test_atac_normalized, X_test_hic_normalized, X_test_genebody_normalized, y_test, exp_name=exp_name, fold_idx=i))\n",
    "\n",
    "print(f'Mean Accuracy: {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.6144 ± 0.0151\n"
     ]
    }
   ],
   "source": [
    "print(f'Mean Accuracy: {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gene",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
