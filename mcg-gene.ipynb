{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import wandb\n",
    "wandb.init(project='gene')\n",
    "\n",
    "df = pd.read_csv('data/Oligo_NN.RNA_DEG.csv')\n",
    "df.set_index('gene', inplace=True)\n",
    "df.head()\n",
    "\n",
    "# non_zero_genes = df[df['DEG'] != 0].index\n",
    "\n",
    "# df = df[df.index.isin(non_zero_genes)]\n",
    "gene2value = df[['DEG']]\n",
    "\n",
    "MCG_FEATURE_NAMES = ['2mo', '9mo', '18mo', '9mo-2mo', '18mo-9mo', '9mo/2mo', '18mo/9mo', 'old-young', 'old/young', 'distance']\n",
    "ATAC_FEATURE_NAMES = ['log2(old/young)', 'distance']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    gene   2mo   9mo  18mo  9mo-2mo  18mo-9mo   9mo/2mo  18mo/9mo  old-young  \\\n",
      "0  Rgs20  0.65  0.60  0.90    -0.05      0.30  0.923077  1.500000       0.25   \n",
      "1  Sulf1  0.36  0.52  0.56     0.16      0.04  1.444444  1.076923       0.20   \n",
      "2  Sulf1  0.43  0.59  0.64     0.16      0.05  1.372093  1.084746       0.21   \n",
      "3   Eya1  0.68  0.62  0.47    -0.06     -0.15  0.911765  0.758065      -0.21   \n",
      "4   Eya1  0.61  0.37  0.45    -0.24      0.08  0.606557  1.216216      -0.16   \n",
      "\n",
      "   old/young  distance  \n",
      "0   1.384615  151241.0  \n",
      "1   1.555556  121205.0  \n",
      "2   1.488372  170142.0  \n",
      "3   0.691176  137980.0  \n",
      "4   0.737705  138254.0  \n"
     ]
    }
   ],
   "source": [
    "# Process mcg data\n",
    "mcg = pd.read_csv('data/Oligo_NN.aDMR_gene.csv')\n",
    "mcg_feat = mcg\n",
    "mcg_feat.rename(columns={'gene_name': 'gene'}, inplace=True)\n",
    "mcg_feat['distance'] = (mcg_feat['gene_start'] - mcg_feat['start']).abs().astype(np.float64)\n",
    "mcg_feat['old/young'] = mcg_feat['18mo'] / mcg_feat['2mo']\n",
    "mcg_feat['9mo-2mo'] = mcg_feat['9mo'] - mcg_feat['2mo']\n",
    "mcg_feat['18mo-9mo'] = mcg_feat['18mo'] - mcg_feat['9mo']\n",
    "mcg_feat['9mo/2mo'] = mcg_feat['9mo'] / mcg_feat['2mo']\n",
    "mcg_feat['18mo/9mo'] = mcg_feat['18mo'] / mcg_feat['9mo']\n",
    "mcg_feat.fillna(0, inplace=True)\n",
    "mcg_feat = mcg_feat[['gene', *MCG_FEATURE_NAMES]]\n",
    "print(mcg_feat.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      gene  log2(old/young)  distance\n",
      "0    Itgb5         1.521456   32221.0\n",
      "1   Begain         1.320315   37592.0\n",
      "2   BEGAIN         1.320315   36795.0\n",
      "3     Eya4         1.588159  247781.0\n",
      "4  Gm27247         1.018367   97190.0\n"
     ]
    }
   ],
   "source": [
    "atac = pd.read_csv('data/Oligo_NN.ATAC_gene.csv')\n",
    "atac_feat = atac\n",
    "atac_feat.rename(columns={'gene_name': 'gene'}, inplace=True)\n",
    "atac_feat['distance'] = (atac_feat['gene_start'] - atac_feat['start']).abs().astype(np.float64)\n",
    "atac_feat = atac_feat[['gene', *ATAC_FEATURE_NAMES]]\n",
    "print(atac_feat.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total genes: 17783 unique genes: 17783\n",
      "genes: Index(['Xkr4', 'Gm1992', 'Gm19938', 'Mrpl15', 'Lypla1', 'Tcea1', 'Rgs20',\n",
      "       'Atp6v1h', 'Oprk1', 'Rb1cc1',\n",
      "       ...\n",
      "       'Smim23', 'Gm31126', 'Gm13546', 'Gm16350', 'Gm26784', 'Gm34654',\n",
      "       'Gm29825', 'AI463170', 'E330034G19Rik', 'B130011K05Rik'],\n",
      "      dtype='object', name='gene', length=17783)\n"
     ]
    }
   ],
   "source": [
    "print(f'Total genes: {len(gene2value)}', f'unique genes: {len(set(gene2value.index))}')\n",
    "print(f'genes: {gene2value.index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mcg_mean = mcg_feat.groupby('gene').mean()\n",
    "# # Sort mcg_mean by gene name\n",
    "# mcg_mean = mcg_mean.loc[gene2value.index]\n",
    "# atac_mean = atac_feat.groupby('gene').mean()\n",
    "# atac_mean = atac_mean.loc[gene2value.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('mcg corr:', mcg_mean.corrwith(gene2value['DEG']))\n",
    "# print('atac corr:', atac_mean.corrwith(gene2value['DEG']))\n",
    "index_order = gene2value.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_3137048/2811104672.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  list_mcg_feat = mcg_feat.groupby('gene').apply(lambda x: x[MCG_FEATURE_NAMES].values.tolist())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0.65, 0.6, 0.9, -0.050000000000000044, 0.30000000000000004, 0.923076923076923, 1.5, 0.25, 1.3846153846153846, 151241.0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_3137048/2811104672.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  list_atac_feat = atac_feat.groupby('gene').apply(lambda x: x[ATAC_FEATURE_NAMES].values.tolist())\n"
     ]
    }
   ],
   "source": [
    "# Train a sequence model on mcg_feat to predict gene2value['log2(old/young)']\n",
    "# Each gene has a sequence of 4 features, 2mo, 9mo, 18mo, old-young\n",
    "# The sequence length is not fixed, so we need to use a dynamic model\n",
    "# Let's use a commonly used sequence prediction model for sentence classification\n",
    "# like LSTM or Transformer\n",
    "\n",
    "# Step 1: Prepare the data\n",
    "list_mcg_feat = mcg_feat.groupby('gene').apply(lambda x: x[MCG_FEATURE_NAMES].values.tolist())\n",
    "list_mcg_feat = list_mcg_feat.reindex(index_order, fill_value=[[0] * len(MCG_FEATURE_NAMES)])\n",
    "x_mcg = list_mcg_feat.values.tolist()\n",
    "\n",
    "list_atac_feat = atac_feat.groupby('gene').apply(lambda x: x[ATAC_FEATURE_NAMES].values.tolist())\n",
    "list_atac_feat = list_atac_feat.reindex(index_order, fill_value=[[0] * len(ATAC_FEATURE_NAMES)])\n",
    "x_atac = list_atac_feat.values.tolist()\n",
    "print(x_mcg[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = gene2value.loc[list_mcg_feat.index]['DEG'].values.tolist()\n",
    "y = np.array([int(i) for i in y])\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero: 16703, non-zero: 1080\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(25)\n",
    "\n",
    "# Separate the data into zero and non-zero y values\n",
    "zero_indices = np.where(y == 0)[0]\n",
    "non_zero_indices = np.where(y != 0)[0]\n",
    "print(f'zero: {len(zero_indices)}, non-zero: {len(non_zero_indices)}')\n",
    "\n",
    "# Sample len(non_zero_indices) indices from each group\n",
    "n_samples = len(non_zero_indices)\n",
    "sampled_zero_indices = np.random.choice(zero_indices, n_samples // 2, replace=False)\n",
    "sampled_non_zero_indices = np.random.choice(non_zero_indices, n_samples, replace=False)\n",
    "\n",
    "# Combine the sampled indices\n",
    "sampled_indices = np.concatenate([sampled_zero_indices, sampled_non_zero_indices])\n",
    "\n",
    "# Create balanced dataset\n",
    "X_balanced = {}\n",
    "X_balanced['mcg'] = [x_mcg[i] for i in sampled_indices]\n",
    "X_balanced['atac'] = [x_atac[i] for i in sampled_indices]\n",
    "y_balanced = y[sampled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization function\n",
    "def normalize_features(train_data, test_data):\n",
    "    # Flatten the lists for easier processing\n",
    "    train_flat = [item for sublist in train_data for item in sublist]\n",
    "    test_flat = [item for sublist in test_data for item in sublist]\n",
    "    feature_dim = len(train_flat[0])\n",
    "    # Separate features\n",
    "    train_other_features = np.array([item[:feature_dim-1] for item in train_flat])\n",
    "    train_distances = np.array([item[feature_dim-1] for item in train_flat])\n",
    "    test_other_features = np.array([item[:feature_dim-1] for item in test_flat])\n",
    "    test_distances = np.array([item[feature_dim-1] for item in test_flat])\n",
    "    \n",
    "    # Normalize other features using min-max scaling based on train data\n",
    "    min_vals = np.min(train_other_features, axis=0)\n",
    "    max_vals = np.max(train_other_features, axis=0)\n",
    "    train_normalized_features = (train_other_features - min_vals) / (max_vals - min_vals)\n",
    "    test_normalized_features = (test_other_features - min_vals) / (max_vals - min_vals)\n",
    "    \n",
    "    # Normalize distances using log transformation and then min-max scaling based on train data\n",
    "    train_log_distances = np.log1p(train_distances)\n",
    "    test_log_distances = np.log1p(test_distances)\n",
    "    min_dist = np.min(train_log_distances)\n",
    "    max_dist = np.max(train_log_distances)\n",
    "    train_normalized_distances = (train_log_distances - min_dist) / (max_dist - min_dist)\n",
    "    test_normalized_distances = (test_log_distances - min_dist) / (max_dist - min_dist)\n",
    "    \n",
    "    # Combine normalized features and distances\n",
    "    def reconstruct_data(features, distances, original_data):\n",
    "        normalized_data = []\n",
    "        idx = 0\n",
    "        for sublist in original_data:\n",
    "            normalized_sublist = []\n",
    "            for _ in sublist:\n",
    "                normalized_sublist.append(list(features[idx][:feature_dim-1]) + [distances[idx]])\n",
    "                idx += 1\n",
    "            normalized_data.append(normalized_sublist)\n",
    "        return normalized_data\n",
    "    \n",
    "    train_normalized = reconstruct_data(train_normalized_features, train_normalized_distances, train_data)\n",
    "    test_normalized = reconstruct_data(test_normalized_features, test_normalized_distances, test_data)\n",
    "    \n",
    "    return train_normalized, test_normalized\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "# Set random seed\n",
    "torch.manual_seed(25)\n",
    "\n",
    "HIDDEN_DIM = 16\n",
    "NUM_LAYERS = 2\n",
    "NUM_HEADS = 1\n",
    "DROPOUT = 0.2\n",
    "LR = 0.001\n",
    "OUTPUT_DIM = 3  # number of classes (-1, 0, 1)\n",
    "\n",
    "\n",
    "# Define the attention-based model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, num_heads=1, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(hidden_dim, num_heads, dim_feedforward=hidden_dim*2, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.classifier = nn.Linear(hidden_dim, output_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=~mask.bool())\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = x.mean(dim=1)\n",
    "        \n",
    "        output = self.classifier(x)\n",
    "        return output, None  # Return None for attention weights as they're not directly accessible\n",
    "\n",
    "# Custom dataset\n",
    "class GeneDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        gene_data = torch.FloatTensor(self.data[idx])\n",
    "        label = torch.LongTensor([self.labels[idx] + 1])  # Add 1 to shift labels to 0, 1, 2\n",
    "        mask = torch.ones(len(gene_data))\n",
    "        return gene_data, label, mask\n",
    "\n",
    "# Collate function for DataLoader\n",
    "def collate_fn(batch):\n",
    "    # Sort the batch by sequence length (descending)\n",
    "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    sequences, labels, masks = zip(*batch)\n",
    "    \n",
    "    # Get lengths of each sequence\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "    max_len = max(lengths)\n",
    "    \n",
    "    # Pad sequences\n",
    "    padded_seqs = torch.zeros(len(sequences), max_len, sequences[0].size(1))\n",
    "    padded_masks = torch.zeros(len(sequences), max_len)\n",
    "    \n",
    "    for i, (seq, length) in enumerate(zip(sequences, lengths)):\n",
    "        padded_seqs[i, :length] = seq\n",
    "        padded_masks[i, :length] = 1\n",
    "    \n",
    "    return padded_seqs, torch.cat(labels), padded_masks\n",
    "\n",
    "def train_model(X_train_normalized, y_train_raw, X_test_normalized, y_test_raw):\n",
    "    input_dim = len(X_train_normalized[0][0])\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = GeneDataset(X_train_normalized, y_train_raw)\n",
    "    test_dataset = GeneDataset(X_test_normalized, y_test_raw)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = TransformerModel(input_dim, HIDDEN_DIM, OUTPUT_DIM, num_layers=NUM_LAYERS, num_heads=NUM_HEADS, dropout=DROPOUT)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 50\n",
    "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=0.0001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        for batch_x, batch_y, batch_mask in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(batch_x, batch_mask)\n",
    "            loss = criterion(outputs, batch_y.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            train_correct += (outputs.argmax(dim=1) == batch_y.squeeze()).sum().item()\n",
    "            train_total += batch_y.size(0)\n",
    "        # scheduler.step()\n",
    "\n",
    "        # scheduler.step()\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y, batch_mask in test_loader:\n",
    "                outputs, _ = model(batch_x, batch_mask)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += batch_y.size(0)\n",
    "                correct += (predicted == batch_y.squeeze()).sum().item()\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        # print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {total_loss/len(train_loader):.4f}, Train Accuracy: {train_correct/train_total:.4f}, Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    # Final evaluation\n",
    "    model.eval()\n",
    "    all_attention_weights = []\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y, batch_mask in test_loader:\n",
    "            outputs, _ = model(batch_x, batch_mask)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    # Print final accuracy\n",
    "    final_accuracy = sum(np.array(all_predictions) == np.array(all_labels).squeeze()) / len(all_labels)\n",
    "    print(f'Final Test Accuracy: {final_accuracy:.4f}')\n",
    "    return final_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]/opt/conda/envs/gene/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n",
      " 20%|██        | 1/5 [00:20<01:23, 20.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 0.5247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:39<00:59, 19.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 0.5062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:59<00:39, 19.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 0.5062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [01:19<00:19, 19.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 0.5309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:38<00:00, 19.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 0.4630\n",
      "Mean Accuracy: 0.5062 ± 0.0237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=25)\n",
    "accuracies = []\n",
    "for train_index, test_index in tqdm(kf.split(X_balanced['mcg']), total=5):\n",
    "    X_train, X_test = [X_balanced['mcg'][i] for i in train_index], [X_balanced['mcg'][i] for i in test_index]\n",
    "    y_train, y_test = [y_balanced[i] for i in train_index], [y_balanced[i] for i in test_index]\n",
    "    X_train_normalized, X_test_normalized = normalize_features(X_train, X_test)\n",
    "    accuracies.append(train_model(X_train_normalized, y_train, X_test_normalized, y_test))\n",
    "print(f'Mean Accuracy: {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:19<01:16, 19.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 0.5617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:38<00:57, 19.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 0.5340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:57<00:38, 19.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 0.5432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [01:17<00:19, 19.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 0.5617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:36<00:00, 19.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 0.4907\n",
      "Mean Accuracy: 0.5383 ± 0.0261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=25)\n",
    "accuracies = []\n",
    "for train_index, test_index in tqdm(kf.split(X_balanced['atac']), total=5):\n",
    "    X_train, X_test = [X_balanced['atac'][i] for i in train_index], [X_balanced['atac'][i] for i in test_index]\n",
    "    y_train, y_test = [y_balanced[i] for i in train_index], [y_balanced[i] for i in test_index]\n",
    "    X_train_normalized, X_test_normalized = normalize_features(X_train, X_test)\n",
    "    accuracies.append(train_model(X_train_normalized, y_train, X_test_normalized, y_test))\n",
    "print(f'Mean Accuracy: {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TwoHeadTransformerModel(nn.Module):\n",
    "    def __init__(self, mcg_input_dim, atac_input_dim, hidden_dim, output_dim, num_layers=2, num_heads=1, dropout=0.1):\n",
    "        super(TwoHeadTransformerModel, self).__init__()\n",
    "        self.mcg_embedding = nn.Linear(mcg_input_dim, hidden_dim)\n",
    "        self.atac_embedding = nn.Linear(atac_input_dim, hidden_dim)\n",
    "        \n",
    "        encoder_layers = nn.TransformerEncoderLayer(hidden_dim, num_heads, dim_feedforward=hidden_dim*2, dropout=dropout, batch_first=True)\n",
    "        self.mcg_transformer = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.atac_transformer = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, mcg_x, mcg_mask, atac_x, atac_mask):\n",
    "        mcg_x = self.mcg_embedding(mcg_x)\n",
    "        atac_x = self.atac_embedding(atac_x)\n",
    "        \n",
    "        mcg_x = self.mcg_transformer(mcg_x, src_key_padding_mask=~mcg_mask.bool())\n",
    "        atac_x = self.atac_transformer(atac_x, src_key_padding_mask=~atac_mask.bool())\n",
    "        \n",
    "        # Global average pooling\n",
    "        mcg_x = mcg_x.mean(dim=1)\n",
    "        atac_x = atac_x.mean(dim=1)\n",
    "        \n",
    "        # Concatenate MCG and ATAC embeddings\n",
    "        combined_x = torch.cat((mcg_x, atac_x), dim=1)\n",
    "        \n",
    "        output = self.classifier(combined_x)\n",
    "        return output\n",
    "\n",
    "class CombinedGeneDataset(Dataset):\n",
    "    def __init__(self, mcg_data, atac_data, labels):\n",
    "        self.mcg_data = mcg_data\n",
    "        self.atac_data = atac_data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        mcg_gene_data = torch.FloatTensor(self.mcg_data[idx])\n",
    "        atac_gene_data = torch.FloatTensor(self.atac_data[idx])\n",
    "        label = torch.LongTensor([self.labels[idx] + 1])  # Add 1 to shift labels to 0, 1, 2\n",
    "        mcg_mask = torch.ones(len(mcg_gene_data))\n",
    "        atac_mask = torch.ones(len(atac_gene_data))\n",
    "        return mcg_gene_data, atac_gene_data, label, mcg_mask, atac_mask\n",
    "\n",
    "def combined_collate_fn(batch):\n",
    "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    mcg_sequences, atac_sequences, labels, mcg_masks, atac_masks = zip(*batch)\n",
    "    \n",
    "    mcg_lengths = [len(seq) for seq in mcg_sequences]\n",
    "    atac_lengths = [len(seq) for seq in atac_sequences]\n",
    "    mcg_max_len = max(mcg_lengths)\n",
    "    atac_max_len = max(atac_lengths)\n",
    "    \n",
    "    padded_mcg_seqs = torch.zeros(len(mcg_sequences), mcg_max_len, mcg_sequences[0].size(1))\n",
    "    padded_atac_seqs = torch.zeros(len(atac_sequences), atac_max_len, atac_sequences[0].size(1))\n",
    "    padded_mcg_masks = torch.zeros(len(mcg_sequences), mcg_max_len)\n",
    "    padded_atac_masks = torch.zeros(len(atac_sequences), atac_max_len)\n",
    "    \n",
    "    for i, (mcg_seq, atac_seq, mcg_length, atac_length) in enumerate(zip(mcg_sequences, atac_sequences, mcg_lengths, atac_lengths)):\n",
    "        padded_mcg_seqs[i, :mcg_length] = mcg_seq\n",
    "        padded_atac_seqs[i, :atac_length] = atac_seq\n",
    "        padded_mcg_masks[i, :mcg_length] = 1\n",
    "        padded_atac_masks[i, :atac_length] = 1\n",
    "    \n",
    "    return padded_mcg_seqs, padded_atac_seqs, torch.cat(labels), padded_mcg_masks, padded_atac_masks\n",
    "\n",
    "def train_combined_model(X_train_mcg, X_train_atac, y_train, X_test_mcg, X_test_atac, y_test):\n",
    "    wandb.init(project='gene')\n",
    "    mcg_input_dim = len(X_train_mcg[0][0])\n",
    "    atac_input_dim = len(X_train_atac[0][0])\n",
    "    \n",
    "    train_dataset = CombinedGeneDataset(X_train_mcg, X_train_atac, y_train)\n",
    "    test_dataset = CombinedGeneDataset(X_test_mcg, X_test_atac, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=combined_collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=combined_collate_fn)\n",
    "\n",
    "    model = TwoHeadTransformerModel(mcg_input_dim, atac_input_dim, HIDDEN_DIM, OUTPUT_DIM, num_layers=NUM_LAYERS, num_heads=NUM_HEADS, dropout=DROPOUT)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    num_epochs = 50\n",
    "    lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=0.0001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        for mcg_x, atac_x, batch_y, mcg_mask, atac_mask in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(mcg_x, mcg_mask, atac_x, atac_mask)\n",
    "            loss = criterion(outputs, batch_y.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "            train_correct += (outputs.argmax(dim=1) == batch_y.squeeze()).sum().item()\n",
    "            train_total += batch_y.size(0)\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for mcg_x, atac_x, batch_y, mcg_mask, atac_mask in test_loader:\n",
    "                outputs = model(mcg_x, mcg_mask, atac_x, atac_mask)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += batch_y.size(0)\n",
    "                correct += (predicted == batch_y.squeeze()).sum().item()\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {total_loss/len(train_loader):.4f}, Train Accuracy: {train_correct/train_total:.4f}, Test Accuracy: {accuracy:.4f}')\n",
    "        wandb.log({'train_loss': total_loss/len(train_loader), 'train_accuracy': train_correct/train_total, 'test_accuracy': accuracy})\n",
    "\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for mcg_x, atac_x, batch_y, mcg_mask, atac_mask in test_loader:\n",
    "            outputs = model(mcg_x, mcg_mask, atac_x, atac_mask)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    final_accuracy = sum(np.array(all_predictions) == np.array(all_labels).squeeze()) / len(all_labels)\n",
    "    print(f'Final Test Accuracy: {final_accuracy:.4f}')\n",
    "    return final_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 1.0521, Train Accuracy: 0.4421, Test Accuracy: 0.5216\n",
      "Epoch [2/50], Train Loss: 1.0077, Train Accuracy: 0.4861, Test Accuracy: 0.5278\n",
      "Epoch [3/50], Train Loss: 1.0000, Train Accuracy: 0.4900, Test Accuracy: 0.5309\n",
      "Epoch [4/50], Train Loss: 0.9922, Train Accuracy: 0.4946, Test Accuracy: 0.5370\n",
      "Epoch [5/50], Train Loss: 0.9863, Train Accuracy: 0.5023, Test Accuracy: 0.5000\n",
      "Epoch [6/50], Train Loss: 0.9902, Train Accuracy: 0.4969, Test Accuracy: 0.5463\n",
      "Epoch [7/50], Train Loss: 0.9779, Train Accuracy: 0.5147, Test Accuracy: 0.5401\n",
      "Epoch [8/50], Train Loss: 0.9692, Train Accuracy: 0.5224, Test Accuracy: 0.5525\n",
      "Epoch [9/50], Train Loss: 0.9632, Train Accuracy: 0.5162, Test Accuracy: 0.5525\n",
      "Epoch [10/50], Train Loss: 0.9645, Train Accuracy: 0.5131, Test Accuracy: 0.5525\n",
      "Epoch [11/50], Train Loss: 0.9620, Train Accuracy: 0.5170, Test Accuracy: 0.5463\n",
      "Epoch [12/50], Train Loss: 0.9691, Train Accuracy: 0.5208, Test Accuracy: 0.5586\n",
      "Epoch [13/50], Train Loss: 0.9579, Train Accuracy: 0.5370, Test Accuracy: 0.5463\n",
      "Epoch [14/50], Train Loss: 0.9637, Train Accuracy: 0.5262, Test Accuracy: 0.5648\n",
      "Epoch [15/50], Train Loss: 0.9601, Train Accuracy: 0.5247, Test Accuracy: 0.5772\n",
      "Epoch [16/50], Train Loss: 0.9487, Train Accuracy: 0.5301, Test Accuracy: 0.5617\n",
      "Epoch [17/50], Train Loss: 0.9620, Train Accuracy: 0.5201, Test Accuracy: 0.5586\n",
      "Epoch [18/50], Train Loss: 0.9464, Train Accuracy: 0.5355, Test Accuracy: 0.5679\n",
      "Epoch [19/50], Train Loss: 0.9522, Train Accuracy: 0.5347, Test Accuracy: 0.5617\n",
      "Epoch [20/50], Train Loss: 0.9500, Train Accuracy: 0.5378, Test Accuracy: 0.5679\n",
      "Epoch [21/50], Train Loss: 0.9516, Train Accuracy: 0.5216, Test Accuracy: 0.5648\n",
      "Epoch [22/50], Train Loss: 0.9486, Train Accuracy: 0.5316, Test Accuracy: 0.5586\n",
      "Epoch [23/50], Train Loss: 0.9451, Train Accuracy: 0.5332, Test Accuracy: 0.5586\n",
      "Epoch [24/50], Train Loss: 0.9521, Train Accuracy: 0.5278, Test Accuracy: 0.5648\n",
      "Epoch [25/50], Train Loss: 0.9447, Train Accuracy: 0.5355, Test Accuracy: 0.5586\n",
      "Epoch [26/50], Train Loss: 0.9582, Train Accuracy: 0.5347, Test Accuracy: 0.5617\n",
      "Epoch [27/50], Train Loss: 0.9406, Train Accuracy: 0.5363, Test Accuracy: 0.5401\n",
      "Epoch [28/50], Train Loss: 0.9486, Train Accuracy: 0.5401, Test Accuracy: 0.5648\n",
      "Epoch [29/50], Train Loss: 0.9540, Train Accuracy: 0.5332, Test Accuracy: 0.5772\n",
      "Epoch [30/50], Train Loss: 0.9478, Train Accuracy: 0.5332, Test Accuracy: 0.5710\n",
      "Epoch [31/50], Train Loss: 0.9589, Train Accuracy: 0.5255, Test Accuracy: 0.5617\n",
      "Epoch [32/50], Train Loss: 0.9465, Train Accuracy: 0.5309, Test Accuracy: 0.5648\n",
      "Epoch [33/50], Train Loss: 0.9426, Train Accuracy: 0.5324, Test Accuracy: 0.5617\n",
      "Epoch [34/50], Train Loss: 0.9467, Train Accuracy: 0.5316, Test Accuracy: 0.5679\n",
      "Epoch [35/50], Train Loss: 0.9445, Train Accuracy: 0.5378, Test Accuracy: 0.5648\n",
      "Epoch [36/50], Train Loss: 0.9475, Train Accuracy: 0.5285, Test Accuracy: 0.5710\n",
      "Epoch [37/50], Train Loss: 0.9344, Train Accuracy: 0.5401, Test Accuracy: 0.5648\n",
      "Epoch [38/50], Train Loss: 0.9450, Train Accuracy: 0.5332, Test Accuracy: 0.5648\n",
      "Epoch [39/50], Train Loss: 0.9427, Train Accuracy: 0.5293, Test Accuracy: 0.5679\n",
      "Epoch [40/50], Train Loss: 0.9365, Train Accuracy: 0.5440, Test Accuracy: 0.5586\n",
      "Epoch [41/50], Train Loss: 0.9397, Train Accuracy: 0.5409, Test Accuracy: 0.5710\n",
      "Epoch [42/50], Train Loss: 0.9513, Train Accuracy: 0.5193, Test Accuracy: 0.5679\n",
      "Epoch [43/50], Train Loss: 0.9405, Train Accuracy: 0.5332, Test Accuracy: 0.5710\n",
      "Epoch [44/50], Train Loss: 0.9387, Train Accuracy: 0.5363, Test Accuracy: 0.5772\n",
      "Epoch [45/50], Train Loss: 0.9407, Train Accuracy: 0.5455, Test Accuracy: 0.5648\n",
      "Epoch [46/50], Train Loss: 0.9394, Train Accuracy: 0.5424, Test Accuracy: 0.5710\n",
      "Epoch [47/50], Train Loss: 0.9526, Train Accuracy: 0.5340, Test Accuracy: 0.5802\n",
      "Epoch [48/50], Train Loss: 0.9499, Train Accuracy: 0.5316, Test Accuracy: 0.5741\n",
      "Epoch [49/50], Train Loss: 0.9303, Train Accuracy: 0.5448, Test Accuracy: 0.5710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:36<02:27, 36.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/50], Train Loss: 0.9318, Train Accuracy: 0.5463, Test Accuracy: 0.5772\n",
      "Final Test Accuracy: 0.5772\n",
      "Epoch [1/50], Train Loss: 1.0685, Train Accuracy: 0.4313, Test Accuracy: 0.4846\n",
      "Epoch [2/50], Train Loss: 1.0088, Train Accuracy: 0.4815, Test Accuracy: 0.4784\n",
      "Epoch [3/50], Train Loss: 0.9925, Train Accuracy: 0.4869, Test Accuracy: 0.5123\n",
      "Epoch [4/50], Train Loss: 0.9881, Train Accuracy: 0.4923, Test Accuracy: 0.5123\n",
      "Epoch [5/50], Train Loss: 0.9924, Train Accuracy: 0.4923, Test Accuracy: 0.5154\n",
      "Epoch [6/50], Train Loss: 0.9875, Train Accuracy: 0.4846, Test Accuracy: 0.5093\n",
      "Epoch [7/50], Train Loss: 0.9823, Train Accuracy: 0.5023, Test Accuracy: 0.5185\n",
      "Epoch [8/50], Train Loss: 0.9835, Train Accuracy: 0.5015, Test Accuracy: 0.5216\n",
      "Epoch [9/50], Train Loss: 0.9690, Train Accuracy: 0.5162, Test Accuracy: 0.5154\n",
      "Epoch [10/50], Train Loss: 0.9667, Train Accuracy: 0.5069, Test Accuracy: 0.5370\n",
      "Epoch [11/50], Train Loss: 0.9704, Train Accuracy: 0.5100, Test Accuracy: 0.5062\n",
      "Epoch [12/50], Train Loss: 0.9645, Train Accuracy: 0.5231, Test Accuracy: 0.5123\n",
      "Epoch [13/50], Train Loss: 0.9619, Train Accuracy: 0.5100, Test Accuracy: 0.5309\n",
      "Epoch [14/50], Train Loss: 0.9529, Train Accuracy: 0.5116, Test Accuracy: 0.5000\n",
      "Epoch [15/50], Train Loss: 0.9446, Train Accuracy: 0.5370, Test Accuracy: 0.5216\n",
      "Epoch [16/50], Train Loss: 0.9440, Train Accuracy: 0.5386, Test Accuracy: 0.5247\n",
      "Epoch [17/50], Train Loss: 0.9611, Train Accuracy: 0.5170, Test Accuracy: 0.5309\n",
      "Epoch [18/50], Train Loss: 0.9461, Train Accuracy: 0.5363, Test Accuracy: 0.5247\n",
      "Epoch [19/50], Train Loss: 0.9486, Train Accuracy: 0.5285, Test Accuracy: 0.5370\n",
      "Epoch [20/50], Train Loss: 0.9351, Train Accuracy: 0.5401, Test Accuracy: 0.5000\n",
      "Epoch [21/50], Train Loss: 0.9383, Train Accuracy: 0.5394, Test Accuracy: 0.5340\n",
      "Epoch [22/50], Train Loss: 0.9311, Train Accuracy: 0.5401, Test Accuracy: 0.5093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:54<03:36, 54.14s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     X_train_mcg_normalized, X_test_mcg_normalized \u001b[38;5;241m=\u001b[39m normalize_features(X_train_mcg, X_test_mcg)\n\u001b[1;32m      9\u001b[0m     X_train_atac_normalized, X_test_atac_normalized \u001b[38;5;241m=\u001b[39m normalize_features(X_train_atac, X_test_atac)\n\u001b[0;32m---> 11\u001b[0m     accuracies\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtrain_combined_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_mcg_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_atac_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mX_test_mcg_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_atac_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(accuracies)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ± \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mstd(accuracies)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[55], line 106\u001b[0m, in \u001b[0;36mtrain_combined_model\u001b[0;34m(X_train_mcg, X_train_atac, y_train, X_test_mcg, X_test_atac, y_test)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m mcg_x, atac_x, batch_y, mcg_mask, atac_mask \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[0;32m--> 106\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmcg_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmcg_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matac_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matac_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m         _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    108\u001b[0m         total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_y\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/gene/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/gene/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[55], line 18\u001b[0m, in \u001b[0;36mTwoHeadTransformerModel.forward\u001b[0;34m(self, mcg_x, mcg_mask, atac_x, atac_mask)\u001b[0m\n\u001b[1;32m     15\u001b[0m mcg_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmcg_embedding(mcg_x)\n\u001b[1;32m     16\u001b[0m atac_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matac_embedding(atac_x)\n\u001b[0;32m---> 18\u001b[0m mcg_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmcg_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmcg_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mmcg_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m atac_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matac_transformer(atac_x, src_key_padding_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m~\u001b[39matac_mask\u001b[38;5;241m.\u001b[39mbool())\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Global average pooling\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/gene/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/gene/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/envs/gene/lib/python3.10/site-packages/torch/nn/modules/transformer.py:511\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    508\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 511\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    519\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.0\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m/opt/conda/envs/gene/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/gene/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/envs/gene/lib/python3.10/site-packages/torch/nn/modules/transformer.py:906\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    902\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(\n\u001b[1;32m    903\u001b[0m         x\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, src_mask, src_key_padding_mask, is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m    905\u001b[0m     )\n\u001b[0;32m--> 906\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ff_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/conda/envs/gene/lib/python3.10/site-packages/torch/nn/modules/transformer.py:931\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._ff_block\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ff_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 931\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)))\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n",
      "File \u001b[0;32m/opt/conda/envs/gene/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/gene/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/envs/gene/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/envs/gene/lib/python3.10/site-packages/torch/nn/modules/module.py:1918\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1909\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1911\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1912\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1913\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1918\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1919\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1920\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=25)\n",
    "accuracies = []\n",
    "for train_index, test_index in tqdm(kf.split(X_balanced['mcg']), total=5):\n",
    "    X_train_mcg, X_test_mcg = [X_balanced['mcg'][i] for i in train_index], [X_balanced['mcg'][i] for i in test_index]\n",
    "    X_train_atac, X_test_atac = [X_balanced['atac'][i] for i in train_index], [X_balanced['atac'][i] for i in test_index]\n",
    "    y_train, y_test = [y_balanced[i] for i in train_index], [y_balanced[i] for i in test_index]\n",
    "    \n",
    "    X_train_mcg_normalized, X_test_mcg_normalized = normalize_features(X_train_mcg, X_test_mcg)\n",
    "    X_train_atac_normalized, X_test_atac_normalized = normalize_features(X_train_atac, X_test_atac)\n",
    "    \n",
    "    accuracies.append(train_combined_model(X_train_mcg_normalized, X_train_atac_normalized, y_train, \n",
    "                                           X_test_mcg_normalized, X_test_atac_normalized, y_test))\n",
    "\n",
    "print(f'Mean Accuracy: {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gene",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
