{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('data/Oligo_NN.RNA_DEG.csv')\n",
    "df.set_index('gene', inplace=True)\n",
    "df.head()\n",
    "\n",
    "# non_zero_genes = df[df['DEG'] != 0].index\n",
    "\n",
    "# df = df[df.index.isin(non_zero_genes)]\n",
    "df\n",
    "FEATURE_NAMES = ['2mo', '9mo', '18mo', '9mo-2mo', '18mo-9mo', '9mo/2mo', '18mo/9mo', 'old-young', 'old/young', 'distance']\n",
    "# FEATURE_NAMES = ['2mo', '9mo', '18mo', '9mo-2mo', '18mo-9mo', '9mo/2mo', '18mo/9mo', 'old-young', 'old/young', 'distance']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene</th>\n",
       "      <th>2mo</th>\n",
       "      <th>9mo</th>\n",
       "      <th>18mo</th>\n",
       "      <th>9mo-2mo</th>\n",
       "      <th>18mo-9mo</th>\n",
       "      <th>9mo/2mo</th>\n",
       "      <th>18mo/9mo</th>\n",
       "      <th>old-young</th>\n",
       "      <th>old/young</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rgs20</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.90</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.384615</td>\n",
       "      <td>151241.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sulf1</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.444444</td>\n",
       "      <td>1.076923</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.555556</td>\n",
       "      <td>121205.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sulf1</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.372093</td>\n",
       "      <td>1.084746</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.488372</td>\n",
       "      <td>170142.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Eya1</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.47</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.758065</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.691176</td>\n",
       "      <td>137980.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eya1</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.606557</td>\n",
       "      <td>1.216216</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.737705</td>\n",
       "      <td>138254.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    gene   2mo   9mo  18mo  9mo-2mo  18mo-9mo   9mo/2mo  18mo/9mo  old-young  \\\n",
       "0  Rgs20  0.65  0.60  0.90    -0.05      0.30  0.923077  1.500000       0.25   \n",
       "1  Sulf1  0.36  0.52  0.56     0.16      0.04  1.444444  1.076923       0.20   \n",
       "2  Sulf1  0.43  0.59  0.64     0.16      0.05  1.372093  1.084746       0.21   \n",
       "3   Eya1  0.68  0.62  0.47    -0.06     -0.15  0.911765  0.758065      -0.21   \n",
       "4   Eya1  0.61  0.37  0.45    -0.24      0.08  0.606557  1.216216      -0.16   \n",
       "\n",
       "   old/young  distance  \n",
       "0   1.384615  151241.0  \n",
       "1   1.555556  121205.0  \n",
       "2   1.488372  170142.0  \n",
       "3   0.691176  137980.0  \n",
       "4   0.737705  138254.0  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene2value = df[['DEG']]\n",
    "\n",
    "\n",
    "mcg = pd.read_csv('data/Oligo_NN.aDMR_gene.csv')\n",
    "mcg_feat = mcg\n",
    "mcg_feat.rename(columns={'gene_name': 'gene'}, inplace=True)\n",
    "mcg_feat['distance'] = (mcg_feat['gene_start'] - mcg_feat['start']).abs().astype(np.float64)\n",
    "mcg_feat['old/young'] = mcg_feat['18mo'] / mcg_feat['2mo']\n",
    "mcg_feat['9mo-2mo'] = mcg_feat['9mo'] - mcg_feat['2mo']\n",
    "mcg_feat['18mo-9mo'] = mcg_feat['18mo'] - mcg_feat['9mo']\n",
    "mcg_feat['9mo/2mo'] = mcg_feat['9mo'] / mcg_feat['2mo']\n",
    "mcg_feat['18mo/9mo'] = mcg_feat['18mo'] / mcg_feat['9mo']\n",
    "mcg_feat = mcg_feat[['gene', *FEATURE_NAMES]]\n",
    "mcg_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2321"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_genes = set(gene2value.index) & set(mcg_feat['gene'])\n",
    "len(shared_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene</th>\n",
       "      <th>2mo</th>\n",
       "      <th>9mo</th>\n",
       "      <th>18mo</th>\n",
       "      <th>9mo-2mo</th>\n",
       "      <th>18mo-9mo</th>\n",
       "      <th>9mo/2mo</th>\n",
       "      <th>18mo/9mo</th>\n",
       "      <th>old-young</th>\n",
       "      <th>old/young</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rgs20</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.90</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.384615</td>\n",
       "      <td>151241.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sulf1</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.444444</td>\n",
       "      <td>1.076923</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.555556</td>\n",
       "      <td>121205.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sulf1</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.372093</td>\n",
       "      <td>1.084746</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.488372</td>\n",
       "      <td>170142.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Eya1</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.47</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.758065</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.691176</td>\n",
       "      <td>137980.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eya1</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.606557</td>\n",
       "      <td>1.216216</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.737705</td>\n",
       "      <td>138254.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    gene   2mo   9mo  18mo  9mo-2mo  18mo-9mo   9mo/2mo  18mo/9mo  old-young  \\\n",
       "0  Rgs20  0.65  0.60  0.90    -0.05      0.30  0.923077  1.500000       0.25   \n",
       "1  Sulf1  0.36  0.52  0.56     0.16      0.04  1.444444  1.076923       0.20   \n",
       "2  Sulf1  0.43  0.59  0.64     0.16      0.05  1.372093  1.084746       0.21   \n",
       "3   Eya1  0.68  0.62  0.47    -0.06     -0.15  0.911765  0.758065      -0.21   \n",
       "4   Eya1  0.61  0.37  0.45    -0.24      0.08  0.606557  1.216216      -0.16   \n",
       "\n",
       "   old/young  distance  \n",
       "0   1.384615  151241.0  \n",
       "1   1.555556  121205.0  \n",
       "2   1.488372  170142.0  \n",
       "3   0.691176  137980.0  \n",
       "4   0.737705  138254.0  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene2value = gene2value[gene2value.index.isin(shared_genes)]\n",
    "mcg_feat = mcg_feat[mcg_feat['gene'].isin(shared_genes)]\n",
    "\n",
    "mcg_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2mo</th>\n",
       "      <th>9mo</th>\n",
       "      <th>18mo</th>\n",
       "      <th>9mo-2mo</th>\n",
       "      <th>18mo-9mo</th>\n",
       "      <th>9mo/2mo</th>\n",
       "      <th>18mo/9mo</th>\n",
       "      <th>old-young</th>\n",
       "      <th>old/young</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gene</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Rgs20</th>\n",
       "      <td>0.650</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.90</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1.384615</td>\n",
       "      <td>151241.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sulf1</th>\n",
       "      <td>0.395</td>\n",
       "      <td>0.555000</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>1.408269</td>\n",
       "      <td>1.080834</td>\n",
       "      <td>0.205</td>\n",
       "      <td>1.521964</td>\n",
       "      <td>145673.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eya1</th>\n",
       "      <td>0.630</td>\n",
       "      <td>0.453333</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-0.176667</td>\n",
       "      <td>-0.003333</td>\n",
       "      <td>0.711663</td>\n",
       "      <td>1.045481</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>0.715183</td>\n",
       "      <td>138168.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stau2</th>\n",
       "      <td>0.370</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>1.540541</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.150</td>\n",
       "      <td>1.405405</td>\n",
       "      <td>23034.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ube2w</th>\n",
       "      <td>0.490</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>1.265306</td>\n",
       "      <td>1.129032</td>\n",
       "      <td>0.210</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>28480.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         2mo       9mo  18mo   9mo-2mo  18mo-9mo   9mo/2mo  18mo/9mo  \\\n",
       "gene                                                                   \n",
       "Rgs20  0.650  0.600000  0.90 -0.050000  0.300000  0.923077  1.500000   \n",
       "Sulf1  0.395  0.555000  0.60  0.160000  0.045000  1.408269  1.080834   \n",
       "Eya1   0.630  0.453333  0.45 -0.176667 -0.003333  0.711663  1.045481   \n",
       "Stau2  0.370  0.570000  0.52  0.200000 -0.050000  1.540541  0.912281   \n",
       "Ube2w  0.490  0.620000  0.70  0.130000  0.080000  1.265306  1.129032   \n",
       "\n",
       "       old-young  old/young       distance  \n",
       "gene                                        \n",
       "Rgs20      0.250   1.384615  151241.000000  \n",
       "Sulf1      0.205   1.521964  145673.500000  \n",
       "Eya1      -0.180   0.715183  138168.666667  \n",
       "Stau2      0.150   1.405405   23034.000000  \n",
       "Ube2w      0.210   1.428571   28480.000000  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcg_mean = mcg_feat.groupby('gene').mean()\n",
    "mcg_mean.head()\n",
    "# Sort mcg_mean by gene name\n",
    "mcg_mean = mcg_mean.loc[gene2value.index]\n",
    "mcg_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gene/lib/python3.10/site-packages/numpy/lib/_function_base_impl.py:2842: RuntimeWarning: invalid value encountered in subtract\n",
      "  X -= avg[:, None]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2mo          0.034833\n",
       "9mo         -0.016470\n",
       "18mo        -0.038960\n",
       "9mo-2mo     -0.046858\n",
       "18mo-9mo    -0.060399\n",
       "9mo/2mo           NaN\n",
       "18mo/9mo          NaN\n",
       "old-young   -0.060147\n",
       "old/young         NaN\n",
       "distance     0.101508\n",
       "dtype: float64"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcg_mean.corrwith(gene2value['DEG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_3008187/1269483849.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  list_mcg_feat = mcg_feat.groupby('gene').apply(lambda x: x[FEATURE_NAMES].values.tolist())\n"
     ]
    }
   ],
   "source": [
    "# Train a sequence model on mcg_feat to predict gene2value['log2(old/young)']\n",
    "# Each gene has a sequence of 4 features, 2mo, 9mo, 18mo, old-young\n",
    "# The sequence length is not fixed, so we need to use a dynamic model\n",
    "# Let's use a commonly used sequence prediction model for sentence classification\n",
    "# like LSTM or Transformer\n",
    "\n",
    "# Step 1: Prepare the data\n",
    "list_mcg_feat = mcg_feat.groupby('gene').apply(lambda x: x[FEATURE_NAMES].values.tolist())\n",
    "x = list_mcg_feat.values.tolist()\n",
    "\n",
    "# # Find the maximum length and number of features\n",
    "# max_len = max(len(seq) for seq in x)\n",
    "# n_features = len(x[0][0])\n",
    "\n",
    "# # Pad sequences to the same length and reshape\n",
    "# x_padded = np.full((len(x), max_len, n_features), 0)\n",
    "# for i, seq in enumerate(x):\n",
    "#     x_padded[i, :len(seq), :] = seq\n",
    "\n",
    "# # x_padded is now a 3D numpy array with shape (n_samples, max_len, n_features)\n",
    "# print(x_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = gene2value.loc[list_mcg_feat.index]['DEG'].values.tolist()\n",
    "y = np.array([int(i) for i in y])\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero: 2116, non-zero: 205\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(25)\n",
    "\n",
    "# Separate the data into zero and non-zero y values\n",
    "zero_indices = np.where(y == 0)[0]\n",
    "non_zero_indices = np.where(y != 0)[0]\n",
    "print(f'zero: {len(zero_indices)}, non-zero: {len(non_zero_indices)}')\n",
    "\n",
    "# Sample len(non_zero_indices) indices from each group\n",
    "n_samples = len(non_zero_indices)\n",
    "sampled_zero_indices = np.random.choice(zero_indices, n_samples // 2, replace=False)\n",
    "sampled_non_zero_indices = np.random.choice(non_zero_indices, n_samples, replace=False)\n",
    "\n",
    "# Combine the sampled indices\n",
    "sampled_indices = np.concatenate([sampled_zero_indices, sampled_non_zero_indices])\n",
    "\n",
    "# Create balanced dataset\n",
    "X_balanced = [x[i] for i in sampled_indices]\n",
    "y_balanced = y[sampled_indices]\n",
    "\n",
    "# Split the balanced dataset into training and testing sets\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization function\n",
    "def normalize_features(train_data, test_data):\n",
    "    # Flatten the lists for easier processing\n",
    "    train_flat = [item for sublist in train_data for item in sublist]\n",
    "    test_flat = [item for sublist in test_data for item in sublist]\n",
    "    \n",
    "    # Separate features\n",
    "    train_other_features = np.array([item[:len(FEATURE_NAMES)-1] for item in train_flat])\n",
    "    train_distances = np.array([item[len(FEATURE_NAMES)-1] for item in train_flat])\n",
    "    test_other_features = np.array([item[:len(FEATURE_NAMES)-1] for item in test_flat])\n",
    "    test_distances = np.array([item[len(FEATURE_NAMES)-1] for item in test_flat])\n",
    "    \n",
    "    # Normalize other features using min-max scaling based on train data\n",
    "    min_vals = np.min(train_other_features, axis=0)\n",
    "    max_vals = np.max(train_other_features, axis=0)\n",
    "    train_normalized_features = (train_other_features - min_vals) / (max_vals - min_vals)\n",
    "    test_normalized_features = (test_other_features - min_vals) / (max_vals - min_vals)\n",
    "    \n",
    "    # Normalize distances using log transformation and then min-max scaling based on train data\n",
    "    train_log_distances = np.log1p(train_distances)\n",
    "    test_log_distances = np.log1p(test_distances)\n",
    "    min_dist = np.min(train_log_distances)\n",
    "    max_dist = np.max(train_log_distances)\n",
    "    train_normalized_distances = (train_log_distances - min_dist) / (max_dist - min_dist)\n",
    "    test_normalized_distances = (test_log_distances - min_dist) / (max_dist - min_dist)\n",
    "    \n",
    "    # Combine normalized features and distances\n",
    "    def reconstruct_data(features, distances, original_data):\n",
    "        normalized_data = []\n",
    "        idx = 0\n",
    "        for sublist in original_data:\n",
    "            normalized_sublist = []\n",
    "            for _ in sublist:\n",
    "                normalized_sublist.append(list(features[idx][:len(FEATURE_NAMES)-1]) + [distances[idx]])\n",
    "                idx += 1\n",
    "            normalized_data.append(normalized_sublist)\n",
    "        return normalized_data\n",
    "    \n",
    "    train_normalized = reconstruct_data(train_normalized_features, train_normalized_distances, train_data)\n",
    "    test_normalized = reconstruct_data(test_normalized_features, test_normalized_distances, test_data)\n",
    "    \n",
    "    return train_normalized, test_normalized\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  0, -1,  0,  0,  0,  1, -1,  1,  0])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_raw[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "# Set random seed\n",
    "torch.manual_seed(25)\n",
    "\n",
    "HIDDEN_DIM = 16\n",
    "NUM_LAYERS = 2\n",
    "NUM_HEADS = 1\n",
    "DROPOUT = 0.2\n",
    "LR = 0.001\n",
    "OUTPUT_DIM = 3  # number of classes (-1, 0, 1)\n",
    "\n",
    "# Define the attention-based model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, num_heads=1, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(hidden_dim, num_heads, dim_feedforward=hidden_dim*2, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.classifier = nn.Linear(hidden_dim, output_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=~mask.bool())\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = x.mean(dim=1)\n",
    "        \n",
    "        output = self.classifier(x)\n",
    "        return output, None  # Return None for attention weights as they're not directly accessible\n",
    "\n",
    "# Custom dataset\n",
    "class GeneDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        gene_data = torch.FloatTensor(self.data[idx])\n",
    "        label = torch.LongTensor([self.labels[idx] + 1])  # Add 1 to shift labels to 0, 1, 2\n",
    "        mask = torch.ones(len(gene_data))\n",
    "        return gene_data, label, mask\n",
    "\n",
    "# Collate function for DataLoader\n",
    "def collate_fn(batch):\n",
    "    # Sort the batch by sequence length (descending)\n",
    "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    sequences, labels, masks = zip(*batch)\n",
    "    \n",
    "    # Get lengths of each sequence\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "    max_len = max(lengths)\n",
    "    \n",
    "    # Pad sequences\n",
    "    padded_seqs = torch.zeros(len(sequences), max_len, sequences[0].size(1))\n",
    "    padded_masks = torch.zeros(len(sequences), max_len)\n",
    "    \n",
    "    for i, (seq, length) in enumerate(zip(sequences, lengths)):\n",
    "        padded_seqs[i, :length] = seq\n",
    "        padded_masks[i, :length] = 1\n",
    "    \n",
    "    return padded_seqs, torch.cat(labels), padded_masks\n",
    "\n",
    "def train_model(X_train_normalized, y_train_raw, X_test_normalized, y_test_raw):\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = GeneDataset(X_train_normalized, y_train_raw)\n",
    "    test_dataset = GeneDataset(X_test_normalized, y_test_raw)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Initialize the model\n",
    "    input_dim = len(FEATURE_NAMES)  # number of features per region\n",
    "    model = TransformerModel(input_dim, HIDDEN_DIM, OUTPUT_DIM, num_layers=NUM_LAYERS, num_heads=NUM_HEADS, dropout=DROPOUT)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 50\n",
    "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=0.0001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        for batch_x, batch_y, batch_mask in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(batch_x, batch_mask)\n",
    "            loss = criterion(outputs, batch_y.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            train_correct += (outputs.argmax(dim=1) == batch_y.squeeze()).sum().item()\n",
    "            train_total += batch_y.size(0)\n",
    "        # scheduler.step()\n",
    "\n",
    "        # scheduler.step()\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y, batch_mask in test_loader:\n",
    "                outputs, _ = model(batch_x, batch_mask)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += batch_y.size(0)\n",
    "                correct += (predicted == batch_y.squeeze()).sum().item()\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {total_loss/len(train_loader):.4f}, Train Accuracy: {train_correct/train_total:.4f}, Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    # Final evaluation\n",
    "    model.eval()\n",
    "    all_attention_weights = []\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y, batch_mask in test_loader:\n",
    "            outputs, _ = model(batch_x, batch_mask)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    # Print final accuracy\n",
    "    final_accuracy = sum(np.array(all_predictions) == np.array(all_labels).squeeze()) / len(all_labels)\n",
    "    print(f'Final Test Accuracy: {final_accuracy:.4f}')\n",
    "    return final_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gene/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 1.1051, Train Accuracy: 0.3918, Test Accuracy: 0.4194\n",
      "Epoch [2/50], Train Loss: 1.0985, Train Accuracy: 0.3755, Test Accuracy: 0.4032\n",
      "Epoch [3/50], Train Loss: 1.0892, Train Accuracy: 0.4122, Test Accuracy: 0.4516\n",
      "Epoch [4/50], Train Loss: 1.0709, Train Accuracy: 0.4571, Test Accuracy: 0.4516\n",
      "Epoch [5/50], Train Loss: 1.0688, Train Accuracy: 0.4286, Test Accuracy: 0.4355\n",
      "Epoch [6/50], Train Loss: 1.0636, Train Accuracy: 0.4612, Test Accuracy: 0.4032\n",
      "Epoch [7/50], Train Loss: 1.0418, Train Accuracy: 0.4939, Test Accuracy: 0.4355\n",
      "Epoch [8/50], Train Loss: 1.0318, Train Accuracy: 0.4776, Test Accuracy: 0.4194\n",
      "Epoch [9/50], Train Loss: 1.0317, Train Accuracy: 0.5061, Test Accuracy: 0.4355\n",
      "Epoch [10/50], Train Loss: 1.0151, Train Accuracy: 0.5224, Test Accuracy: 0.4677\n",
      "Epoch [11/50], Train Loss: 1.0101, Train Accuracy: 0.5102, Test Accuracy: 0.4677\n",
      "Epoch [12/50], Train Loss: 1.0248, Train Accuracy: 0.5020, Test Accuracy: 0.4516\n",
      "Epoch [13/50], Train Loss: 1.0045, Train Accuracy: 0.5265, Test Accuracy: 0.4839\n",
      "Epoch [14/50], Train Loss: 0.9841, Train Accuracy: 0.5551, Test Accuracy: 0.4355\n",
      "Epoch [15/50], Train Loss: 0.9976, Train Accuracy: 0.5265, Test Accuracy: 0.4355\n",
      "Epoch [16/50], Train Loss: 0.9975, Train Accuracy: 0.5265, Test Accuracy: 0.4516\n",
      "Epoch [17/50], Train Loss: 0.9957, Train Accuracy: 0.5224, Test Accuracy: 0.4516\n",
      "Epoch [18/50], Train Loss: 0.9824, Train Accuracy: 0.5265, Test Accuracy: 0.4194\n",
      "Epoch [19/50], Train Loss: 0.9850, Train Accuracy: 0.5469, Test Accuracy: 0.4032\n",
      "Epoch [20/50], Train Loss: 0.9760, Train Accuracy: 0.5469, Test Accuracy: 0.4032\n",
      "Epoch [21/50], Train Loss: 0.9681, Train Accuracy: 0.5265, Test Accuracy: 0.4194\n",
      "Epoch [22/50], Train Loss: 0.9855, Train Accuracy: 0.5061, Test Accuracy: 0.4516\n",
      "Epoch [23/50], Train Loss: 0.9751, Train Accuracy: 0.5265, Test Accuracy: 0.4516\n",
      "Epoch [24/50], Train Loss: 0.9714, Train Accuracy: 0.5347, Test Accuracy: 0.4194\n",
      "Epoch [25/50], Train Loss: 0.9688, Train Accuracy: 0.5388, Test Accuracy: 0.4677\n",
      "Epoch [26/50], Train Loss: 0.9632, Train Accuracy: 0.5592, Test Accuracy: 0.4677\n",
      "Epoch [27/50], Train Loss: 0.9495, Train Accuracy: 0.5469, Test Accuracy: 0.4677\n",
      "Epoch [28/50], Train Loss: 0.9798, Train Accuracy: 0.5347, Test Accuracy: 0.4355\n",
      "Epoch [29/50], Train Loss: 0.9526, Train Accuracy: 0.5469, Test Accuracy: 0.4355\n",
      "Epoch [30/50], Train Loss: 0.9607, Train Accuracy: 0.5429, Test Accuracy: 0.4355\n",
      "Epoch [31/50], Train Loss: 0.9706, Train Accuracy: 0.5224, Test Accuracy: 0.4032\n",
      "Epoch [32/50], Train Loss: 0.9577, Train Accuracy: 0.5388, Test Accuracy: 0.4355\n",
      "Epoch [33/50], Train Loss: 0.9520, Train Accuracy: 0.5510, Test Accuracy: 0.4839\n",
      "Epoch [34/50], Train Loss: 0.9541, Train Accuracy: 0.5551, Test Accuracy: 0.4194\n",
      "Epoch [35/50], Train Loss: 0.9491, Train Accuracy: 0.5673, Test Accuracy: 0.4516\n",
      "Epoch [36/50], Train Loss: 0.9500, Train Accuracy: 0.5673, Test Accuracy: 0.4677\n",
      "Epoch [37/50], Train Loss: 0.9640, Train Accuracy: 0.5224, Test Accuracy: 0.4355\n",
      "Epoch [38/50], Train Loss: 0.9442, Train Accuracy: 0.5714, Test Accuracy: 0.4516\n",
      "Epoch [39/50], Train Loss: 0.9473, Train Accuracy: 0.5633, Test Accuracy: 0.4516\n",
      "Epoch [40/50], Train Loss: 0.9432, Train Accuracy: 0.5469, Test Accuracy: 0.4516\n",
      "Epoch [41/50], Train Loss: 0.9281, Train Accuracy: 0.5755, Test Accuracy: 0.4677\n",
      "Epoch [42/50], Train Loss: 0.9367, Train Accuracy: 0.5673, Test Accuracy: 0.4194\n",
      "Epoch [43/50], Train Loss: 0.9403, Train Accuracy: 0.5714, Test Accuracy: 0.4355\n",
      "Epoch [44/50], Train Loss: 0.9302, Train Accuracy: 0.5673, Test Accuracy: 0.4194\n",
      "Epoch [45/50], Train Loss: 0.9472, Train Accuracy: 0.5592, Test Accuracy: 0.4355\n",
      "Epoch [46/50], Train Loss: 0.9343, Train Accuracy: 0.5429, Test Accuracy: 0.4516\n",
      "Epoch [47/50], Train Loss: 0.9146, Train Accuracy: 0.5796, Test Accuracy: 0.4032\n",
      "Epoch [48/50], Train Loss: 0.9297, Train Accuracy: 0.5673, Test Accuracy: 0.4516\n",
      "Epoch [49/50], Train Loss: 0.9263, Train Accuracy: 0.5714, Test Accuracy: 0.4355\n",
      "Epoch [50/50], Train Loss: 0.9149, Train Accuracy: 0.5592, Test Accuracy: 0.4516\n",
      "Final Test Accuracy: 0.4516\n",
      "Epoch [1/50], Train Loss: 1.1520, Train Accuracy: 0.3388, Test Accuracy: 0.4032\n",
      "Epoch [2/50], Train Loss: 1.1077, Train Accuracy: 0.3796, Test Accuracy: 0.4032\n",
      "Epoch [3/50], Train Loss: 1.0896, Train Accuracy: 0.3796, Test Accuracy: 0.4194\n",
      "Epoch [4/50], Train Loss: 1.0807, Train Accuracy: 0.4122, Test Accuracy: 0.3871\n",
      "Epoch [5/50], Train Loss: 1.0718, Train Accuracy: 0.4653, Test Accuracy: 0.4032\n",
      "Epoch [6/50], Train Loss: 1.0623, Train Accuracy: 0.4612, Test Accuracy: 0.4032\n",
      "Epoch [7/50], Train Loss: 1.0518, Train Accuracy: 0.4816, Test Accuracy: 0.3871\n",
      "Epoch [8/50], Train Loss: 1.0503, Train Accuracy: 0.4694, Test Accuracy: 0.3548\n",
      "Epoch [9/50], Train Loss: 1.0283, Train Accuracy: 0.4939, Test Accuracy: 0.3871\n",
      "Epoch [10/50], Train Loss: 1.0339, Train Accuracy: 0.4939, Test Accuracy: 0.3710\n",
      "Epoch [11/50], Train Loss: 1.0296, Train Accuracy: 0.4857, Test Accuracy: 0.3710\n",
      "Epoch [12/50], Train Loss: 1.0328, Train Accuracy: 0.4939, Test Accuracy: 0.3548\n",
      "Epoch [13/50], Train Loss: 1.0269, Train Accuracy: 0.4980, Test Accuracy: 0.3710\n",
      "Epoch [14/50], Train Loss: 1.0097, Train Accuracy: 0.4816, Test Accuracy: 0.3710\n",
      "Epoch [15/50], Train Loss: 1.0183, Train Accuracy: 0.4939, Test Accuracy: 0.3710\n",
      "Epoch [16/50], Train Loss: 1.0326, Train Accuracy: 0.4857, Test Accuracy: 0.4032\n",
      "Epoch [17/50], Train Loss: 1.0093, Train Accuracy: 0.4980, Test Accuracy: 0.3548\n",
      "Epoch [18/50], Train Loss: 1.0182, Train Accuracy: 0.4857, Test Accuracy: 0.3710\n",
      "Epoch [19/50], Train Loss: 1.0234, Train Accuracy: 0.4694, Test Accuracy: 0.4032\n",
      "Epoch [20/50], Train Loss: 1.0099, Train Accuracy: 0.4816, Test Accuracy: 0.3548\n",
      "Epoch [21/50], Train Loss: 0.9977, Train Accuracy: 0.4980, Test Accuracy: 0.3548\n",
      "Epoch [22/50], Train Loss: 1.0075, Train Accuracy: 0.5265, Test Accuracy: 0.3871\n",
      "Epoch [23/50], Train Loss: 0.9910, Train Accuracy: 0.5102, Test Accuracy: 0.4032\n",
      "Epoch [24/50], Train Loss: 0.9917, Train Accuracy: 0.5143, Test Accuracy: 0.4194\n",
      "Epoch [25/50], Train Loss: 1.0090, Train Accuracy: 0.5102, Test Accuracy: 0.4194\n",
      "Epoch [26/50], Train Loss: 1.0194, Train Accuracy: 0.4898, Test Accuracy: 0.4194\n",
      "Epoch [27/50], Train Loss: 0.9955, Train Accuracy: 0.5347, Test Accuracy: 0.4355\n",
      "Epoch [28/50], Train Loss: 0.9910, Train Accuracy: 0.5429, Test Accuracy: 0.4516\n",
      "Epoch [29/50], Train Loss: 0.9936, Train Accuracy: 0.5143, Test Accuracy: 0.4194\n",
      "Epoch [30/50], Train Loss: 0.9813, Train Accuracy: 0.5306, Test Accuracy: 0.4194\n",
      "Epoch [31/50], Train Loss: 0.9963, Train Accuracy: 0.5224, Test Accuracy: 0.4516\n",
      "Epoch [32/50], Train Loss: 0.9818, Train Accuracy: 0.5143, Test Accuracy: 0.4355\n",
      "Epoch [33/50], Train Loss: 0.9897, Train Accuracy: 0.5265, Test Accuracy: 0.4355\n",
      "Epoch [34/50], Train Loss: 0.9868, Train Accuracy: 0.5143, Test Accuracy: 0.4516\n",
      "Epoch [35/50], Train Loss: 0.9832, Train Accuracy: 0.5184, Test Accuracy: 0.4516\n",
      "Epoch [36/50], Train Loss: 0.9682, Train Accuracy: 0.5469, Test Accuracy: 0.4677\n",
      "Epoch [37/50], Train Loss: 0.9717, Train Accuracy: 0.5265, Test Accuracy: 0.4516\n",
      "Epoch [38/50], Train Loss: 0.9700, Train Accuracy: 0.5184, Test Accuracy: 0.4677\n",
      "Epoch [39/50], Train Loss: 0.9813, Train Accuracy: 0.5265, Test Accuracy: 0.4839\n",
      "Epoch [40/50], Train Loss: 0.9656, Train Accuracy: 0.5020, Test Accuracy: 0.4516\n",
      "Epoch [41/50], Train Loss: 0.9676, Train Accuracy: 0.5347, Test Accuracy: 0.4516\n",
      "Epoch [42/50], Train Loss: 0.9718, Train Accuracy: 0.5224, Test Accuracy: 0.4677\n",
      "Epoch [43/50], Train Loss: 0.9782, Train Accuracy: 0.5224, Test Accuracy: 0.4516\n",
      "Epoch [44/50], Train Loss: 0.9632, Train Accuracy: 0.5347, Test Accuracy: 0.4516\n",
      "Epoch [45/50], Train Loss: 0.9533, Train Accuracy: 0.5673, Test Accuracy: 0.4516\n",
      "Epoch [46/50], Train Loss: 0.9688, Train Accuracy: 0.5265, Test Accuracy: 0.4355\n",
      "Epoch [47/50], Train Loss: 0.9619, Train Accuracy: 0.5551, Test Accuracy: 0.4677\n",
      "Epoch [48/50], Train Loss: 0.9526, Train Accuracy: 0.5184, Test Accuracy: 0.4677\n",
      "Epoch [49/50], Train Loss: 0.9616, Train Accuracy: 0.5265, Test Accuracy: 0.4677\n",
      "Epoch [50/50], Train Loss: 0.9600, Train Accuracy: 0.5388, Test Accuracy: 0.4677\n",
      "Final Test Accuracy: 0.4677\n",
      "Epoch [1/50], Train Loss: 1.1066, Train Accuracy: 0.3049, Test Accuracy: 0.2787\n",
      "Epoch [2/50], Train Loss: 1.0936, Train Accuracy: 0.4065, Test Accuracy: 0.2787\n",
      "Epoch [3/50], Train Loss: 1.0774, Train Accuracy: 0.4065, Test Accuracy: 0.2787\n",
      "Epoch [4/50], Train Loss: 1.0764, Train Accuracy: 0.4228, Test Accuracy: 0.2951\n",
      "Epoch [5/50], Train Loss: 1.0727, Train Accuracy: 0.3862, Test Accuracy: 0.3279\n",
      "Epoch [6/50], Train Loss: 1.0648, Train Accuracy: 0.4431, Test Accuracy: 0.2787\n",
      "Epoch [7/50], Train Loss: 1.0691, Train Accuracy: 0.4268, Test Accuracy: 0.2787\n",
      "Epoch [8/50], Train Loss: 1.0643, Train Accuracy: 0.4106, Test Accuracy: 0.5082\n",
      "Epoch [9/50], Train Loss: 1.0608, Train Accuracy: 0.4553, Test Accuracy: 0.4590\n",
      "Epoch [10/50], Train Loss: 1.0523, Train Accuracy: 0.4472, Test Accuracy: 0.3115\n",
      "Epoch [11/50], Train Loss: 1.0606, Train Accuracy: 0.4187, Test Accuracy: 0.4754\n",
      "Epoch [12/50], Train Loss: 1.0423, Train Accuracy: 0.4593, Test Accuracy: 0.5082\n",
      "Epoch [13/50], Train Loss: 1.0464, Train Accuracy: 0.4431, Test Accuracy: 0.4262\n",
      "Epoch [14/50], Train Loss: 1.0283, Train Accuracy: 0.4593, Test Accuracy: 0.4590\n",
      "Epoch [15/50], Train Loss: 1.0417, Train Accuracy: 0.4472, Test Accuracy: 0.5082\n",
      "Epoch [16/50], Train Loss: 1.0210, Train Accuracy: 0.4878, Test Accuracy: 0.4918\n",
      "Epoch [17/50], Train Loss: 1.0196, Train Accuracy: 0.5163, Test Accuracy: 0.4754\n",
      "Epoch [18/50], Train Loss: 1.0205, Train Accuracy: 0.4553, Test Accuracy: 0.4918\n",
      "Epoch [19/50], Train Loss: 1.0115, Train Accuracy: 0.5041, Test Accuracy: 0.4754\n",
      "Epoch [20/50], Train Loss: 1.0199, Train Accuracy: 0.4553, Test Accuracy: 0.4754\n",
      "Epoch [21/50], Train Loss: 1.0614, Train Accuracy: 0.4593, Test Accuracy: 0.5246\n",
      "Epoch [22/50], Train Loss: 1.0336, Train Accuracy: 0.4512, Test Accuracy: 0.4918\n",
      "Epoch [23/50], Train Loss: 1.0173, Train Accuracy: 0.5000, Test Accuracy: 0.6557\n",
      "Epoch [24/50], Train Loss: 1.0165, Train Accuracy: 0.4959, Test Accuracy: 0.4590\n",
      "Epoch [25/50], Train Loss: 1.0065, Train Accuracy: 0.4512, Test Accuracy: 0.4754\n",
      "Epoch [26/50], Train Loss: 0.9914, Train Accuracy: 0.4919, Test Accuracy: 0.5410\n",
      "Epoch [27/50], Train Loss: 0.9941, Train Accuracy: 0.4878, Test Accuracy: 0.5246\n",
      "Epoch [28/50], Train Loss: 0.9948, Train Accuracy: 0.4959, Test Accuracy: 0.5410\n",
      "Epoch [29/50], Train Loss: 0.9973, Train Accuracy: 0.5122, Test Accuracy: 0.5574\n",
      "Epoch [30/50], Train Loss: 1.0023, Train Accuracy: 0.4756, Test Accuracy: 0.5246\n",
      "Epoch [31/50], Train Loss: 0.9781, Train Accuracy: 0.5325, Test Accuracy: 0.5902\n",
      "Epoch [32/50], Train Loss: 0.9993, Train Accuracy: 0.4837, Test Accuracy: 0.4918\n",
      "Epoch [33/50], Train Loss: 0.9948, Train Accuracy: 0.5000, Test Accuracy: 0.6230\n",
      "Epoch [34/50], Train Loss: 0.9630, Train Accuracy: 0.5366, Test Accuracy: 0.5738\n",
      "Epoch [35/50], Train Loss: 0.9988, Train Accuracy: 0.4878, Test Accuracy: 0.5902\n",
      "Epoch [36/50], Train Loss: 0.9960, Train Accuracy: 0.5000, Test Accuracy: 0.5738\n",
      "Epoch [37/50], Train Loss: 0.9891, Train Accuracy: 0.5285, Test Accuracy: 0.5902\n",
      "Epoch [38/50], Train Loss: 0.9827, Train Accuracy: 0.5407, Test Accuracy: 0.5574\n",
      "Epoch [39/50], Train Loss: 0.9734, Train Accuracy: 0.5163, Test Accuracy: 0.5410\n",
      "Epoch [40/50], Train Loss: 0.9731, Train Accuracy: 0.5325, Test Accuracy: 0.5246\n",
      "Epoch [41/50], Train Loss: 0.9859, Train Accuracy: 0.5000, Test Accuracy: 0.5410\n",
      "Epoch [42/50], Train Loss: 0.9933, Train Accuracy: 0.5122, Test Accuracy: 0.5902\n",
      "Epoch [43/50], Train Loss: 0.9846, Train Accuracy: 0.4837, Test Accuracy: 0.5246\n",
      "Epoch [44/50], Train Loss: 0.9858, Train Accuracy: 0.5163, Test Accuracy: 0.5738\n",
      "Epoch [45/50], Train Loss: 0.9841, Train Accuracy: 0.5163, Test Accuracy: 0.5410\n",
      "Epoch [46/50], Train Loss: 0.9754, Train Accuracy: 0.5325, Test Accuracy: 0.5246\n",
      "Epoch [47/50], Train Loss: 0.9910, Train Accuracy: 0.4837, Test Accuracy: 0.5902\n",
      "Epoch [48/50], Train Loss: 0.9853, Train Accuracy: 0.5366, Test Accuracy: 0.5246\n",
      "Epoch [49/50], Train Loss: 0.9888, Train Accuracy: 0.5081, Test Accuracy: 0.5902\n",
      "Epoch [50/50], Train Loss: 0.9864, Train Accuracy: 0.5244, Test Accuracy: 0.5574\n",
      "Final Test Accuracy: 0.5574\n",
      "Epoch [1/50], Train Loss: 1.2203, Train Accuracy: 0.3415, Test Accuracy: 0.4098\n",
      "Epoch [2/50], Train Loss: 1.1157, Train Accuracy: 0.3171, Test Accuracy: 0.2295\n",
      "Epoch [3/50], Train Loss: 1.1239, Train Accuracy: 0.2683, Test Accuracy: 0.3443\n",
      "Epoch [4/50], Train Loss: 1.0997, Train Accuracy: 0.3171, Test Accuracy: 0.4098\n",
      "Epoch [5/50], Train Loss: 1.1020, Train Accuracy: 0.3699, Test Accuracy: 0.4098\n",
      "Epoch [6/50], Train Loss: 1.1025, Train Accuracy: 0.3659, Test Accuracy: 0.4098\n",
      "Epoch [7/50], Train Loss: 1.1069, Train Accuracy: 0.3618, Test Accuracy: 0.4098\n",
      "Epoch [8/50], Train Loss: 1.0983, Train Accuracy: 0.3659, Test Accuracy: 0.4098\n",
      "Epoch [9/50], Train Loss: 1.0852, Train Accuracy: 0.3862, Test Accuracy: 0.4098\n",
      "Epoch [10/50], Train Loss: 1.0964, Train Accuracy: 0.3740, Test Accuracy: 0.4098\n",
      "Epoch [11/50], Train Loss: 1.0863, Train Accuracy: 0.3780, Test Accuracy: 0.4098\n",
      "Epoch [12/50], Train Loss: 1.0867, Train Accuracy: 0.3943, Test Accuracy: 0.4098\n",
      "Epoch [13/50], Train Loss: 1.0827, Train Accuracy: 0.4146, Test Accuracy: 0.4098\n",
      "Epoch [14/50], Train Loss: 1.0799, Train Accuracy: 0.4268, Test Accuracy: 0.4098\n",
      "Epoch [15/50], Train Loss: 1.0722, Train Accuracy: 0.4228, Test Accuracy: 0.4918\n",
      "Epoch [16/50], Train Loss: 1.0592, Train Accuracy: 0.4390, Test Accuracy: 0.4426\n",
      "Epoch [17/50], Train Loss: 1.0515, Train Accuracy: 0.4350, Test Accuracy: 0.3934\n",
      "Epoch [18/50], Train Loss: 1.0428, Train Accuracy: 0.4512, Test Accuracy: 0.4590\n",
      "Epoch [19/50], Train Loss: 1.0308, Train Accuracy: 0.4837, Test Accuracy: 0.3607\n",
      "Epoch [20/50], Train Loss: 1.0181, Train Accuracy: 0.5081, Test Accuracy: 0.4426\n",
      "Epoch [21/50], Train Loss: 1.0358, Train Accuracy: 0.4797, Test Accuracy: 0.4590\n",
      "Epoch [22/50], Train Loss: 1.0034, Train Accuracy: 0.5203, Test Accuracy: 0.4918\n",
      "Epoch [23/50], Train Loss: 1.0032, Train Accuracy: 0.5122, Test Accuracy: 0.4590\n",
      "Epoch [24/50], Train Loss: 1.0015, Train Accuracy: 0.5122, Test Accuracy: 0.5246\n",
      "Epoch [25/50], Train Loss: 0.9890, Train Accuracy: 0.5488, Test Accuracy: 0.4918\n",
      "Epoch [26/50], Train Loss: 0.9931, Train Accuracy: 0.4919, Test Accuracy: 0.5082\n",
      "Epoch [27/50], Train Loss: 1.0017, Train Accuracy: 0.5244, Test Accuracy: 0.4918\n",
      "Epoch [28/50], Train Loss: 0.9869, Train Accuracy: 0.5122, Test Accuracy: 0.5082\n",
      "Epoch [29/50], Train Loss: 0.9844, Train Accuracy: 0.5122, Test Accuracy: 0.5082\n",
      "Epoch [30/50], Train Loss: 0.9862, Train Accuracy: 0.5285, Test Accuracy: 0.4918\n",
      "Epoch [31/50], Train Loss: 0.9765, Train Accuracy: 0.5163, Test Accuracy: 0.4590\n",
      "Epoch [32/50], Train Loss: 0.9788, Train Accuracy: 0.4959, Test Accuracy: 0.4918\n",
      "Epoch [33/50], Train Loss: 0.9739, Train Accuracy: 0.5447, Test Accuracy: 0.5082\n",
      "Epoch [34/50], Train Loss: 0.9653, Train Accuracy: 0.5528, Test Accuracy: 0.5082\n",
      "Epoch [35/50], Train Loss: 0.9553, Train Accuracy: 0.5528, Test Accuracy: 0.4918\n",
      "Epoch [36/50], Train Loss: 0.9597, Train Accuracy: 0.5163, Test Accuracy: 0.5082\n",
      "Epoch [37/50], Train Loss: 0.9602, Train Accuracy: 0.5407, Test Accuracy: 0.5082\n",
      "Epoch [38/50], Train Loss: 0.9680, Train Accuracy: 0.5203, Test Accuracy: 0.5082\n",
      "Epoch [39/50], Train Loss: 0.9549, Train Accuracy: 0.5407, Test Accuracy: 0.4918\n",
      "Epoch [40/50], Train Loss: 0.9626, Train Accuracy: 0.5163, Test Accuracy: 0.5082\n",
      "Epoch [41/50], Train Loss: 0.9436, Train Accuracy: 0.5488, Test Accuracy: 0.5082\n",
      "Epoch [42/50], Train Loss: 0.9672, Train Accuracy: 0.5244, Test Accuracy: 0.4918\n",
      "Epoch [43/50], Train Loss: 0.9519, Train Accuracy: 0.5366, Test Accuracy: 0.4918\n",
      "Epoch [44/50], Train Loss: 0.9641, Train Accuracy: 0.5528, Test Accuracy: 0.4754\n",
      "Epoch [45/50], Train Loss: 0.9632, Train Accuracy: 0.5325, Test Accuracy: 0.5246\n",
      "Epoch [46/50], Train Loss: 0.9447, Train Accuracy: 0.5569, Test Accuracy: 0.4754\n",
      "Epoch [47/50], Train Loss: 0.9374, Train Accuracy: 0.5407, Test Accuracy: 0.4918\n",
      "Epoch [48/50], Train Loss: 0.9513, Train Accuracy: 0.5488, Test Accuracy: 0.4918\n",
      "Epoch [49/50], Train Loss: 0.9254, Train Accuracy: 0.5447, Test Accuracy: 0.5246\n",
      "Epoch [50/50], Train Loss: 0.9358, Train Accuracy: 0.5610, Test Accuracy: 0.4754\n",
      "Final Test Accuracy: 0.4754\n",
      "Epoch [1/50], Train Loss: 1.1045, Train Accuracy: 0.3374, Test Accuracy: 0.5410\n",
      "Epoch [2/50], Train Loss: 1.0783, Train Accuracy: 0.4106, Test Accuracy: 0.5574\n",
      "Epoch [3/50], Train Loss: 1.0794, Train Accuracy: 0.4187, Test Accuracy: 0.5246\n",
      "Epoch [4/50], Train Loss: 1.0737, Train Accuracy: 0.4350, Test Accuracy: 0.5574\n",
      "Epoch [5/50], Train Loss: 1.0672, Train Accuracy: 0.4350, Test Accuracy: 0.5574\n",
      "Epoch [6/50], Train Loss: 1.0560, Train Accuracy: 0.4472, Test Accuracy: 0.5738\n",
      "Epoch [7/50], Train Loss: 1.0537, Train Accuracy: 0.4390, Test Accuracy: 0.5574\n",
      "Epoch [8/50], Train Loss: 1.0502, Train Accuracy: 0.4512, Test Accuracy: 0.5738\n",
      "Epoch [9/50], Train Loss: 1.0467, Train Accuracy: 0.4309, Test Accuracy: 0.5738\n",
      "Epoch [10/50], Train Loss: 1.0425, Train Accuracy: 0.4390, Test Accuracy: 0.5738\n",
      "Epoch [11/50], Train Loss: 1.0321, Train Accuracy: 0.4431, Test Accuracy: 0.6066\n",
      "Epoch [12/50], Train Loss: 1.0316, Train Accuracy: 0.4472, Test Accuracy: 0.5738\n",
      "Epoch [13/50], Train Loss: 1.0320, Train Accuracy: 0.4390, Test Accuracy: 0.6066\n",
      "Epoch [14/50], Train Loss: 1.0270, Train Accuracy: 0.4715, Test Accuracy: 0.6230\n",
      "Epoch [15/50], Train Loss: 1.0252, Train Accuracy: 0.5000, Test Accuracy: 0.5574\n",
      "Epoch [16/50], Train Loss: 1.0189, Train Accuracy: 0.4472, Test Accuracy: 0.5902\n",
      "Epoch [17/50], Train Loss: 1.0277, Train Accuracy: 0.4512, Test Accuracy: 0.5738\n",
      "Epoch [18/50], Train Loss: 1.0314, Train Accuracy: 0.4797, Test Accuracy: 0.5574\n",
      "Epoch [19/50], Train Loss: 1.0117, Train Accuracy: 0.4431, Test Accuracy: 0.5574\n",
      "Epoch [20/50], Train Loss: 1.0211, Train Accuracy: 0.4512, Test Accuracy: 0.6066\n",
      "Epoch [21/50], Train Loss: 1.0149, Train Accuracy: 0.4512, Test Accuracy: 0.5738\n",
      "Epoch [22/50], Train Loss: 1.0059, Train Accuracy: 0.4959, Test Accuracy: 0.6230\n",
      "Epoch [23/50], Train Loss: 1.0032, Train Accuracy: 0.4837, Test Accuracy: 0.6066\n",
      "Epoch [24/50], Train Loss: 1.0203, Train Accuracy: 0.4350, Test Accuracy: 0.5410\n",
      "Epoch [25/50], Train Loss: 1.0138, Train Accuracy: 0.4593, Test Accuracy: 0.6066\n",
      "Epoch [26/50], Train Loss: 1.0038, Train Accuracy: 0.4634, Test Accuracy: 0.5574\n",
      "Epoch [27/50], Train Loss: 1.0071, Train Accuracy: 0.4837, Test Accuracy: 0.5738\n",
      "Epoch [28/50], Train Loss: 0.9915, Train Accuracy: 0.4715, Test Accuracy: 0.5902\n",
      "Epoch [29/50], Train Loss: 0.9889, Train Accuracy: 0.5041, Test Accuracy: 0.5410\n",
      "Epoch [30/50], Train Loss: 0.9979, Train Accuracy: 0.4919, Test Accuracy: 0.6230\n",
      "Epoch [31/50], Train Loss: 1.0013, Train Accuracy: 0.4512, Test Accuracy: 0.5902\n",
      "Epoch [32/50], Train Loss: 0.9857, Train Accuracy: 0.5041, Test Accuracy: 0.5902\n",
      "Epoch [33/50], Train Loss: 0.9971, Train Accuracy: 0.5122, Test Accuracy: 0.5574\n",
      "Epoch [34/50], Train Loss: 0.9890, Train Accuracy: 0.4837, Test Accuracy: 0.6066\n",
      "Epoch [35/50], Train Loss: 0.9941, Train Accuracy: 0.5122, Test Accuracy: 0.5410\n",
      "Epoch [36/50], Train Loss: 0.9913, Train Accuracy: 0.4675, Test Accuracy: 0.5738\n",
      "Epoch [37/50], Train Loss: 0.9988, Train Accuracy: 0.4797, Test Accuracy: 0.5574\n",
      "Epoch [38/50], Train Loss: 0.9814, Train Accuracy: 0.5081, Test Accuracy: 0.5738\n",
      "Epoch [39/50], Train Loss: 0.9802, Train Accuracy: 0.5122, Test Accuracy: 0.5902\n",
      "Epoch [40/50], Train Loss: 0.9926, Train Accuracy: 0.4878, Test Accuracy: 0.5410\n",
      "Epoch [41/50], Train Loss: 0.9814, Train Accuracy: 0.5122, Test Accuracy: 0.6066\n",
      "Epoch [42/50], Train Loss: 0.9944, Train Accuracy: 0.4553, Test Accuracy: 0.5738\n",
      "Epoch [43/50], Train Loss: 0.9884, Train Accuracy: 0.5000, Test Accuracy: 0.6230\n",
      "Epoch [44/50], Train Loss: 0.9895, Train Accuracy: 0.5163, Test Accuracy: 0.5738\n",
      "Epoch [45/50], Train Loss: 0.9827, Train Accuracy: 0.5203, Test Accuracy: 0.5738\n",
      "Epoch [46/50], Train Loss: 0.9798, Train Accuracy: 0.5163, Test Accuracy: 0.5902\n",
      "Epoch [47/50], Train Loss: 0.9708, Train Accuracy: 0.5285, Test Accuracy: 0.5902\n",
      "Epoch [48/50], Train Loss: 0.9808, Train Accuracy: 0.5000, Test Accuracy: 0.5410\n",
      "Epoch [49/50], Train Loss: 0.9700, Train Accuracy: 0.5163, Test Accuracy: 0.5738\n",
      "Epoch [50/50], Train Loss: 0.9793, Train Accuracy: 0.5163, Test Accuracy: 0.5902\n",
      "Final Test Accuracy: 0.5902\n",
      "Mean Accuracy: 0.5085 ± 0.0549\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=25)\n",
    "accuracies = []\n",
    "for train_index, test_index in kf.split(X_balanced):\n",
    "    X_train, X_test = [X_balanced[i] for i in train_index], [X_balanced[i] for i in test_index]\n",
    "    y_train, y_test = [y_balanced[i] for i in train_index], [y_balanced[i] for i in test_index]\n",
    "    X_train_normalized, X_test_normalized = normalize_features(X_train, X_test)\n",
    "    accuracies.append(train_model(X_train_normalized, y_train, X_test_normalized, y_test))\n",
    "print(f'Mean Accuracy: {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gene",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
