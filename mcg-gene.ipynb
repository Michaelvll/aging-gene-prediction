{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('data/Oligo_NN.RNA_DEG.csv')\n",
    "df.set_index('gene', inplace=True)\n",
    "df.head()\n",
    "\n",
    "# non_zero_genes = df[df['DEG'] != 0].index\n",
    "\n",
    "# df = df[df.index.isin(non_zero_genes)]\n",
    "df\n",
    "FEATURE_NAMES = ['2mo', '9mo', '18mo', '9mo-2mo', '18mo-9mo', '9mo/2mo', '18mo/9mo', 'old-young', 'old/young', 'distance']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene</th>\n",
       "      <th>2mo</th>\n",
       "      <th>9mo</th>\n",
       "      <th>18mo</th>\n",
       "      <th>9mo-2mo</th>\n",
       "      <th>18mo-9mo</th>\n",
       "      <th>9mo/2mo</th>\n",
       "      <th>18mo/9mo</th>\n",
       "      <th>old-young</th>\n",
       "      <th>old/young</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rgs20</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.90</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.384615</td>\n",
       "      <td>151241.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sulf1</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.444444</td>\n",
       "      <td>1.076923</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.555556</td>\n",
       "      <td>121205.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sulf1</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.372093</td>\n",
       "      <td>1.084746</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.488372</td>\n",
       "      <td>170142.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Eya1</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.47</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.758065</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.691176</td>\n",
       "      <td>137980.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eya1</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.606557</td>\n",
       "      <td>1.216216</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.737705</td>\n",
       "      <td>138254.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    gene   2mo   9mo  18mo  9mo-2mo  18mo-9mo   9mo/2mo  18mo/9mo  old-young  \\\n",
       "0  Rgs20  0.65  0.60  0.90    -0.05      0.30  0.923077  1.500000       0.25   \n",
       "1  Sulf1  0.36  0.52  0.56     0.16      0.04  1.444444  1.076923       0.20   \n",
       "2  Sulf1  0.43  0.59  0.64     0.16      0.05  1.372093  1.084746       0.21   \n",
       "3   Eya1  0.68  0.62  0.47    -0.06     -0.15  0.911765  0.758065      -0.21   \n",
       "4   Eya1  0.61  0.37  0.45    -0.24      0.08  0.606557  1.216216      -0.16   \n",
       "\n",
       "   old/young  distance  \n",
       "0   1.384615  151241.0  \n",
       "1   1.555556  121205.0  \n",
       "2   1.488372  170142.0  \n",
       "3   0.691176  137980.0  \n",
       "4   0.737705  138254.0  "
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene2value = df[['DEG']]\n",
    "\n",
    "\n",
    "mcg = pd.read_csv('data/Oligo_NN.aDMR_gene.csv')\n",
    "mcg_feat = mcg\n",
    "mcg_feat.rename(columns={'gene_name': 'gene'}, inplace=True)\n",
    "mcg_feat['distance'] = (mcg_feat['gene_start'] - mcg_feat['start']).abs().astype(np.float64)\n",
    "mcg_feat['old/young'] = mcg_feat['18mo'] / mcg_feat['2mo']\n",
    "mcg_feat['9mo-2mo'] = mcg_feat['9mo'] - mcg_feat['2mo']\n",
    "mcg_feat['18mo-9mo'] = mcg_feat['18mo'] - mcg_feat['9mo']\n",
    "mcg_feat['9mo/2mo'] = mcg_feat['9mo'] / mcg_feat['2mo']\n",
    "mcg_feat['18mo/9mo'] = mcg_feat['18mo'] / mcg_feat['9mo']\n",
    "mcg_feat = mcg_feat[['gene', *FEATURE_NAMES]]\n",
    "mcg_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2321"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_genes = set(gene2value.index) & set(mcg_feat['gene'])\n",
    "len(shared_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene</th>\n",
       "      <th>2mo</th>\n",
       "      <th>9mo</th>\n",
       "      <th>18mo</th>\n",
       "      <th>9mo-2mo</th>\n",
       "      <th>18mo-9mo</th>\n",
       "      <th>9mo/2mo</th>\n",
       "      <th>18mo/9mo</th>\n",
       "      <th>old-young</th>\n",
       "      <th>old/young</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rgs20</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.90</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.384615</td>\n",
       "      <td>151241.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sulf1</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.444444</td>\n",
       "      <td>1.076923</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.555556</td>\n",
       "      <td>121205.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sulf1</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.372093</td>\n",
       "      <td>1.084746</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.488372</td>\n",
       "      <td>170142.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Eya1</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.47</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.758065</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.691176</td>\n",
       "      <td>137980.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eya1</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.606557</td>\n",
       "      <td>1.216216</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.737705</td>\n",
       "      <td>138254.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    gene   2mo   9mo  18mo  9mo-2mo  18mo-9mo   9mo/2mo  18mo/9mo  old-young  \\\n",
       "0  Rgs20  0.65  0.60  0.90    -0.05      0.30  0.923077  1.500000       0.25   \n",
       "1  Sulf1  0.36  0.52  0.56     0.16      0.04  1.444444  1.076923       0.20   \n",
       "2  Sulf1  0.43  0.59  0.64     0.16      0.05  1.372093  1.084746       0.21   \n",
       "3   Eya1  0.68  0.62  0.47    -0.06     -0.15  0.911765  0.758065      -0.21   \n",
       "4   Eya1  0.61  0.37  0.45    -0.24      0.08  0.606557  1.216216      -0.16   \n",
       "\n",
       "   old/young  distance  \n",
       "0   1.384615  151241.0  \n",
       "1   1.555556  121205.0  \n",
       "2   1.488372  170142.0  \n",
       "3   0.691176  137980.0  \n",
       "4   0.737705  138254.0  "
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene2value = gene2value[gene2value.index.isin(shared_genes)]\n",
    "mcg_feat = mcg_feat[mcg_feat['gene'].isin(shared_genes)]\n",
    "\n",
    "mcg_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2mo</th>\n",
       "      <th>9mo</th>\n",
       "      <th>18mo</th>\n",
       "      <th>9mo-2mo</th>\n",
       "      <th>18mo-9mo</th>\n",
       "      <th>9mo/2mo</th>\n",
       "      <th>18mo/9mo</th>\n",
       "      <th>old-young</th>\n",
       "      <th>old/young</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gene</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Rgs20</th>\n",
       "      <td>0.650</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.90</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1.384615</td>\n",
       "      <td>151241.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sulf1</th>\n",
       "      <td>0.395</td>\n",
       "      <td>0.555000</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>1.408269</td>\n",
       "      <td>1.080834</td>\n",
       "      <td>0.205</td>\n",
       "      <td>1.521964</td>\n",
       "      <td>145673.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eya1</th>\n",
       "      <td>0.630</td>\n",
       "      <td>0.453333</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-0.176667</td>\n",
       "      <td>-0.003333</td>\n",
       "      <td>0.711663</td>\n",
       "      <td>1.045481</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>0.715183</td>\n",
       "      <td>138168.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stau2</th>\n",
       "      <td>0.370</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>1.540541</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.150</td>\n",
       "      <td>1.405405</td>\n",
       "      <td>23034.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ube2w</th>\n",
       "      <td>0.490</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>1.265306</td>\n",
       "      <td>1.129032</td>\n",
       "      <td>0.210</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>28480.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         2mo       9mo  18mo   9mo-2mo  18mo-9mo   9mo/2mo  18mo/9mo  \\\n",
       "gene                                                                   \n",
       "Rgs20  0.650  0.600000  0.90 -0.050000  0.300000  0.923077  1.500000   \n",
       "Sulf1  0.395  0.555000  0.60  0.160000  0.045000  1.408269  1.080834   \n",
       "Eya1   0.630  0.453333  0.45 -0.176667 -0.003333  0.711663  1.045481   \n",
       "Stau2  0.370  0.570000  0.52  0.200000 -0.050000  1.540541  0.912281   \n",
       "Ube2w  0.490  0.620000  0.70  0.130000  0.080000  1.265306  1.129032   \n",
       "\n",
       "       old-young  old/young       distance  \n",
       "gene                                        \n",
       "Rgs20      0.250   1.384615  151241.000000  \n",
       "Sulf1      0.205   1.521964  145673.500000  \n",
       "Eya1      -0.180   0.715183  138168.666667  \n",
       "Stau2      0.150   1.405405   23034.000000  \n",
       "Ube2w      0.210   1.428571   28480.000000  "
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcg_mean = mcg_feat.groupby('gene').mean()\n",
    "mcg_mean.head()\n",
    "# Sort mcg_mean by gene name\n",
    "mcg_mean = mcg_mean.loc[gene2value.index]\n",
    "mcg_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gene/lib/python3.10/site-packages/numpy/lib/_function_base_impl.py:2842: RuntimeWarning: invalid value encountered in subtract\n",
      "  X -= avg[:, None]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2mo          0.034833\n",
       "9mo         -0.016470\n",
       "18mo        -0.038960\n",
       "9mo-2mo     -0.046858\n",
       "18mo-9mo    -0.060399\n",
       "9mo/2mo           NaN\n",
       "18mo/9mo          NaN\n",
       "old-young   -0.060147\n",
       "old/young         NaN\n",
       "distance     0.101508\n",
       "dtype: float64"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcg_mean.corrwith(gene2value['DEG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_2659864/1269483849.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  list_mcg_feat = mcg_feat.groupby('gene').apply(lambda x: x[FEATURE_NAMES].values.tolist())\n"
     ]
    }
   ],
   "source": [
    "# Train a sequence model on mcg_feat to predict gene2value['log2(old/young)']\n",
    "# Each gene has a sequence of 4 features, 2mo, 9mo, 18mo, old-young\n",
    "# The sequence length is not fixed, so we need to use a dynamic model\n",
    "# Let's use a commonly used sequence prediction model for sentence classification\n",
    "# like LSTM or Transformer\n",
    "\n",
    "# Step 1: Prepare the data\n",
    "list_mcg_feat = mcg_feat.groupby('gene').apply(lambda x: x[FEATURE_NAMES].values.tolist())\n",
    "x = list_mcg_feat.values.tolist()\n",
    "\n",
    "# # Find the maximum length and number of features\n",
    "# max_len = max(len(seq) for seq in x)\n",
    "# n_features = len(x[0][0])\n",
    "\n",
    "# # Pad sequences to the same length and reshape\n",
    "# x_padded = np.full((len(x), max_len, n_features), 0)\n",
    "# for i, seq in enumerate(x):\n",
    "#     x_padded[i, :len(seq), :] = seq\n",
    "\n",
    "# # x_padded is now a 3D numpy array with shape (n_samples, max_len, n_features)\n",
    "# print(x_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = gene2value.loc[list_mcg_feat.index]['DEG'].values.tolist()\n",
    "y = np.array([int(i) for i in y])\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero: 2116, non-zero: 205\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Separate the data into zero and non-zero y values\n",
    "zero_indices = np.where(y == 0)[0]\n",
    "non_zero_indices = np.where(y != 0)[0]\n",
    "print(f'zero: {len(zero_indices)}, non-zero: {len(non_zero_indices)}')\n",
    "\n",
    "# Sample len(non_zero_indices) indices from each group\n",
    "n_samples = len(non_zero_indices)\n",
    "sampled_zero_indices = np.random.choice(zero_indices, n_samples // 2, replace=False)\n",
    "sampled_non_zero_indices = np.random.choice(non_zero_indices, n_samples, replace=False)\n",
    "\n",
    "# Combine the sampled indices\n",
    "sampled_indices = np.concatenate([sampled_zero_indices, sampled_non_zero_indices])\n",
    "\n",
    "# Create balanced dataset\n",
    "X_balanced = [x[i] for i in sampled_indices]\n",
    "y_balanced = y[sampled_indices]\n",
    "\n",
    "# Split the balanced dataset into training and testing sets\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 245 test: 62\n"
     ]
    }
   ],
   "source": [
    "# Normalize the features\n",
    "\n",
    "# Normalization function\n",
    "def normalize_features(train_data, test_data):\n",
    "    # Flatten the lists for easier processing\n",
    "    train_flat = [item for sublist in train_data for item in sublist]\n",
    "    test_flat = [item for sublist in test_data for item in sublist]\n",
    "    \n",
    "    # Separate features\n",
    "    train_other_features = np.array([item[:len(FEATURE_NAMES)-1] for item in train_flat])\n",
    "    train_distances = np.array([item[len(FEATURE_NAMES)-1] for item in train_flat])\n",
    "    test_other_features = np.array([item[:len(FEATURE_NAMES)-1] for item in test_flat])\n",
    "    test_distances = np.array([item[len(FEATURE_NAMES)-1] for item in test_flat])\n",
    "    \n",
    "    # Normalize other features using min-max scaling based on train data\n",
    "    min_vals = np.min(train_other_features, axis=0)\n",
    "    max_vals = np.max(train_other_features, axis=0)\n",
    "    train_normalized_features = (train_other_features - min_vals) / (max_vals - min_vals)\n",
    "    test_normalized_features = (test_other_features - min_vals) / (max_vals - min_vals)\n",
    "    \n",
    "    # Normalize distances using log transformation and then min-max scaling based on train data\n",
    "    train_log_distances = np.log1p(train_distances)\n",
    "    test_log_distances = np.log1p(test_distances)\n",
    "    min_dist = np.min(train_log_distances)\n",
    "    max_dist = np.max(train_log_distances)\n",
    "    train_normalized_distances = (train_log_distances - min_dist) / (max_dist - min_dist)\n",
    "    test_normalized_distances = (test_log_distances - min_dist) / (max_dist - min_dist)\n",
    "    \n",
    "    # Combine normalized features and distances\n",
    "    def reconstruct_data(features, distances, original_data):\n",
    "        normalized_data = []\n",
    "        idx = 0\n",
    "        for sublist in original_data:\n",
    "            normalized_sublist = []\n",
    "            for _ in sublist:\n",
    "                normalized_sublist.append(list(features[idx][:len(FEATURE_NAMES)-1]) + [distances[idx]])\n",
    "                idx += 1\n",
    "            normalized_data.append(normalized_sublist)\n",
    "        return normalized_data\n",
    "    \n",
    "    train_normalized = reconstruct_data(train_normalized_features, train_normalized_distances, train_data)\n",
    "    test_normalized = reconstruct_data(test_normalized_features, test_normalized_distances, test_data)\n",
    "    \n",
    "    return train_normalized, test_normalized\n",
    "\n",
    "# Apply normalization\n",
    "X_train_normalized, X_test_normalized = normalize_features(X_train_raw, X_test_raw)\n",
    "\n",
    "print('train:', len(X_train_normalized), 'test:', len(X_test_normalized))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, -1,  1,  1,  0,  0,  1,  1, -1, -1])"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_raw[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.1344, Test Acc: 0.3710\n",
      "Epoch 2, Train Loss: 1.1059, Test Acc: 0.3548\n",
      "Epoch 3, Train Loss: 1.1107, Test Acc: 0.3548\n",
      "Epoch 4, Train Loss: 1.1070, Test Acc: 0.3548\n",
      "Epoch 5, Train Loss: 1.0900, Test Acc: 0.3548\n",
      "Epoch 6, Train Loss: 1.0916, Test Acc: 0.3548\n",
      "Epoch 7, Train Loss: 1.0933, Test Acc: 0.3548\n",
      "Epoch 8, Train Loss: 1.0997, Test Acc: 0.3710\n",
      "Epoch 9, Train Loss: 1.0936, Test Acc: 0.3548\n",
      "Epoch 10, Train Loss: 1.0962, Test Acc: 0.3548\n",
      "Epoch 11, Train Loss: 1.0884, Test Acc: 0.3548\n",
      "Epoch 12, Train Loss: 1.0926, Test Acc: 0.3548\n",
      "Epoch 13, Train Loss: 1.0909, Test Acc: 0.3548\n",
      "Epoch 14, Train Loss: 1.0991, Test Acc: 0.3548\n",
      "Epoch 15, Train Loss: 1.0923, Test Acc: 0.3548\n",
      "Epoch 16, Train Loss: 1.0914, Test Acc: 0.3548\n",
      "Epoch 17, Train Loss: 1.0888, Test Acc: 0.3548\n",
      "Epoch 18, Train Loss: 1.0935, Test Acc: 0.3548\n",
      "Epoch 19, Train Loss: 1.0884, Test Acc: 0.3548\n",
      "Epoch 20, Train Loss: 1.0899, Test Acc: 0.3548\n",
      "Epoch 21, Train Loss: 1.1088, Test Acc: 0.3548\n",
      "Epoch 22, Train Loss: 1.0863, Test Acc: 0.3548\n",
      "Epoch 23, Train Loss: 1.0912, Test Acc: 0.3548\n",
      "Epoch 24, Train Loss: 1.0948, Test Acc: 0.3548\n",
      "Epoch 25, Train Loss: 1.0892, Test Acc: 0.3548\n",
      "Epoch 26, Train Loss: 1.0690, Test Acc: 0.3548\n",
      "Epoch 27, Train Loss: 1.0829, Test Acc: 0.3548\n",
      "Epoch 28, Train Loss: 1.0812, Test Acc: 0.4677\n",
      "Epoch 29, Train Loss: 1.0689, Test Acc: 0.4677\n",
      "Epoch 30, Train Loss: 1.0585, Test Acc: 0.3548\n",
      "Epoch 31, Train Loss: 1.0439, Test Acc: 0.4677\n",
      "Epoch 32, Train Loss: 1.0599, Test Acc: 0.4677\n",
      "Epoch 33, Train Loss: 1.0566, Test Acc: 0.4677\n",
      "Epoch 34, Train Loss: 1.0482, Test Acc: 0.4677\n",
      "Epoch 35, Train Loss: 1.0393, Test Acc: 0.4677\n",
      "Epoch 36, Train Loss: 1.0715, Test Acc: 0.3710\n",
      "Epoch 37, Train Loss: 1.0553, Test Acc: 0.4677\n",
      "Epoch 38, Train Loss: 1.0820, Test Acc: 0.3710\n",
      "Epoch 39, Train Loss: 1.0732, Test Acc: 0.4677\n",
      "Epoch 40, Train Loss: 1.0578, Test Acc: 0.4677\n",
      "Epoch 41, Train Loss: 1.0641, Test Acc: 0.3548\n",
      "Epoch 42, Train Loss: 1.0454, Test Acc: 0.4677\n",
      "Epoch 43, Train Loss: 1.0566, Test Acc: 0.4677\n",
      "Epoch 44, Train Loss: 1.0689, Test Acc: 0.3710\n",
      "Epoch 45, Train Loss: 1.0705, Test Acc: 0.4677\n",
      "Epoch 46, Train Loss: 1.0666, Test Acc: 0.3548\n",
      "Epoch 47, Train Loss: 1.0522, Test Acc: 0.3710\n",
      "Epoch 48, Train Loss: 1.0846, Test Acc: 0.4677\n",
      "Epoch 49, Train Loss: 1.0519, Test Acc: 0.4677\n",
      "Epoch 50, Train Loss: 1.0665, Test Acc: 0.4677\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Train a sequence model on the training set\n",
    "# Use pytorch to train a LSTM classifier\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model = LSTMClassifier(input_size=len(FEATURE_NAMES), hidden_size=16, num_classes=3)\n",
    "# Initialize the weights\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "# Pad sequences to the same length\n",
    "def pad_sequences(sequences):\n",
    "    return pad_sequence([torch.FloatTensor(seq) for seq in sequences], batch_first=True)\n",
    "\n",
    "X_train = pad_sequences(X_train_normalized)\n",
    "X_test = pad_sequences(X_test_normalized)\n",
    "y_train = torch.tensor([int(i) + 1 for i in y_train_raw], dtype=torch.long)\n",
    "y_test = torch.tensor([int(i) + 1 for i in y_test_raw], dtype=torch.long)\n",
    "\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Train the model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test)\n",
    "        predictions = test_outputs.argmax(dim=1)\n",
    "        test_acc = (test_outputs.argmax(dim=1) == y_test).float().mean()\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {total_loss/len(train_loader):.4f}, Test Acc: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 1.1156, Train Accuracy: 0.3020, Test Accuracy: 0.3387\n",
      "Epoch [2/50], Train Loss: 1.0943, Train Accuracy: 0.3633, Test Accuracy: 0.3548\n",
      "Epoch [3/50], Train Loss: 1.0902, Train Accuracy: 0.3837, Test Accuracy: 0.3387\n",
      "Epoch [4/50], Train Loss: 1.0806, Train Accuracy: 0.3837, Test Accuracy: 0.3226\n",
      "Epoch [5/50], Train Loss: 1.0814, Train Accuracy: 0.3918, Test Accuracy: 0.3387\n",
      "Epoch [6/50], Train Loss: 1.0742, Train Accuracy: 0.4122, Test Accuracy: 0.3871\n",
      "Epoch [7/50], Train Loss: 1.0648, Train Accuracy: 0.4245, Test Accuracy: 0.3387\n",
      "Epoch [8/50], Train Loss: 1.0599, Train Accuracy: 0.4531, Test Accuracy: 0.4355\n",
      "Epoch [9/50], Train Loss: 1.0514, Train Accuracy: 0.4612, Test Accuracy: 0.4355\n",
      "Epoch [10/50], Train Loss: 1.0516, Train Accuracy: 0.4694, Test Accuracy: 0.4032\n",
      "Epoch [11/50], Train Loss: 1.0401, Train Accuracy: 0.4816, Test Accuracy: 0.4355\n",
      "Epoch [12/50], Train Loss: 1.0396, Train Accuracy: 0.4653, Test Accuracy: 0.4355\n",
      "Epoch [13/50], Train Loss: 1.0345, Train Accuracy: 0.4653, Test Accuracy: 0.4355\n",
      "Epoch [14/50], Train Loss: 1.0331, Train Accuracy: 0.4571, Test Accuracy: 0.4355\n",
      "Epoch [15/50], Train Loss: 1.0272, Train Accuracy: 0.4694, Test Accuracy: 0.4355\n",
      "Epoch [16/50], Train Loss: 1.0263, Train Accuracy: 0.4612, Test Accuracy: 0.4355\n",
      "Epoch [17/50], Train Loss: 1.0217, Train Accuracy: 0.4776, Test Accuracy: 0.4032\n",
      "Epoch [18/50], Train Loss: 1.0228, Train Accuracy: 0.4776, Test Accuracy: 0.4355\n",
      "Epoch [19/50], Train Loss: 1.0178, Train Accuracy: 0.4735, Test Accuracy: 0.4194\n",
      "Epoch [20/50], Train Loss: 1.0145, Train Accuracy: 0.4776, Test Accuracy: 0.4194\n",
      "Epoch [21/50], Train Loss: 1.0177, Train Accuracy: 0.4653, Test Accuracy: 0.4355\n",
      "Epoch [22/50], Train Loss: 1.0112, Train Accuracy: 0.4612, Test Accuracy: 0.4032\n",
      "Epoch [23/50], Train Loss: 1.0134, Train Accuracy: 0.4816, Test Accuracy: 0.4032\n",
      "Epoch [24/50], Train Loss: 1.0131, Train Accuracy: 0.4694, Test Accuracy: 0.4355\n",
      "Epoch [25/50], Train Loss: 1.0104, Train Accuracy: 0.4735, Test Accuracy: 0.4194\n",
      "Epoch [26/50], Train Loss: 1.0078, Train Accuracy: 0.4776, Test Accuracy: 0.4355\n",
      "Epoch [27/50], Train Loss: 1.0107, Train Accuracy: 0.4612, Test Accuracy: 0.4355\n",
      "Epoch [28/50], Train Loss: 1.0057, Train Accuracy: 0.4735, Test Accuracy: 0.4355\n",
      "Epoch [29/50], Train Loss: 1.0144, Train Accuracy: 0.4612, Test Accuracy: 0.4516\n",
      "Epoch [30/50], Train Loss: 1.0035, Train Accuracy: 0.4571, Test Accuracy: 0.4355\n",
      "Epoch [31/50], Train Loss: 0.9985, Train Accuracy: 0.4857, Test Accuracy: 0.4194\n",
      "Epoch [32/50], Train Loss: 0.9969, Train Accuracy: 0.4857, Test Accuracy: 0.4355\n",
      "Epoch [33/50], Train Loss: 1.0065, Train Accuracy: 0.4776, Test Accuracy: 0.4194\n",
      "Epoch [34/50], Train Loss: 1.0047, Train Accuracy: 0.4694, Test Accuracy: 0.4355\n",
      "Epoch [35/50], Train Loss: 0.9995, Train Accuracy: 0.4694, Test Accuracy: 0.4355\n",
      "Epoch [36/50], Train Loss: 0.9971, Train Accuracy: 0.4939, Test Accuracy: 0.4355\n",
      "Epoch [37/50], Train Loss: 0.9987, Train Accuracy: 0.4694, Test Accuracy: 0.4355\n",
      "Epoch [38/50], Train Loss: 0.9982, Train Accuracy: 0.4653, Test Accuracy: 0.4032\n",
      "Epoch [39/50], Train Loss: 0.9941, Train Accuracy: 0.4857, Test Accuracy: 0.4355\n",
      "Epoch [40/50], Train Loss: 0.9928, Train Accuracy: 0.5061, Test Accuracy: 0.4355\n",
      "Epoch [41/50], Train Loss: 0.9985, Train Accuracy: 0.4816, Test Accuracy: 0.4194\n",
      "Epoch [42/50], Train Loss: 0.9954, Train Accuracy: 0.4653, Test Accuracy: 0.4032\n",
      "Epoch [43/50], Train Loss: 0.9924, Train Accuracy: 0.4939, Test Accuracy: 0.4194\n",
      "Epoch [44/50], Train Loss: 0.9911, Train Accuracy: 0.4898, Test Accuracy: 0.4355\n",
      "Epoch [45/50], Train Loss: 0.9898, Train Accuracy: 0.4816, Test Accuracy: 0.4032\n",
      "Epoch [46/50], Train Loss: 0.9913, Train Accuracy: 0.4735, Test Accuracy: 0.4355\n",
      "Epoch [47/50], Train Loss: 0.9841, Train Accuracy: 0.4857, Test Accuracy: 0.4355\n",
      "Epoch [48/50], Train Loss: 0.9827, Train Accuracy: 0.4816, Test Accuracy: 0.4355\n",
      "Epoch [49/50], Train Loss: 0.9850, Train Accuracy: 0.4898, Test Accuracy: 0.4516\n",
      "Epoch [50/50], Train Loss: 0.9870, Train Accuracy: 0.4735, Test Accuracy: 0.4194\n",
      "Final Test Accuracy: 0.4194\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(25)\n",
    "\n",
    "# Define the attention-based model\n",
    "class AttentionModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(AttentionModel, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        self.classifier = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        attention_weights = self.attention(x).squeeze(-1)\n",
    "        attention_weights = attention_weights.masked_fill(mask == 0, float('-inf'))\n",
    "        attention_weights = torch.softmax(attention_weights, dim=1)\n",
    "        \n",
    "        weighted_sum = torch.sum(x * attention_weights.unsqueeze(-1), dim=1)\n",
    "        \n",
    "        output = self.classifier(weighted_sum)\n",
    "        return output, attention_weights\n",
    "\n",
    "# Custom dataset\n",
    "class GeneDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        gene_data = torch.FloatTensor(self.data[idx])\n",
    "        label = torch.LongTensor([self.labels[idx] + 1])  # Add 1 to shift labels to 0, 1, 2\n",
    "        mask = torch.ones(len(gene_data))\n",
    "        return gene_data, label, mask\n",
    "\n",
    "# Collate function for DataLoader\n",
    "def collate_fn(batch):\n",
    "    # Sort the batch by sequence length (descending)\n",
    "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    sequences, labels, masks = zip(*batch)\n",
    "    \n",
    "    # Get lengths of each sequence\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "    max_len = max(lengths)\n",
    "    \n",
    "    # Pad sequences\n",
    "    padded_seqs = torch.zeros(len(sequences), max_len, sequences[0].size(1))\n",
    "    padded_masks = torch.zeros(len(sequences), max_len)\n",
    "    \n",
    "    for i, (seq, length) in enumerate(zip(sequences, lengths)):\n",
    "        padded_seqs[i, :length] = seq\n",
    "        padded_masks[i, :length] = 1\n",
    "    \n",
    "    return padded_seqs, torch.cat(labels), padded_masks\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = GeneDataset(X_train_normalized, y_train_raw)\n",
    "test_dataset = GeneDataset(X_test_normalized, y_test_raw)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Initialize the model\n",
    "input_dim = len(FEATURE_NAMES)  # number of features per region\n",
    "hidden_dim = 64\n",
    "output_dim = 3  # number of classes (-1, 0, 1)\n",
    "model = AttentionModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=0.0001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for batch_x, batch_y, batch_mask in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(batch_x, batch_mask)\n",
    "        loss = criterion(outputs, batch_y.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        train_correct += (outputs.argmax(dim=1) == batch_y.squeeze()).sum().item()\n",
    "        train_total += batch_y.size(0)\n",
    "\n",
    "    # scheduler.step()\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y, batch_mask in test_loader:\n",
    "            outputs, _ = model(batch_x, batch_mask)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y.squeeze()).sum().item()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {total_loss/len(train_loader):.4f}, Train Accuracy: {train_correct/train_total:.4f}, Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Final evaluation\n",
    "model.eval()\n",
    "all_attention_weights = []\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y, batch_mask in test_loader:\n",
    "        outputs, attention_weights = model(batch_x, batch_mask)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_attention_weights.extend(attention_weights.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "# Print final accuracy\n",
    "final_accuracy = sum(np.array(all_predictions) == np.array(all_labels).squeeze()) / len(all_labels)\n",
    "print(f'Final Test Accuracy: {final_accuracy:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gene",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
