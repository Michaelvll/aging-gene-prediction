{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmichaelvll\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/gcpuser/gene/wandb/run-20241019_234002-xay7eq8i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/michaelvll/gene/runs/xay7eq8i' target=\"_blank\">gentle-silence-39</a></strong> to <a href='https://wandb.ai/michaelvll/gene' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/michaelvll/gene' target=\"_blank\">https://wandb.ai/michaelvll/gene</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/michaelvll/gene/runs/xay7eq8i' target=\"_blank\">https://wandb.ai/michaelvll/gene/runs/xay7eq8i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:xay7eq8i) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gentle-silence-39</strong> at: <a href='https://wandb.ai/michaelvll/gene/runs/xay7eq8i' target=\"_blank\">https://wandb.ai/michaelvll/gene/runs/xay7eq8i</a><br/> View project at: <a href='https://wandb.ai/michaelvll/gene' target=\"_blank\">https://wandb.ai/michaelvll/gene</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241019_234002-xay7eq8i/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:xay7eq8i). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/gcpuser/gene/wandb/run-20241019_234003-o10ys5uh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/michaelvll/gene/runs/o10ys5uh' target=\"_blank\">swift-planet-40</a></strong> to <a href='https://wandb.ai/michaelvll/gene' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/michaelvll/gene' target=\"_blank\">https://wandb.ai/michaelvll/gene</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/michaelvll/gene/runs/o10ys5uh' target=\"_blank\">https://wandb.ai/michaelvll/gene/runs/o10ys5uh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    gene   2mo   9mo  18mo  9mo-2mo  18mo-9mo   9mo/2mo  18mo/9mo  old-young  \\\n",
      "0  Rgs20  0.65  0.60  0.90    -0.05      0.30  0.923077  1.500000       0.25   \n",
      "1  Sulf1  0.36  0.52  0.56     0.16      0.04  1.444444  1.076923       0.20   \n",
      "2  Sulf1  0.43  0.59  0.64     0.16      0.05  1.372093  1.084746       0.21   \n",
      "3   Eya1  0.68  0.62  0.47    -0.06     -0.15  0.911765  0.758065      -0.21   \n",
      "4   Eya1  0.61  0.37  0.45    -0.24      0.08  0.606557  1.216216      -0.16   \n",
      "\n",
      "   old/young  distance  \n",
      "0   1.384615  151241.0  \n",
      "1   1.555556  121205.0  \n",
      "2   1.488372  170142.0  \n",
      "3   0.691176  137980.0  \n",
      "4   0.737705  138254.0  \n",
      "      gene  log2(old/young)  distance\n",
      "0    Itgb5         1.521456   32221.0\n",
      "1   Begain         1.320315   37592.0\n",
      "2   BEGAIN         1.320315   36795.0\n",
      "3     Eya4         1.588159  247781.0\n",
      "4  Gm27247         1.018367   97190.0\n",
      "[[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0.65, 0.6, 0.9, -0.050000000000000044, 0.30000000000000004, 0.923076923076923, 1.5, 0.25, 1.3846153846153846, 151241.0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]\n",
      "zero: 16703, non-zero: 1080\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from data_loader import load_data, get_balanced_data, normalize_features\n",
    "\n",
    "import wandb\n",
    "wandb.init(project='gene')\n",
    "\n",
    "data = load_data()\n",
    "X_balanced, y_balanced = get_balanced_data(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 16\n",
    "NUM_LAYERS = 2\n",
    "NUM_HEADS = 1\n",
    "DROPOUT = 0.2\n",
    "LR = 0.001\n",
    "OUTPUT_DIM = 3  # number of classes (-1, 0, 1)\n",
    "NUM_EPOCHS = 200\n",
    "\n",
    "\n",
    "class TwoHeadTransformerModel(nn.Module):\n",
    "    def __init__(self, mcg_input_dim, atac_input_dim, hidden_dim, output_dim, num_layers=2, num_heads=1, dropout=0.1):\n",
    "        super(TwoHeadTransformerModel, self).__init__()\n",
    "        self.mcg_embedding = nn.Linear(mcg_input_dim, hidden_dim)\n",
    "        self.atac_embedding = nn.Linear(atac_input_dim, hidden_dim)\n",
    "        \n",
    "        encoder_layers = nn.TransformerEncoderLayer(hidden_dim, num_heads, dim_feedforward=hidden_dim*2, dropout=dropout, batch_first=True)\n",
    "        self.mcg_transformer = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.atac_transformer = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, mcg_x, mcg_mask, atac_x, atac_mask):\n",
    "        mcg_x = self.mcg_embedding(mcg_x)\n",
    "        atac_x = self.atac_embedding(atac_x)\n",
    "        \n",
    "        mcg_x = self.mcg_transformer(mcg_x, src_key_padding_mask=~mcg_mask.bool())\n",
    "        atac_x = self.atac_transformer(atac_x, src_key_padding_mask=~atac_mask.bool())\n",
    "        \n",
    "        # Global average pooling\n",
    "        mcg_x = mcg_x.mean(dim=1)\n",
    "        atac_x = atac_x.mean(dim=1)\n",
    "        \n",
    "        # Concatenate MCG and ATAC embeddings\n",
    "        combined_x = torch.cat((mcg_x, atac_x), dim=1)\n",
    "        \n",
    "        output = self.classifier(combined_x)\n",
    "        return output\n",
    "\n",
    "class CombinedGeneDataset(Dataset):\n",
    "    def __init__(self, mcg_data, atac_data, labels):\n",
    "        self.mcg_data = mcg_data\n",
    "        self.atac_data = atac_data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        mcg_gene_data = torch.FloatTensor(self.mcg_data[idx])\n",
    "        atac_gene_data = torch.FloatTensor(self.atac_data[idx])\n",
    "        label = torch.LongTensor([self.labels[idx] + 1])  # Add 1 to shift labels to 0, 1, 2\n",
    "        mcg_mask = torch.ones(len(mcg_gene_data))\n",
    "        atac_mask = torch.ones(len(atac_gene_data))\n",
    "        return mcg_gene_data, atac_gene_data, label, mcg_mask, atac_mask\n",
    "\n",
    "def combined_collate_fn(batch):\n",
    "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    mcg_sequences, atac_sequences, labels, mcg_masks, atac_masks = zip(*batch)\n",
    "    \n",
    "    mcg_lengths = [len(seq) for seq in mcg_sequences]\n",
    "    atac_lengths = [len(seq) for seq in atac_sequences]\n",
    "    mcg_max_len = max(mcg_lengths)\n",
    "    atac_max_len = max(atac_lengths)\n",
    "    \n",
    "    padded_mcg_seqs = torch.zeros(len(mcg_sequences), mcg_max_len, mcg_sequences[0].size(1))\n",
    "    padded_atac_seqs = torch.zeros(len(atac_sequences), atac_max_len, atac_sequences[0].size(1))\n",
    "    padded_mcg_masks = torch.zeros(len(mcg_sequences), mcg_max_len)\n",
    "    padded_atac_masks = torch.zeros(len(atac_sequences), atac_max_len)\n",
    "    \n",
    "    for i, (mcg_seq, atac_seq, mcg_length, atac_length) in enumerate(zip(mcg_sequences, atac_sequences, mcg_lengths, atac_lengths)):\n",
    "        padded_mcg_seqs[i, :mcg_length] = mcg_seq\n",
    "        padded_atac_seqs[i, :atac_length] = atac_seq\n",
    "        padded_mcg_masks[i, :mcg_length] = 1\n",
    "        padded_atac_masks[i, :atac_length] = 1\n",
    "    \n",
    "    return padded_mcg_seqs, padded_atac_seqs, torch.cat(labels), padded_mcg_masks, padded_atac_masks\n",
    "\n",
    "def train_combined_model(X_train_mcg, X_train_atac, y_train, X_test_mcg, X_test_atac, y_test, exp_name):\n",
    "    wandb.init(project='gene', group=exp_name)\n",
    "    mcg_input_dim = len(X_train_mcg[0][0])\n",
    "    atac_input_dim = len(X_train_atac[0][0])\n",
    "    \n",
    "    train_dataset = CombinedGeneDataset(X_train_mcg, X_train_atac, y_train)\n",
    "    test_dataset = CombinedGeneDataset(X_test_mcg, X_test_atac, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=combined_collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=combined_collate_fn)\n",
    "\n",
    "    model = TwoHeadTransformerModel(mcg_input_dim, atac_input_dim, HIDDEN_DIM, OUTPUT_DIM, num_layers=NUM_LAYERS, num_heads=NUM_HEADS, dropout=DROPOUT)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=0.0001)\n",
    "\n",
    "    for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        for mcg_x, atac_x, batch_y, mcg_mask, atac_mask in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(mcg_x, mcg_mask, atac_x, atac_mask)\n",
    "            loss = criterion(outputs, batch_y.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            train_correct += (outputs.argmax(dim=1) == batch_y.squeeze()).sum().item()\n",
    "            train_total += batch_y.size(0)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for mcg_x, atac_x, batch_y, mcg_mask, atac_mask in test_loader:\n",
    "                outputs = model(mcg_x, mcg_mask, atac_x, atac_mask)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += batch_y.size(0)\n",
    "                correct += (predicted == batch_y.squeeze()).sum().item()\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        # print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {total_loss/len(train_loader):.4f}, Train Accuracy: {train_correct/train_total:.4f}, Test Accuracy: {accuracy:.4f}')\n",
    "        wandb.log({'epoch': epoch, 'LR': optimizer.param_groups[0]['lr'], 'train_loss': total_loss/len(train_loader), 'train_accuracy': train_correct/train_total, 'test_accuracy': accuracy})\n",
    "\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for mcg_x, atac_x, batch_y, mcg_mask, atac_mask in test_loader:\n",
    "            outputs = model(mcg_x, mcg_mask, atac_x, atac_mask)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    final_accuracy = sum(np.array(all_predictions) == np.array(all_labels).squeeze()) / len(all_labels)\n",
    "    print(f'Final Test Accuracy: {final_accuracy:.4f}')\n",
    "    return final_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:o10ys5uh) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">swift-planet-40</strong> at: <a href='https://wandb.ai/michaelvll/gene/runs/o10ys5uh' target=\"_blank\">https://wandb.ai/michaelvll/gene/runs/o10ys5uh</a><br/> View project at: <a href='https://wandb.ai/michaelvll/gene' target=\"_blank\">https://wandb.ai/michaelvll/gene</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241019_234003-o10ys5uh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:o10ys5uh). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/gcpuser/gene/wandb/run-20241019_234010-bwoz6jhz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/michaelvll/gene/runs/bwoz6jhz' target=\"_blank\">legendary-dawn-41</a></strong> to <a href='https://wandb.ai/michaelvll/gene' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/michaelvll/gene' target=\"_blank\">https://wandb.ai/michaelvll/gene</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/michaelvll/gene/runs/bwoz6jhz' target=\"_blank\">https://wandb.ai/michaelvll/gene/runs/bwoz6jhz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gene/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n",
      "100%|██████████| 200/200 [02:30<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 0.5710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:bwoz6jhz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>LR</td><td>█████████▇▇▇▇▇▆▆▆▅▅▅▅▅▄▄▄▃▃▃▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇█</td></tr><tr><td>test_accuracy</td><td>▃▄▆▁▇▇▆▇▆█▇▇█▇▇▇██▇█████▇▇██████████████</td></tr><tr><td>train_accuracy</td><td>▁▃▃▄▄▅▄▅▅▅▆▇▆▆▇▇▆▆▇▇▇▆▇▇▇▇█▇▇█▇▇▇█▇▇█▇▇█</td></tr><tr><td>train_loss</td><td>██▇▆▆▅▅▅▅▆▄▄▃▄▄▄▃▄▃▄▃▃▃▁▂▂▂▂▂▂▂▂▁▂▁▂▁▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>LR</td><td>0.0001</td></tr><tr><td>epoch</td><td>199</td></tr><tr><td>test_accuracy</td><td>0.57099</td></tr><tr><td>train_accuracy</td><td>0.57485</td></tr><tr><td>train_loss</td><td>0.89676</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">legendary-dawn-41</strong> at: <a href='https://wandb.ai/michaelvll/gene/runs/bwoz6jhz' target=\"_blank\">https://wandb.ai/michaelvll/gene/runs/bwoz6jhz</a><br/> View project at: <a href='https://wandb.ai/michaelvll/gene' target=\"_blank\">https://wandb.ai/michaelvll/gene</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241019_234010-bwoz6jhz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:bwoz6jhz). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/gcpuser/gene/wandb/run-20241019_234243-qdtw0nzl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/michaelvll/gene/runs/qdtw0nzl' target=\"_blank\">dazzling-surf-42</a></strong> to <a href='https://wandb.ai/michaelvll/gene' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/michaelvll/gene' target=\"_blank\">https://wandb.ai/michaelvll/gene</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/michaelvll/gene/runs/qdtw0nzl' target=\"_blank\">https://wandb.ai/michaelvll/gene/runs/qdtw0nzl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gene/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n",
      " 96%|█████████▌| 192/200 [02:24<00:05,  1.35it/s]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "exp_name = f'two-head-{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "wandb.config.update({\n",
    "    'hidden_dim': HIDDEN_DIM,\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'num_heads': NUM_HEADS,\n",
    "    'dropout': DROPOUT,\n",
    "    'lr': LR,\n",
    "    'output_dim': OUTPUT_DIM,\n",
    "    'num_epochs': NUM_EPOCHS\n",
    "})\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=25)\n",
    "accuracies = []\n",
    "for train_index, test_index in kf.split(X_balanced['mcg']):\n",
    "    X_train_mcg, X_test_mcg = [X_balanced['mcg'][i] for i in train_index], [X_balanced['mcg'][i] for i in test_index]\n",
    "    X_train_atac, X_test_atac = [X_balanced['atac'][i] for i in train_index], [X_balanced['atac'][i] for i in test_index]\n",
    "    y_train, y_test = [y_balanced[i] for i in train_index], [y_balanced[i] for i in test_index]\n",
    "    \n",
    "    X_train_mcg_normalized, X_test_mcg_normalized = normalize_features(X_train_mcg, X_test_mcg)\n",
    "    X_train_atac_normalized, X_test_atac_normalized = normalize_features(X_train_atac, X_test_atac)\n",
    "    \n",
    "    accuracies.append(train_combined_model(X_train_mcg_normalized, X_train_atac_normalized, y_train, \n",
    "                                           X_test_mcg_normalized, X_test_atac_normalized, y_test, exp_name=exp_name))\n",
    "\n",
    "print(f'Mean Accuracy: {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gene",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
