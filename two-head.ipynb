{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:gslh0ni3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pretty-disco-46</strong> at: <a href='https://wandb.ai/michaelvll/gene/runs/gslh0ni3' target=\"_blank\">https://wandb.ai/michaelvll/gene/runs/gslh0ni3</a><br/> View project at: <a href='https://wandb.ai/michaelvll/gene' target=\"_blank\">https://wandb.ai/michaelvll/gene</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241020_032920-gslh0ni3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:gslh0ni3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/gcpuser/gene/wandb/run-20241020_032945-to997tbg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/michaelvll/gene/runs/to997tbg' target=\"_blank\">hearty-plant-47</a></strong> to <a href='https://wandb.ai/michaelvll/gene' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/michaelvll/gene' target=\"_blank\">https://wandb.ai/michaelvll/gene</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/michaelvll/gene/runs/to997tbg' target=\"_blank\">https://wandb.ai/michaelvll/gene/runs/to997tbg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    gene   2mo   9mo  18mo  9mo-2mo  18mo-9mo   9mo/2mo  18mo/9mo  old-young  \\\n",
      "0  Rgs20  0.65  0.60  0.90    -0.05      0.30  0.923077  1.500000       0.25   \n",
      "1  Sulf1  0.36  0.52  0.56     0.16      0.04  1.444444  1.076923       0.20   \n",
      "2  Sulf1  0.43  0.59  0.64     0.16      0.05  1.372093  1.084746       0.21   \n",
      "3   Eya1  0.68  0.62  0.47    -0.06     -0.15  0.911765  0.758065      -0.21   \n",
      "4   Eya1  0.61  0.37  0.45    -0.24      0.08  0.606557  1.216216      -0.16   \n",
      "\n",
      "   old/young  distance  \n",
      "0   1.384615  151241.0  \n",
      "1   1.555556  121205.0  \n",
      "2   1.488372  170142.0  \n",
      "3   0.691176  137980.0  \n",
      "4   0.737705  138254.0  \n",
      "      gene  log2(old/young)  distance\n",
      "0    Itgb5         1.521456   32221.0\n",
      "1   Begain         1.320315   37592.0\n",
      "2   BEGAIN         1.320315   36795.0\n",
      "3     Eya4         1.588159  247781.0\n",
      "4  Gm27247         1.018367   97190.0\n",
      "[[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0.65, 0.6, 0.9, -0.050000000000000044, 0.30000000000000004, 0.923076923076923, 1.5, 0.25, 1.3846153846153846, 151241.0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]\n",
      "zero: 16703, non-zero: 1080\n",
      "1620 1620\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from data_loader import load_data, get_balanced_data, normalize_features\n",
    "\n",
    "import wandb\n",
    "wandb.init(project='gene')\n",
    "\n",
    "data = load_data()\n",
    "X_balanced, y_balanced = get_balanced_data(data)\n",
    "print(len(X_balanced['mcg']), len(X_balanced['atac']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 16\n",
    "NUM_LAYERS = 2\n",
    "NUM_HEADS = 1\n",
    "DROPOUT = 0.2\n",
    "LR = 0.001\n",
    "OUTPUT_DIM = 3  # number of classes (-1, 0, 1)\n",
    "NUM_EPOCHS = 200\n",
    "\n",
    "\n",
    "class TwoHeadTransformerModel(nn.Module):\n",
    "    def __init__(self, mcg_input_dim, atac_input_dim, hidden_dim, output_dim, num_layers=2, num_heads=1, dropout=0.1):\n",
    "        super(TwoHeadTransformerModel, self).__init__()\n",
    "        self.mcg_embedding = nn.Linear(mcg_input_dim, hidden_dim)\n",
    "        self.atac_embedding = nn.Linear(atac_input_dim, hidden_dim)\n",
    "        \n",
    "        encoder_layers = nn.TransformerEncoderLayer(hidden_dim, num_heads, dim_feedforward=hidden_dim*2, dropout=dropout, batch_first=True)\n",
    "        self.mcg_transformer = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.atac_transformer = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, mcg_x, mcg_mask, atac_x, atac_mask):\n",
    "        mcg_x = self.mcg_embedding(mcg_x)\n",
    "        atac_x = self.atac_embedding(atac_x)\n",
    "        \n",
    "        mcg_x = self.mcg_transformer(mcg_x, src_key_padding_mask=~mcg_mask.bool())\n",
    "        atac_x = self.atac_transformer(atac_x, src_key_padding_mask=~atac_mask.bool())\n",
    "        \n",
    "        # Global average pooling\n",
    "        mcg_x = mcg_x.mean(dim=1)\n",
    "        atac_x = atac_x.mean(dim=1)\n",
    "        \n",
    "        # Concatenate MCG and ATAC embeddings\n",
    "        combined_x = torch.cat((mcg_x, atac_x), dim=1)\n",
    "        \n",
    "        output = self.classifier(combined_x)\n",
    "        return output\n",
    "\n",
    "class CombinedGeneDataset(Dataset):\n",
    "    def __init__(self, mcg_data, atac_data, labels):\n",
    "        self.mcg_data = mcg_data\n",
    "        self.atac_data = atac_data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        mcg_gene_data = torch.FloatTensor(self.mcg_data[idx])\n",
    "        atac_gene_data = torch.FloatTensor(self.atac_data[idx])\n",
    "        label = torch.LongTensor([self.labels[idx] + 1])  # Add 1 to shift labels to 0, 1, 2\n",
    "        mcg_mask = torch.ones(len(mcg_gene_data))\n",
    "        atac_mask = torch.ones(len(atac_gene_data))\n",
    "        return mcg_gene_data, atac_gene_data, label, mcg_mask, atac_mask\n",
    "\n",
    "def combined_collate_fn(batch):\n",
    "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    mcg_sequences, atac_sequences, labels, mcg_masks, atac_masks = zip(*batch)\n",
    "    \n",
    "    mcg_lengths = [len(seq) for seq in mcg_sequences]\n",
    "    atac_lengths = [len(seq) for seq in atac_sequences]\n",
    "    mcg_max_len = max(mcg_lengths)\n",
    "    atac_max_len = max(atac_lengths)\n",
    "    \n",
    "    padded_mcg_seqs = torch.zeros(len(mcg_sequences), mcg_max_len, mcg_sequences[0].size(1))\n",
    "    padded_atac_seqs = torch.zeros(len(atac_sequences), atac_max_len, atac_sequences[0].size(1))\n",
    "    padded_mcg_masks = torch.zeros(len(mcg_sequences), mcg_max_len)\n",
    "    padded_atac_masks = torch.zeros(len(atac_sequences), atac_max_len)\n",
    "    \n",
    "    for i, (mcg_seq, atac_seq, mcg_length, atac_length) in enumerate(zip(mcg_sequences, atac_sequences, mcg_lengths, atac_lengths)):\n",
    "        padded_mcg_seqs[i, :mcg_length] = mcg_seq\n",
    "        padded_atac_seqs[i, :atac_length] = atac_seq\n",
    "        padded_mcg_masks[i, :mcg_length] = 1\n",
    "        padded_atac_masks[i, :atac_length] = 1\n",
    "    \n",
    "    return padded_mcg_seqs, padded_atac_seqs, torch.cat(labels), padded_mcg_masks, padded_atac_masks\n",
    "\n",
    "def train_combined_model(X_train_mcg, X_train_atac, y_train, X_test_mcg, X_test_atac, y_test, exp_name):\n",
    "    wandb.init(project='gene', group=exp_name)\n",
    "    mcg_input_dim = len(X_train_mcg[0][0])\n",
    "    atac_input_dim = len(X_train_atac[0][0])\n",
    "    \n",
    "    train_dataset = CombinedGeneDataset(X_train_mcg, X_train_atac, y_train)\n",
    "    test_dataset = CombinedGeneDataset(X_test_mcg, X_test_atac, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=combined_collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=combined_collate_fn)\n",
    "\n",
    "    model = TwoHeadTransformerModel(mcg_input_dim, atac_input_dim, HIDDEN_DIM, OUTPUT_DIM, num_layers=NUM_LAYERS, num_heads=NUM_HEADS, dropout=DROPOUT)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=0.0001)\n",
    "\n",
    "    for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        for mcg_x, atac_x, batch_y, mcg_mask, atac_mask in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(mcg_x, mcg_mask, atac_x, atac_mask)\n",
    "            loss = criterion(outputs, batch_y.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            train_correct += (outputs.argmax(dim=1) == batch_y.squeeze()).sum().item()\n",
    "            train_total += batch_y.size(0)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for mcg_x, atac_x, batch_y, mcg_mask, atac_mask in test_loader:\n",
    "                outputs = model(mcg_x, mcg_mask, atac_x, atac_mask)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += batch_y.size(0)\n",
    "                correct += (predicted == batch_y.squeeze()).sum().item()\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        # print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {total_loss/len(train_loader):.4f}, Train Accuracy: {train_correct/train_total:.4f}, Test Accuracy: {accuracy:.4f}')\n",
    "        wandb.log({'epoch': epoch, 'LR': optimizer.param_groups[0]['lr'], 'train_loss': total_loss/len(train_loader), 'train_accuracy': train_correct/train_total, 'test_accuracy': accuracy})\n",
    "\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for mcg_x, atac_x, batch_y, mcg_mask, atac_mask in test_loader:\n",
    "            outputs = model(mcg_x, mcg_mask, atac_x, atac_mask)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    final_accuracy = sum(np.array(all_predictions) == np.array(all_labels).squeeze()) / len(all_labels)\n",
    "    print(f'Final Test Accuracy: {final_accuracy:.4f}')\n",
    "    return final_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:to997tbg) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hearty-plant-47</strong> at: <a href='https://wandb.ai/michaelvll/gene/runs/to997tbg' target=\"_blank\">https://wandb.ai/michaelvll/gene/runs/to997tbg</a><br/> View project at: <a href='https://wandb.ai/michaelvll/gene' target=\"_blank\">https://wandb.ai/michaelvll/gene</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241020_032945-to997tbg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:to997tbg). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/gcpuser/gene/wandb/run-20241020_032952-fi03qr2d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/michaelvll/gene/runs/fi03qr2d' target=\"_blank\">resilient-elevator-48</a></strong> to <a href='https://wandb.ai/michaelvll/gene' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/michaelvll/gene' target=\"_blank\">https://wandb.ai/michaelvll/gene</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/michaelvll/gene/runs/fi03qr2d' target=\"_blank\">https://wandb.ai/michaelvll/gene/runs/fi03qr2d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gene/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n",
      " 63%|██████▎   | 126/200 [01:34<00:55,  1.33it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     X_train_mcg_normalized, X_test_mcg_normalized \u001b[38;5;241m=\u001b[39m normalize_features(X_train_mcg, X_test_mcg)\n\u001b[1;32m     23\u001b[0m     X_train_atac_normalized, X_test_atac_normalized \u001b[38;5;241m=\u001b[39m normalize_features(X_train_atac, X_test_atac)\n\u001b[0;32m---> 25\u001b[0m     accuracies\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtrain_combined_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_mcg_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_atac_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mX_test_mcg_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_atac_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp_name\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(accuracies)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ± \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mstd(accuracies)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 105\u001b[0m, in \u001b[0;36mtrain_combined_model\u001b[0;34m(X_train_mcg, X_train_atac, y_train, X_test_mcg, X_test_atac, y_test, exp_name)\u001b[0m\n\u001b[1;32m    103\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(mcg_x, mcg_mask, atac_x, atac_mask)\n\u001b[1;32m    104\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y\u001b[38;5;241m.\u001b[39msqueeze())\n\u001b[0;32m--> 105\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    107\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/conda/envs/gene/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/gene/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/gene/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "exp_name = f'two-head-{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "wandb.config.update({\n",
    "    'hidden_dim': HIDDEN_DIM,\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'num_heads': NUM_HEADS,\n",
    "    'dropout': DROPOUT,\n",
    "    'lr': LR,\n",
    "    'output_dim': OUTPUT_DIM,\n",
    "    'num_epochs': NUM_EPOCHS\n",
    "})\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=25)\n",
    "accuracies = []\n",
    "for train_index, test_index in kf.split(X_balanced['mcg']):\n",
    "    X_train_mcg, X_test_mcg = [X_balanced['mcg'][i] for i in train_index], [X_balanced['mcg'][i] for i in test_index]\n",
    "    X_train_atac, X_test_atac = [X_balanced['atac'][i] for i in train_index], [X_balanced['atac'][i] for i in test_index]\n",
    "    y_train, y_test = [y_balanced[i] for i in train_index], [y_balanced[i] for i in test_index]\n",
    "    \n",
    "    X_train_mcg_normalized, X_test_mcg_normalized = normalize_features(X_train_mcg, X_test_mcg)\n",
    "    X_train_atac_normalized, X_test_atac_normalized = normalize_features(X_train_atac, X_test_atac)\n",
    "    \n",
    "    accuracies.append(train_combined_model(X_train_mcg_normalized, X_train_atac_normalized, y_train, \n",
    "                                           X_test_mcg_normalized, X_test_atac_normalized, y_test, exp_name=exp_name))\n",
    "\n",
    "print(f'Mean Accuracy: {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gene",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
